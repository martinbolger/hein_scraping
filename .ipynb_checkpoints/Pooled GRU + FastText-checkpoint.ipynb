{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\Documents\\Python_Scripts\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "dir = os.getcwd()\n",
    "print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = os.path.join(dir, 'toxic_comments_data', 'crawl-300d-2M.vec')\n",
    "\n",
    "train = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(dir,'toxic_comments_data', 'sample_submission.csv'))\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "\n",
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x1 = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "    conc = concatenate([x1, x2])\n",
    "    avg_pool = GlobalAveragePooling1D()(conc)\n",
    "    max_pool = GlobalMaxPooling1D()(conc)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    x = Dense(64, activation='relu')(conc)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/4\n",
      " - 3061s - loss: 0.0534 - acc: 0.9808 - val_loss: 0.0473 - val_acc: 0.9815\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.984913 \n",
      "\n",
      "Epoch 2/4\n",
      " - 2360s - loss: 0.0442 - acc: 0.9832 - val_loss: 0.0477 - val_acc: 0.9824\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986456 \n",
      "\n",
      "Epoch 3/4\n",
      " - 2521s - loss: 0.0414 - acc: 0.9839 - val_loss: 0.0434 - val_acc: 0.9834\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.987820 \n",
      "\n",
      "Epoch 4/4\n",
      " - 2793s - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0442 - val_acc: 0.9836\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.987418 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0778526ab4a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# serialize model to JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#  the keras model which is trained is defined as 'model' in this example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_num.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# keras library import  for Saving and loading model and weights\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "# serialize model to JSON\n",
    "#  the keras model which is trained is defined as 'model' in this example\n",
    "model_json = model.to_json()\n",
    "with open(\"model_num.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_num.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "# load json and create model\n",
    "json_file = open('model_num.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_num.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9992951  0.49124226 0.9698933  0.11353455 0.9058463  0.54024   ]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,  2030,   378,  4878,   723,     8,    58, 20911,\n",
       "          84,   888,   349,    16,  3439,    73,    21,     6,     5,\n",
       "        6226,     6,  1555,     7,    56,   378,  5462,  1488,   578,\n",
       "        5869,     5,    94,     6,     2,  3771,    30,   340,     6,\n",
       "         742,    37,  4878,   723,     8,    35,  4222,    10,  1205,\n",
       "         653,   400,   476, 17214,     9,   227,    15,   154,     5,\n",
       "       20074,     8,   247, 23545,    48,  4329,    52,    24,     4,\n",
       "        2108,   155,  2432,   578,  2428,    94,   218,   143,   490,\n",
       "          85])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.9901292e-01 4.6842294e-03 4.4172296e-01 1.1461467e-04 7.0217258e-01\n",
      "  4.6742605e-03]]\n"
     ]
    }
   ],
   "source": [
    "test_text = [' The full podcast is basically Joe slowly realising how stupid she is.']\n",
    "test_text = np.asarray(test_text)\n",
    "test_text = tokenizer.texts_to_sequences(test_text)\n",
    "test_text = sequence.pad_sequences(test_text, maxlen=maxlen)\n",
    "y_pred = loaded_model.predict(test_text, batch_size=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "max_features = 30000\n",
    "maxlen = 100\n",
    "comments_1 = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'Test_comments', 'comments_1.csv'))\n",
    "comments_2 = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'Test_comments', 'comments_2.csv'))\n",
    "comments_3 = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'Test_comments', 'comments_3.csv'))\n",
    "comments_6 = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'Test_comments', 'comments_6.csv'))\n",
    "comments_7 = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'Test_comments', 'comments_7.csv'))\n",
    "test_set = pd.read_csv(os.path.join(dir, 'toxic_comments_data', 'Test_comments', 'test_set.csv'))\n",
    "comments_1\n",
    "for comment in comments_1.commentText.dropna():\n",
    "# for comment in test_set.response_text.dropna():\n",
    "    test_text = np.asarray([comment])\n",
    "    test_text = tokenizer.texts_to_sequences(test_text)\n",
    "    test_text = sequence.pad_sequences(test_text, maxlen=maxlen)\n",
    "    y_pred = loaded_model.predict(test_text, batch_size=1)\n",
    "    if np.max(y_pred)>0.7:\n",
    "        print(comment)\n",
    "        print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n"
     ]
    }
   ],
   "source": [
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "print(X_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-gpu",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
