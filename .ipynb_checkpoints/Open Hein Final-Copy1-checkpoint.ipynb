{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")#put the adress of your page here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this part to automatically login, but DUO two step authentication is still required. You will have to enter that manually. After entering the DUO code, you will be given a warning that the information you enter could be sent over an insecure connection. Click continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'meb2fv'\n",
    "password = '0ver In 2018!'\n",
    "driver.find_element_by_id(\"user\").send_keys(username);\n",
    "driver.find_element_by_id(\"pass\").send_keys(password);\n",
    "driver.find_element_by_xpath(\"/html/body/main/div[2]/fieldset/form/input\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have logged in, Selenium is able to navigate to any webpage. We will navigate to the pages on the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Brandon \n",
      " Brandon L. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "last_name = 'Garrett'\n",
    "mid_first_name = 'Brandon'\n",
    "def search_names(mid_first_name, last_name):\n",
    "    link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/LuceneSearch?typea=title&termsa=&operator=AND&typeb=creator&termsb=' + last_name + '+' + mid_first_name + '&operatorb=AND&typec=text&termsc=&operatorc=AND&typed=title&termsd=&operatord=AND&typee=title&termse=&operatore=AND&typef=title&termsf=&yearlo=&yearhi=&tabfrom=&searchtype=field&collection=all&submit=Go'\n",
    "    driver.get(link)\n",
    "    element = driver.find_element_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[3]/div[2]')\n",
    "    title_index = 3\n",
    "    alt_fm_names = []\n",
    "    while element:\n",
    "        my_list = element.text\n",
    "        name_value = [a for a in my_list.split('\\n') if last_name and mid_first_name and '(Cited' in a ]\n",
    "        if name_value:\n",
    "            new_fm = name_value[0].split('(')[0].split(',')[1]\n",
    "            if not new_fm in alt_fm_names:\n",
    "                alt_fm_names.append(new_fm)\n",
    "                print(new_fm)\n",
    "        title_index +=4\n",
    "        try:\n",
    "            element = driver.find_element_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]')\n",
    "        except:\n",
    "            element = []\n",
    "    return alt_fm_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "while element:\n",
    "    data_stream = []\n",
    "    x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "    element = driver.find_elements_by_xpath(x_path_title)\n",
    "    #Get title:\n",
    "    if element:\n",
    "        scraped_papers = True\n",
    "        for elm in element:\n",
    "            data_stream.append(elm.text)\n",
    "\n",
    "        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]')\n",
    "        for elm in element:\n",
    "            my_list = elm.text\n",
    "        if [a for a in my_list.split('\\n') if last_name in a]:   \n",
    "            data_stream.append([a for a in my_list.split('\\n') if last_name in a][0])\n",
    "        else:\n",
    "            data_stream.append('na')\n",
    "        if [a for a in my_list.split('\\n') if 'Vol.' in a]:\n",
    "            data_stream.append([a for a in my_list.split('\\n') if 'Vol.' in a][0])\n",
    "        else:\n",
    "            data_stream.append('na')\n",
    "        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "        for elm in element:\n",
    "            cited_text = elm.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(*args):\n",
    "    cur_path = os.getcwd()\n",
    "    for value in args:\n",
    "        cur_path  = os.path.join(cur_path, value)\n",
    "    return cur_path\n",
    "\n",
    "def to_float_or_int(input_list):\n",
    "    new_list = []\n",
    "    for x in input_list:\n",
    "        x = x.replace(',','')\n",
    "        try:\n",
    "            value = int(x)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                value = float(x)\n",
    "            except:\n",
    "                value = ''\n",
    "        new_list.append(value)\n",
    "    return new_list\n",
    "\n",
    "def create_dataframe_dict(my_list, person, school, data_type):\n",
    "    my_dict = {'Person': person, \n",
    "              'School': school, \n",
    "              'Type': data_type}\n",
    "    for stat in stats:\n",
    "        my_dict[stat] = ''\n",
    "        for item in my_list:   \n",
    "            if item[0] == stat:\n",
    "                my_dict[stat] = item[1]  \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No remaining pages to scrape from Kerry Abrams.\n",
      "No remaining pages to scrape from Matthew Adler.\n",
      "No remaining pages to scrape from Katharine Bartlett.\n",
      "No remaining pages to scrape from Sara Beale.\n",
      "Name Stuart Benjamin is not in the database. You may be missing a middle initial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\Anaconda3\\envs\\webscraping\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No remaining pages to scrape from Joseph Blocher.\n",
      "No remaining pages to scrape from James Boyle.\n",
      "No remaining pages to scrape from Curtis Bradley.\n",
      "No remaining pages to scrape from Michael Bradley.\n",
      "No remaining pages to scrape from Rachel D. Brewster.\n",
      "No remaining pages to scrape from Allen Buchanan.\n",
      "No remaining pages to scrape from Samuel Buell.\n",
      "No remaining pages to scrape from Guy-Uriel E. Charles.\n",
      "No remaining pages to scrape from Charles T. Clotfelter.\n",
      "Name Wesley Cohen is not in the database. You may be missing a middle initial.\n",
      "No remaining pages to scrape from Doriane Coleman.\n",
      "No remaining pages to scrape from James Cox.\n",
      "Name John de Figueiredo is not in the database. You may be missing a middle initial.\n",
      "No remaining pages to scrape from Deborah A. DeMott.\n",
      "No remaining pages to scrape from Nita Farahany.\n",
      "No remaining pages to scrape from Joel Fleishman.\n",
      "No remaining pages to scrape from Michael D. Frakes.\n",
      "No remaining pages to scrape from Brandon Garrett.\n",
      "Name Lisa Griffin is not in the database. You may be missing a middle initial.\n",
      "Name Gaurang Mitu Gulati is not in the database. You may be missing a middle initial.\n",
      "No remaining pages to scrape from Paul Haagen.\n",
      "No remaining pages to scrape from Stanley Hauerwas.\n",
      "No remaining pages to scrape from Laurence R. Helfer.\n",
      "Name Karla Holloway is not in the database. You may be missing a middle initial.\n",
      "No remaining pages to scrape from Trina Jones.\n",
      "No remaining pages to scrape from Jack Knight.\n",
      "No remaining pages to scrape from Kimberly D. Krawiec.\n",
      "No remaining pages to scrape from Margaret Lemos.\n",
      "No remaining pages to scrape from David Levi.\n",
      "No remaining pages to scrape from Mathew McCubbins.\n",
      "No remaining pages to scrape from Francis E. McGovern.\n",
      "No remaining pages to scrape from Thomas B. Metzloff.\n",
      "No remaining pages to scrape from Ralf Michaels.\n",
      "Name Darrell Miller is not in the database. You may be missing a middle initial.\n",
      "No remaining pages to scrape from Madeline Morris.\n",
      "No remaining pages to scrape from H. Jefferson Powell.\n",
      "No remaining pages to scrape from Arti K. Rai.\n",
      "No remaining pages to scrape from Jerome Reichman.\n",
      "No remaining pages to scrape from Barak D. Richman.\n",
      "No remaining pages to scrape from Richard L. Schmalbeck.\n",
      "No remaining pages to scrape from Christopher Schroeder.\n",
      "No remaining pages to scrape from Steven L. Schwarcz.\n",
      "Name Neil Siegel is not in the database. You may be missing a middle initial.\n",
      "No remaining pages to scrape from John Weistart.\n",
      "Name Jonathan Wiener is not in the database. You may be missing a middle initial.\n",
      "No remaining pages to scrape from Ernest Young.\n",
      "No remaining pages to scrape from Lawrence Zelenak.\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "stats = ['Cited by Cases','Cited by Articles','Accessed (Past 12 Months)','Cited by Articles (Past 10 Years)', 'Cited by Articles (Past 1-2 years)', 'ScholarCheck Rank', 'Average Citations per Article', 'Average Citations per Document', 'Self-Citations']\n",
    "path = os.path.join(os.getcwd(), 'school_data')\n",
    "files = os.listdir(path)\n",
    "for file in files[5:6]:\n",
    "    main_df = pd.DataFrame()\n",
    "    skip_df = pd.DataFrame(columns = ['Full Name', 'School'])\n",
    "    data = pd.read_csv(os.path.join(os.getcwd(), 'school_data', file))    \n",
    "    for i in range(len(data)):\n",
    "        got_page = False\n",
    "        scraped_papers = False\n",
    "        prev_mid_first_name = '#@#'\n",
    "        mid_first_name = data['First Name'][i]\n",
    "        last_name = data['Last Name'][i]\n",
    "        full_name = mid_first_name + ' ' +  last_name\n",
    "        school = data['School'][i]\n",
    "        title = data['Title'][i]\n",
    "        while got_page == False:\n",
    "            link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + mid_first_name + '&collection=journals'\n",
    "            driver.get(link)\n",
    "            soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "            table_rows = soup.findAll('td', {'style': 'text-align:right;'})\n",
    "            if str(table_rows[1]) == '<td style=\"text-align:right;\"> 1 </td>':\n",
    "                got_page = False\n",
    "                new_names = False\n",
    "                link_index = 1\n",
    "                while new_names == False:\n",
    "                    try:\n",
    "                        if link_index == 1:\n",
    "                            element =driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[2]/div/ul/li/a')\n",
    "                        else: \n",
    "                            element =driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[2]/div/ul/li[' + str(link_index) + ']/a')            \n",
    "\n",
    "                        if last_name == element.text.split(', ')[0] and mid_first_name in element.text.split(', ')[1]:\n",
    "                            last_name = element.text.split(', ')[0]\n",
    "                            mid_first_name = element.text.split(', ')[1]\n",
    "                            new_names = True\n",
    "                    except: \n",
    "                        new_names = True\n",
    "                        got_page = True\n",
    "                        if not scraped_papers:\n",
    "                            print('Name ' + full_name + ' is not in the database. You may be missing a middle initial.')\n",
    "                            skip_df = skip_df.append(pd.DataFrame([[full_name, school, title]], columns = ['Full Name', 'School', 'Title']))\n",
    "                        else:\n",
    "                            print('No remaining pages to scrape from {}.'.format(full_name))\n",
    "                    link_index += 1\n",
    "            elif str(table_rows[1]) != '<td style=\"text-align:right;\"> 1 </td>': \n",
    "                element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]/div[1]')\n",
    "                table_element = element.text.split('\\n')\n",
    "                number_list = []\n",
    "                rank_list = []\n",
    "                stat_list = []\n",
    "                for stat in stats:\n",
    "                #     print(stat)\n",
    "                    find_index = [table_element.index(s) for s in table_element if stat == s]\n",
    "                    if find_index:\n",
    "                        my_list = table_element[find_index[0]+1].split(' ')\n",
    "                        number_list.append(my_list[0])\n",
    "                        stat_list.append(stat)\n",
    "                        if len(my_list) > 1:\n",
    "                            rank_list.append(my_list[-1])\n",
    "                    if stat == 'Self-Citations':\n",
    "                        find_index = [table_element.index(s) for s in table_element if stat in s]\n",
    "                        if find_index:\n",
    "                            stat_list.append(stat)\n",
    "                            number_list.append(table_element[find_index[0]].split(' ')[1])\n",
    "                zip_number_list = list(zip(stat_list, number_list))\n",
    "                zip_rank_list = list(zip(stat_list, rank_list))\n",
    "                number_dict = create_dataframe_dict(zip_number_list, full_name, school, 'number')\n",
    "                rank_dict = create_dataframe_dict(zip_rank_list, full_name, school, 'rank')\n",
    "                df_number = pd.DataFrame.from_dict(number_dict, orient='index').transpose()\n",
    "                df_rank = pd.DataFrame.from_dict(rank_dict, orient='index').transpose()\n",
    "                df_sub = pd.concat([df_number, df_rank])\n",
    "                main_df = pd.concat([main_df, df_sub])\n",
    "                title_index = 3\n",
    "#                 stats_index = 4\n",
    "#                 topic_index = 0\n",
    "#                 topic_div_index = 0\n",
    "                topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "                element = title_index\n",
    "                df = pd.DataFrame(columns = ['Title', 'Author(s)', 'Journal', 'Cited (articles)', 'Cited (cases)', 'Accessed', 'Topics'])\n",
    "                while element:\n",
    "                    data_stream = []\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                    element = driver.find_elements_by_xpath(x_path_title)\n",
    "                    #Get title:\n",
    "                    if element:\n",
    "                        scraped_papers = True\n",
    "                        for elm in element:\n",
    "                            data_stream.append(elm.text)\n",
    "\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]')\n",
    "                        for elm in element:\n",
    "                            my_list = elm.text\n",
    "                        if [a for a in my_list.split('\\n') if last_name in a]:   \n",
    "                            data_stream.append([a for a in my_list.split('\\n') if last_name in a][0])\n",
    "                        else:\n",
    "                            data_stream.append('na')\n",
    "                        if [a for a in my_list.split('\\n') if 'Vol.' in a]:\n",
    "                            data_stream.append([a for a in my_list.split('\\n') if 'Vol.' in a][0])\n",
    "                        else:\n",
    "                            data_stream.append('na')\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                        for elm in element:\n",
    "                            cited_text = elm.text\n",
    "                        article_citations = 'na'\n",
    "                        case_citations = 'na'\n",
    "                        accessed = 'na'\n",
    "                        if not isinstance(cited_text, list):\n",
    "                            cited_text = cited_text.split('\\n')\n",
    "                            cited_text\n",
    "                            for stat in cited_text:\n",
    "                                if 'Article' in stat:\n",
    "                                    article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                                if 'Case' in stat:\n",
    "                                    case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                                if 'Accessed' in stat:\n",
    "                                    accessed = int(re.search(r'\\d+', stat).group())\n",
    "                        data_stream.append(article_citations)\n",
    "                        data_stream.append(case_citations)\n",
    "                        data_stream.append(accessed)\n",
    "                        try:\n",
    "                            data_stream.append(topic_array[topic_div_index].text.split(':')[1])\n",
    "                        except:\n",
    "                            data_stream.append('na')\n",
    "                        df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'Journal', 'Cited (articles)', 'Cited (cases)', 'Accessed', 'Topics']))\n",
    "                        stats_index +=4\n",
    "                        title_index += 4\n",
    "                        topic_div_index +=1\n",
    "                        #Check that next paper exists:\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                        element = driver.find_elements_by_xpath(x_path_title)\n",
    "                df.to_csv(create_path('author_papers', '{}_{}_papers.csv'.format(full_name, school)))\n",
    "                got_page = False\n",
    "                new_names = False\n",
    "                link_index = 1\n",
    "                while new_names == False:\n",
    "                    try:\n",
    "                        if link_index == 1:\n",
    "                            element =driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[2]/div/ul/li/a')\n",
    "                        else: \n",
    "                            element =driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[2]/div/ul/li[' + str(link_index) + ']/a')            \n",
    "                        if last_name == element.text.split(', ')[0] and mid_first_name in element.text.split(', ')[1] and mid_first_name != prev_mid_first_name:\n",
    "                            prev_mid_first_name = mid_first_name\n",
    "                            last_name = element.text.split(', ')[0]\n",
    "                            mid_first_name = element.text.split(', ')[1]\n",
    "                            new_names = True\n",
    "                            full_name = mid_first_name + ' ' +  last_name\n",
    "                    except: \n",
    "                        new_names = True\n",
    "                        got_page = True\n",
    "                        print('No remaining pages to scrape from {}.'.format(full_name))\n",
    "                    link_index += 1\n",
    "                time.sleep(3)\n",
    "                \n",
    "skip_df.to_csv(create_path('skipped_names', '{}_skipped.csv'.format(school)))\n",
    "main_df.to_csv(create_path('school_stats', '{}_stats.csv'.format(school)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_name = 'Gubler'\n",
    "mid_first_name = 'Zachary'\n",
    "link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + mid_first_name + '&collection=journals'\n",
    "driver.get(link)\n",
    "soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "table_rows = soup.findAll('td', {'style': 'text-align:right;'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>School</th>\n",
       "      <th>Type</th>\n",
       "      <th>Cited by Cases</th>\n",
       "      <th>Cited by Articles</th>\n",
       "      <th>Accessed (Past 12 Months)</th>\n",
       "      <th>Cited by Articles (Past 10 Years)</th>\n",
       "      <th>Cited by Articles (Past 1-2 years)</th>\n",
       "      <th>ScholarCheck Rank</th>\n",
       "      <th>Average Citations per Article</th>\n",
       "      <th>Average Citations per Document</th>\n",
       "      <th>Self-Citations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James Weinstein</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>number</td>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "      <td>634</td>\n",
       "      <td>295</td>\n",
       "      <td>35</td>\n",
       "      <td></td>\n",
       "      <td>22.36</td>\n",
       "      <td>15.87</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James Weinstein</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>rank</td>\n",
       "      <td>17,210</td>\n",
       "      <td>2,164</td>\n",
       "      <td>1,761</td>\n",
       "      <td>1,264</td>\n",
       "      <td>837</td>\n",
       "      <td>2,104</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Person   School    Type Cited by Cases Cited by Articles  \\\n",
       "0  James Weinstein  Arizona  number              3               492   \n",
       "0  James Weinstein  Arizona    rank         17,210             2,164   \n",
       "\n",
       "  Accessed (Past 12 Months) Cited by Articles (Past 10 Years)  \\\n",
       "0                       634                               295   \n",
       "0                     1,761                             1,264   \n",
       "\n",
       "  Cited by Articles (Past 1-2 years) ScholarCheck Rank  \\\n",
       "0                                 35                     \n",
       "0                                837             2,104   \n",
       "\n",
       "  Average Citations per Article Average Citations per Document Self-Citations  \n",
       "0                         22.36                          15.87             80  \n",
       "0                                                                              "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = ['Cited by Cases','Cited by Articles','Accessed (Past 12 Months)','Cited by Articles (Past 10 Years)', 'Cited by Articles (Past 1-2 years)', 'ScholarCheck Rank', 'Average Citations per Article', 'Average Citations per Document', 'Self-Citations']\n",
    "last_name = 'Weinstein'\n",
    "mid_first_name = 'James'\n",
    "full_name = mid_first_name + ' ' + last_name\n",
    "school = 'Arizona'\n",
    "link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + mid_first_name + '&collection=journals'\n",
    "driver.get(link)\n",
    "element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]/div[1]')\n",
    "table_element = element.text.split('\\n')\n",
    "number_list = []\n",
    "rank_list = []\n",
    "stat_list = []\n",
    "for stat in stats:\n",
    "#     print(stat)\n",
    "    find_index = [table_element.index(s) for s in table_element if stat == s]\n",
    "    if find_index:\n",
    "        my_list = table_element[find_index[0]+1].split(' ')\n",
    "        number_list.append(my_list[0])\n",
    "        stat_list.append(stat)\n",
    "        if len(my_list) > 1:\n",
    "            rank_list.append(my_list[-1])\n",
    "    if stat == 'Self-Citations':\n",
    "        find_index = [table_element.index(s) for s in table_element if stat in s]\n",
    "        if find_index:\n",
    "            stat_list.append(stat)\n",
    "            number_list.append(table_element[find_index[0]].split(' ')[1])\n",
    "zip_number_list = list(zip(stat_list, number_list))\n",
    "zip_rank_list = list(zip(stat_list, rank_list))\n",
    "number_dict = create_dataframe_dict(zip_number_list, full_name, school, 'number')\n",
    "rank_dict = create_dataframe_dict(zip_rank_list, full_name, school, 'rank')\n",
    "df_number = pd.DataFrame.from_dict(number_dict, orient='index').transpose()\n",
    "df_rank = pd.DataFrame.from_dict(rank_dict, orient='index').transpose()\n",
    "df = pd.concat([df_number, df_rank])\n",
    "df\n",
    "# df = pd.DataFrame(columns = ['person', 'school', 'stat', 'number', 'rank'])\n",
    "# df['person'] = pd.Series([full_name]*len(number_list))\n",
    "# df['school'] = pd.Series([school]*len(number_list))\n",
    "# df['stat'] = pd.Series(stat_list)\n",
    "# df['number'] = pd.Series(number_list)\n",
    "# df['rank'] = pd.Series(rank_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>school</th>\n",
       "      <th>stat type</th>\n",
       "      <th>Cited by Cases</th>\n",
       "      <th>Cited by Articles</th>\n",
       "      <th>Accessed (Past 12 Months)</th>\n",
       "      <th>Cited by Articles (Past 10 Years)</th>\n",
       "      <th>Cited by Articles (Past 1-2 years)</th>\n",
       "      <th>ScholarCheck Rank</th>\n",
       "      <th>Average Citations per Article</th>\n",
       "      <th>Average Citations per Document</th>\n",
       "      <th>Self-Citations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [person, school, stat type, Cited by Cases, Cited by Articles, Accessed (Past 12 Months), Cited by Articles (Past 10 Years), Cited by Articles (Past 1-2 years), ScholarCheck Rank, Average Citations per Article, Average Citations per Document, Self-Citations]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_labels = ['person', 'school', 'stat type'] + stats\n",
    "df = pd.DataFrame(columns = column_labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cited by Articles', 'Accessed (Past 12 Months)', 'Cited by Articles (Past 10 Years)', 'Cited by Articles (Past 1-2 years)', 'ScholarCheck Rank', 'Average Citations per Article', 'Average Citations per Document', 'Self-Citations']\n",
      "['99', '212', '99', '18', '', '9.90', '9.00', '6']\n",
      "['10,679', '8,754', '4,126', '1,898', '7,876']\n"
     ]
    }
   ],
   "source": [
    "element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]/div[1]')\n",
    "stats = ['Cited by Cases','Cited by Articles','Accessed (Past 12 Months)','Cited by Articles (Past 10 Years)', 'Cited by Articles (Past 1-2 years)', 'ScholarCheck Rank', 'Average Citations per Article', 'Average Citations per Document', 'Self-Citations']\n",
    "table_element = element.text.split('\\n')\n",
    "number_list = []\n",
    "rank_list = []\n",
    "stat_list = []\n",
    "for stat in stats:\n",
    "#     print(stat)\n",
    "    find_index = [table_element.index(s) for s in table_element if stat == s]\n",
    "    if find_index:\n",
    "        my_list = table_element[find_index[0]+1].split(' ')\n",
    "        number_list.append(my_list[0])\n",
    "        stat_list.append(stat)\n",
    "        if len(my_list) > 1:\n",
    "            rank_list.append(my_list[-1])\n",
    "    if stat == 'Self-Citations':\n",
    "        find_index = [table_element.index(s) for s in table_element if stat in s]\n",
    "        stat_list.append(stat)\n",
    "        number_list.append(table_element[find_index[0]].split(' ')[1])\n",
    "\n",
    "print(stat_list)\n",
    "print(number_list)\n",
    "print(rank_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 'hi'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {}\n",
    "my_dict['hello'] = 'hi'\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
