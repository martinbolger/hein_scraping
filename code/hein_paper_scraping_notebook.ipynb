{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "web_scraping",
   "display_name": "web_scraping",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names\n",
    "from modules.data_manipulation_functions import remove_commas, check_files, concat_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "alt_names = pd.read_excel(intr_path / \"alt_names.xlsx\")\n",
    "data = alt_names\n",
    "data_type = \"lateral\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Darrell A. H.', 'Darrell A.H.']\n",
      "Looking for Darrell A. H. Miller\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Darrell A. H. Miller.\n",
      "Looking for Darrell A.H. Miller\n",
      "Either no data was found for Darrell A.H. Miller or the page has already been scraped. Moving to the next name.\n",
      "Paul Miller\n",
      "Name list: ['Paul B.']\n",
      "Looking for Paul B. Miller\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Paul B. Miller.\n",
      "Robert Miller\n",
      "Name list: ['Robert']\n",
      "Looking for Robert Miller\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Robert Miller.\n",
      "Nicholas Mirkay\n",
      "Name list: ['Nicholas A.']\n",
      "Looking for Nicholas A. Mirkay\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Nicholas A. Mirkay.\n",
      "Thomas Mitchell\n",
      "Name list: ['Thomas W.', 'Thomas']\n",
      "Looking for Thomas W. Mitchell\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Thomas W. Mitchell.\n",
      "Looking for Thomas Mitchell\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Thomas Mitchell.\n",
      "Seema Mohapatra\n",
      "Name list: ['Seema']\n",
      "Looking for Seema Mohapatra\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Seema Mohapatra.\n",
      "Edward Morrison\n",
      "Name list: ['Edward R.', 'Edward']\n",
      "Looking for Edward R. Morrison\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Edward R. Morrison.\n",
      "Looking for Edward Morrison\n",
      "Either no data was found for Edward Morrison or the page has already been scraped. Moving to the next name.\n",
      "Susan Morse\n",
      "Name list: ['Susan', 'Susan C.']\n",
      "Looking for Susan Morse\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Susan Morse.\n",
      "Looking for Susan C. Morse\n",
      "Either no data was found for Susan C. Morse or the page has already been scraped. Moving to the next name.\n",
      "Samuel Moyn\n",
      "Name list: ['Samuel']\n",
      "Looking for Samuel Moyn\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Samuel Moyn.\n",
      "Melissa Murray\n",
      "Name list: ['Melissa']\n",
      "Looking for Melissa Murray\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Melissa Murray.\n",
      "Alexandra Natapoff\n",
      "Name list: ['Alexandra']\n",
      "Looking for Alexandra Natapoff\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Alexandra Natapoff.\n",
      "Douglas NeJaime\n",
      "Name list: ['Douglas']\n",
      "Looking for Douglas NeJaime\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Douglas NeJaime.\n",
      "Xuan-Thao Nguyen\n",
      "Name list: ['Xuan-Thao']\n",
      "Looking for Xuan-Thao Nguyen\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Xuan-Thao Nguyen.\n",
      "Victoria Nourse\n",
      "Name list: ['Victoria', 'Victoria F.']\n",
      "Looking for Victoria Nourse\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Victoria Nourse.\n",
      "Looking for Victoria F. Nourse\n",
      "Either no data was found for Victoria F. Nourse or the page has already been scraped. Moving to the next name.\n",
      "Anne O'Connell\n",
      "Name list: ['Anne', 'Anne Joseph', 'Anne Joseph;']\n",
      "Looking for Anne O'Connell\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Anne O'Connell.\n",
      "Looking for Anne Joseph O'Connell\n",
      "Either no data was found for Anne Joseph O'Connell or the page has already been scraped. Moving to the next name.\n",
      "Looking for Anne Joseph; O'Connell\n",
      "Either no data was found for Anne Joseph; O'Connell or the page has already been scraped. Moving to the next name.\n",
      "Shu-Yi Oei\n",
      "Name list: ['Shu-Yi']\n",
      "Looking for Shu-Yi Oei\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Shu-Yi Oei.\n",
      "Paul Ohm\n",
      "Name list: ['Paul']\n",
      "Looking for Paul Ohm\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Paul Ohm.\n",
      "Ruth Okediji\n",
      "Name list: ['Ruth', 'Ruth Gana', 'Ruth L.']\n",
      "Looking for Ruth Okediji\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Ruth Okediji.\n",
      "Looking for Ruth Gana Okediji\n",
      "Either no data was found for Ruth Gana Okediji or the page has already been scraped. Moving to the next name.\n",
      "Looking for Ruth L. Okediji\n",
      "Either no data was found for Ruth L. Okediji or the page has already been scraped. Moving to the next name.\n",
      "Saule Omarove\n",
      "Name Saule Omarove was not found. Adding to the skipped names dataset.\n",
      "Angela Onwuachi-Willig\n",
      "Name list: ['Angela']\n",
      "Looking for Angela Onwuachi-Willig\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Angela Onwuachi-Willig.\n",
      "David Orentlicher\n",
      "Name list: ['David']\n",
      "Looking for David Orentlicher\n",
      "Scraping the page\n",
      "No remaining pages to scrape for David Orentlicher.\n",
      "Leigh Osofsky\n",
      "Name list: ['Leigh']\n",
      "Looking for Leigh Osofsky\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Leigh Osofsky.\n",
      "Dave Owen\n",
      "Name list: ['Dave']\n",
      "Looking for Dave Owen\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Dave Owen.\n",
      "Mary-Rose Papandrea\n",
      "Name list: ['Mary-Rose']\n",
      "Looking for Mary-Rose Papandrea\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Mary-Rose Papandrea.\n",
      "Jordan Paradise\n",
      "Name list: ['Jordan', 'Jordan K.']\n",
      "Looking for Jordan Paradise\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Jordan Paradise.\n",
      "Looking for Jordan K. Paradise\n",
      "Either no data was found for Jordan K. Paradise or the page has already been scraped. Moving to the next name.\n",
      "Rafael Pardo\n",
      "Name list: ['Rafael I.', 'Rafael (I)']\n",
      "Looking for Rafael I. Pardo\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Rafael I. Pardo.\n",
      "Looking for Rafael (I) Pardo\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Rafael (I) Pardo.\n",
      "James Park\n",
      "Name list: ['James']\n",
      "Looking for James Park\n",
      "Scraping the page\n",
      "No remaining pages to scrape for James Park.\n",
      "Ed Parson\n",
      "Name Ed Parson was not found. Adding to the skipped names dataset.\n",
      "Frank Partnoy\n",
      "Name list: ['Frank']\n",
      "Looking for Frank Partnoy\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Frank Partnoy.\n",
      "Frank Pasquale\n",
      "Name list: ['Frank']\n",
      "Looking for Frank Pasquale\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Frank Pasquale.\n",
      "Eduardo Penalver\n",
      "Name list: ['Eduardo', 'Eduardo M.']\n",
      "Looking for Eduardo Penalver\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Eduardo Penalver.\n",
      "Looking for Eduardo M. Penalver\n",
      "Either no data was found for Eduardo M. Penalver or the page has already been scraped. Moving to the next name.\n",
      "Dylan Penningroth\n",
      "Name list: ['Dylan C.']\n",
      "Looking for Dylan C. Penningroth\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Dylan C. Penningroth.\n",
      "Nathaniel Persily\n",
      "Name list: ['Nathaniel']\n",
      "Looking for Nathaniel Persily\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Nathaniel Persily.\n",
      "Gregg Polsky\n",
      "Name list: ['Gregg', 'Gregg D.']\n",
      "Looking for Gregg Polsky\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Gregg Polsky.\n",
      "Looking for Gregg D. Polsky\n",
      "Either no data was found for Gregg D. Polsky or the page has already been scraped. Moving to the next name.\n",
      "H. Powell\n",
      "Name list: ['H.']\n",
      "Looking for H. Powell\n",
      "Scraping the page\n",
      "No remaining pages to scrape for H. Powell.\n",
      "Wendell Pritchett\n",
      "Name list: ['Wendell E.', 'Wendell']\n",
      "Looking for Wendell E. Pritchett\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Wendell E. Pritchett.\n",
      "Looking for Wendell Pritchett\n",
      "Either no data was found for Wendell Pritchett or the page has already been scraped. Moving to the next name.\n",
      "Ellen Pryor\n",
      "Name list: ['Ellen', 'Ellen S.', 'Ellen Smith']\n",
      "Looking for Ellen Pryor\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Ellen Pryor.\n",
      "Looking for Ellen S. Pryor\n",
      "Either no data was found for Ellen S. Pryor or the page has already been scraped. Moving to the next name.\n",
      "Looking for Ellen Smith Pryor\n",
      "Either no data was found for Ellen Smith Pryor or the page has already been scraped. Moving to the next name.\n",
      "Scott Pryor\n",
      "Name list: ['Scott']\n",
      "Looking for Scott Pryor\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Scott Pryor.\n",
      "Jedediah Purdy\n",
      "Name list: ['Jedediah']\n",
      "Looking for Jedediah Purdy\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Jedediah Purdy.\n",
      "Intisar Rabb\n",
      "Name list: ['Intisar', 'Intisar A.']\n",
      "Looking for Intisar Rabb\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Intisar Rabb.\n",
      "Looking for Intisar A. Rabb\n",
      "Either no data was found for Intisar A. Rabb or the page has already been scraped. Moving to the next name.\n",
      "Srividhya Ragavan\n",
      "Name list: ['Srividhya']\n",
      "Looking for Srividhya Ragavan\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Srividhya Ragavan.\n",
      "Robert Rhee\n",
      "Name list: ['Robert J.']\n",
      "Looking for Robert J. Rhee\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Robert J. Rhee.\n",
      "L. Richardson\n",
      "Name list: ['L. Song']\n",
      "Looking for L. Song Richardson\n",
      "Scraping the page\n",
      "No remaining pages to scrape for L. Song Richardson.\n",
      "Alice Ristroph\n",
      "Name list: ['Alice']\n",
      "Looking for Alice Ristroph\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Alice Ristroph.\n",
      "Kalyani Robbins\n",
      "Name list: ['Kalyani']\n",
      "Looking for Kalyani Robbins\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Kalyani Robbins.\n",
      "Dorothy Roberts\n",
      "Name list: ['Dorothy', 'Dorothy E.']\n",
      "Looking for Dorothy Roberts\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Dorothy Roberts.\n",
      "Looking for Dorothy E. Roberts\n",
      "Either no data was found for Dorothy E. Roberts or the page has already been scraped. Moving to the next name.\n",
      "Edward Rock\n",
      "Name list: ['Edward', 'Edward B.']\n",
      "Looking for Edward Rock\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Edward Rock.\n",
      "Looking for Edward B. Rock\n",
      "Either no data was found for Edward B. Rock or the page has already been scraped. Moving to the next name.\n",
      "Cristina Rodriguez\n",
      "Name list: ['Cristina', 'Cristina M.']\n",
      "Looking for Cristina Rodriguez\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Cristina Rodriguez.\n",
      "Looking for Cristina M. Rodriguez\n",
      "Either no data was found for Cristina M. Rodriguez or the page has already been scraped. Moving to the next name.\n",
      "Christopher Roederer\n",
      "Name list: ['Christopher', 'Christopher J.']\n",
      "Looking for Christopher Roederer\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Christopher Roederer.\n",
      "Looking for Christopher J. Roederer\n",
      "Either no data was found for Christopher J. Roederer or the page has already been scraped. Moving to the next name.\n",
      "Benjamin Rooij\n",
      "Name Benjamin Rooij was not found. Adding to the skipped names dataset.\n",
      "Jim Rossi\n",
      "Name list: ['Jim']\n",
      "Looking for Jim Rossi\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Jim Rossi.\n",
      "Troy Rule\n",
      "Name list: ['Troy', 'Troy A.']\n",
      "Looking for Troy Rule\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Troy Rule.\n",
      "Looking for Troy A. Rule\n",
      "Either no data was found for Troy A. Rule or the page has already been scraped. Moving to the next name.\n",
      "Erin Ryan\n",
      "Name list: ['Erin']\n",
      "Looking for Erin Ryan\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Erin Ryan.\n",
      "Victoria Sahani\n",
      "Name list: ['Victoria Shannon']\n",
      "Looking for Victoria Shannon Sahani\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Victoria Shannon Sahani.\n",
      "Hillary Sale\n",
      "Name list: ['Hillary A.']\n",
      "Looking for Hillary A. Sale\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Hillary A. Sale.\n",
      "James Salzman\n",
      "Name list: ['James']\n",
      "Looking for James Salzman\n",
      "Scraping the page\n",
      "No remaining pages to scrape for James Salzman.\n",
      "Adam Samaha\n",
      "Name list: ['Adam M.']\n",
      "Looking for Adam M. Samaha\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Adam M. Samaha.\n",
      "Amy Schmitz\n",
      "Name list: ['Amy J.']\n",
      "Looking for Amy J. Schmitz\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Amy J. Schmitz.\n",
      "David Schwartz\n",
      "Name list: ['David', 'David L.']\n",
      "Looking for David Schwartz\n",
      "Scraping the page\n",
      "No remaining pages to scrape for David Schwartz.\n",
      "Looking for David L. Schwartz\n",
      "Scraping the page\n",
      "No remaining pages to scrape for David L. Schwartz.\n",
      "Christopher Serkin\n",
      "Name list: ['Christopher']\n",
      "Looking for Christopher Serkin\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Christopher Serkin.\n",
      "Gregory Shaffer\n",
      "Name list: ['Gregory']\n",
      "Looking for Gregory Shaffer\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Gregory Shaffer.\n",
      "Darien Shanske\n",
      "Name list: ['Darien']\n",
      "Looking for Darien Shanske\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Darien Shanske.\n",
      "Jessica Silbey\n",
      "Name list: ['Jessica']\n",
      "Looking for Jessica Silbey\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Jessica Silbey.\n",
      "Michael Simkovic\n",
      "Name list: ['Michael', 'Michael N.']\n",
      "Looking for Michael Simkovic\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Michael Simkovic.\n",
      "Looking for Michael N. Simkovic\n",
      "Either no data was found for Michael N. Simkovic or the page has already been scraped. Moving to the next name.\n",
      "Beth Simmons\n",
      "Name list: ['Beth', 'Beth A.']\n",
      "Looking for Beth Simmons\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Beth Simmons.\n",
      "Looking for Beth A. Simmons\n",
      "Either no data was found for Beth A. Simmons or the page has already been scraped. Moving to the next name.\n",
      "Kenneth Simons\n",
      "Name Kenneth Simons was not found. Adding to the skipped names dataset.\n",
      "David Sklansky\n",
      "Name list: ['David Alan', 'David A.']\n",
      "Looking for David Alan Sklansky\n",
      "Scraping the page\n",
      "No remaining pages to scrape for David Alan Sklansky.\n",
      "Looking for David A. Sklansky\n",
      "Either no data was found for David A. Sklansky or the page has already been scraped. Moving to the next name.\n",
      "Brad Snyder\n",
      "Name list: ['Brad']\n",
      "Looking for Brad Snyder\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Brad Snyder.\n",
      "A. Spencer\n",
      "Name list: ['A. Benjamin']\n",
      "Looking for A. Benjamin Spencer\n",
      "Scraping the page\n",
      "No remaining pages to scrape for A. Benjamin Spencer.\n",
      "Matthew Spitzer\n",
      "Name list: ['Matthew', 'Matthew L.']\n",
      "Looking for Matthew Spitzer\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Matthew Spitzer.\n",
      "Looking for Matthew L. Spitzer\n",
      "Either no data was found for Matthew L. Spitzer or the page has already been scraped. Moving to the next name.\n",
      "Christopher Sprigman\n",
      "Name list: ['Christopher', 'Christopher Jon']\n",
      "Looking for Christopher Sprigman\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Christopher Sprigman.\n",
      "Looking for Christopher Jon Sprigman\n",
      "Either no data was found for Christopher Jon Sprigman or the page has already been scraped. Moving to the next name.\n",
      "Paul Stancil\n",
      "Name list: ['Paul']\n",
      "Looking for Paul Stancil\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Paul Stancil.\n",
      "Jean Stefancic\n",
      "Name list: ['Jean']\n",
      "Looking for Jean Stefancic\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Jean Stefancic.\n",
      "Alex Stein\n",
      "Name list: ['Alex']\n",
      "Looking for Alex Stein\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Alex Stein.\n",
      "Adam Steinman\n",
      "Name list: ['Adam', 'Adam N.']\n",
      "Looking for Adam Steinman\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Adam Steinman.\n",
      "Looking for Adam N. Steinman\n",
      "Either no data was found for Adam N. Steinman or the page has already been scraped. Moving to the next name.\n",
      "Kristen Stilt\n",
      "Name list: ['Kristen', 'Kristen A.']\n",
      "Looking for Kristen Stilt\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Kristen Stilt.\n",
      "Looking for Kristen A. Stilt\n",
      "Either no data was found for Kristen A. Stilt or the page has already been scraped. Moving to the next name.\n",
      "David Studdert\n",
      "Name list: ['David M.', 'David']\n",
      "Looking for David M. Studdert\n",
      "Scraping the page\n",
      "No remaining pages to scrape for David M. Studdert.\n",
      "Looking for David Studdert\n",
      "Either no data was found for David Studdert or the page has already been scraped. Moving to the next name.\n",
      "Madhavi Sunder\n",
      "Name list: ['Madhavi']\n",
      "Looking for Madhavi Sunder\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Madhavi Sunder.\n",
      "Alan Sykes\n",
      "Name list: ['Alan', 'Alan O.']\n",
      "Looking for Alan Sykes\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Alan Sykes.\n",
      "Looking for Alan O. Sykes\n",
      "Either no data was found for Alan O. Sykes or the page has already been scraped. Moving to the next name.\n",
      "Eric Talley\n",
      "Name list: ['Eric', 'Eric L.']\n",
      "Looking for Eric Talley\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Eric Talley.\n",
      "Looking for Eric L. Talley\n",
      "Either no data was found for Eric L. Talley or the page has already been scraped. Moving to the next name.\n",
      "Kim Talus\n",
      "Name list: ['Kim']\n",
      "Looking for Kim Talus\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Kim Talus.\n",
      "Franita Tolson\n",
      "Name list: ['Franita']\n",
      "Looking for Franita Tolson\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Franita Tolson.\n",
      "Christopher Tomlins\n",
      "Name list: ['Christopher', 'Christopher L.']\n",
      "Looking for Christopher Tomlins\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Christopher Tomlins.\n",
      "Looking for Christopher L. Tomlins\n",
      "Either no data was found for Christopher L. Tomlins or the page has already been scraped. Moving to the next name.\n",
      "Gerald Torres\n",
      "Name list: ['Gerald']\n",
      "Looking for Gerald Torres\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Gerald Torres.\n",
      "Elizabeth Trujillo\n",
      "Name list: ['Elizabeth']\n",
      "Looking for Elizabeth Trujillo\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Elizabeth Trujillo.\n",
      "Deborah Tuerkheimer\n",
      "Name list: ['Deborah']\n",
      "Looking for Deborah Tuerkheimer\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Deborah Tuerkheimer.\n",
      "Rebecca Tushnet\n",
      "Name list: ['Rebecca']\n",
      "Looking for Rebecca Tushnet\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Rebecca Tushnet.\n",
      "Amanda Tyler\n",
      "Name list: ['Amanda L.']\n",
      "Looking for Amanda L. Tyler\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Amanda L. Tyler.\n",
      "Ryan Vacca\n",
      "Name list: ['Ryan', 'Ryan G.']\n",
      "Looking for Ryan Vacca\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Ryan Vacca.\n",
      "Looking for Ryan G. Vacca\n",
      "Either no data was found for Ryan G. Vacca or the page has already been scraped. Moving to the next name.\n",
      "Katharine Van Tassel\n",
      "Name list: ['Katharine']\n",
      "Looking for Katharine Van Tassel\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Katharine Van Tassel.\n",
      "Urska Velikonja\n",
      "Name list: ['Urska']\n",
      "Looking for Urska Velikonja\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Urska Velikonja.\n",
      "Rose Villazor\n",
      "Name list: ['Rose Cuison']\n",
      "Looking for Rose Cuison Villazor\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Rose Cuison Villazor.\n",
      "Steve Vladeck\n",
      "Name list: ['Steve']\n",
      "Looking for Steve Vladeck\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Steve Vladeck.\n",
      "Gina Warren\n",
      "Name list: ['Gina', 'Gina S.']\n",
      "Looking for Gina Warren\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Gina Warren.\n",
      "Looking for Gina S. Warren\n",
      "Either no data was found for Gina S. Warren or the page has already been scraped. Moving to the next name.\n",
      "Melissa Wasserman\n",
      "Name list: ['Melissa F.', 'Melissa Feeney']\n",
      "Looking for Melissa F. Wasserman\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Melissa F. Wasserman.\n",
      "Looking for Melissa Feeney Wasserman\n",
      "Either no data was found for Melissa Feeney Wasserman or the page has already been scraped. Moving to the next name.\n",
      "Nancy Welsh\n",
      "Name list: ['Nancy', 'Nancy A.']\n",
      "Looking for Nancy Welsh\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Nancy Welsh.\n",
      "Looking for Nancy A. Welsh\n",
      "Either no data was found for Nancy A. Welsh or the page has already been scraped. Moving to the next name.\n",
      "Robin Wilson\n",
      "Name list: ['Robin', 'Robin Fretwell']\n",
      "Looking for Robin Wilson\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Robin Wilson.\n",
      "Looking for Robin Fretwell Wilson\n",
      "Either no data was found for Robin Fretwell Wilson or the page has already been scraped. Moving to the next name.\n",
      "Andrew Woods\n",
      "Name list: ['Andrew Keane', 'Andrew K.']\n",
      "Looking for Andrew Keane Woods\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Andrew Keane Woods.\n",
      "Looking for Andrew K. Woods\n",
      "Either no data was found for Andrew K. Woods or the page has already been scraped. Moving to the next name.\n",
      "Kevin Woodson\n",
      "Name list: ['Kevin']\n",
      "Looking for Kevin Woodson\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Kevin Woodson.\n",
      "Del Wright\n",
      "Name list: ['Del Jr.']\n",
      "Looking for Del Jr. Wright\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Del Jr. Wright.\n",
      "Gideon Yaffe\n",
      "Name list: ['Gideon']\n",
      "Looking for Gideon Yaffe\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Gideon Yaffe.\n",
      "Ellen Yaroshefsky\n",
      "Name list: ['Ellen']\n",
      "Looking for Ellen Yaroshefsky\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Ellen Yaroshefsky.\n",
      "Ruqaiijah Yearby\n",
      "Name list: ['Ruqaiijah', 'Ruqaiijah A.']\n",
      "Looking for Ruqaiijah Yearby\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Ruqaiijah Yearby.\n",
      "Looking for Ruqaiijah A. Yearby\n",
      "Either no data was found for Ruqaiijah A. Yearby or the page has already been scraped. Moving to the next name.\n",
      "Peter Yu\n",
      "Name list: ['Peter K.']\n",
      "Looking for Peter K. Yu\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Peter K. Yu.\n",
      "Kathryn Zeiler\n",
      "Name list: ['Kathryn']\n",
      "Looking for Kathryn Zeiler\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Kathryn Zeiler.\n",
      "Sandi Zellmer\n",
      "Name list: ['Sandi B.']\n",
      "Looking for Sandi B. Zellmer\n",
      "Scraping the page\n",
      "No remaining pages to scrape for Sandi B. Zellmer.\n"
     ]
    }
   ],
   "source": [
    "# Initilization\n",
    "# Page name is a list of the name for all of the pages that we have scraped.\n",
    "# This is the name that actually appears on the webpage. This helps prevent\n",
    "# us from having to rescrape pages multiple times.\n",
    "page_name = []\n",
    "err_fm_names = []\n",
    "skip_df = pd.DataFrame()\n",
    "\n",
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    #This line gets the school URLs from the dataframe\n",
    "    if data_type == \"lateral\":\n",
    "        school_url = [data['Short URL Origin'][i], data['Short URL Destination'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "        new_school = data['Destination School'][i]\n",
    "    elif data_type == \"control\":\n",
    "        school_url = [data['Short URL Origin'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "\n",
    "    # If there were no matching names, the value is nan. This means that the value does not equal itself.\n",
    "    #  The name is added to the skipped names list and the loop moves onto the next name. \n",
    "    fm_names_str = data['fm_names'][i]\n",
    "    if fm_names_str != fm_names_str:\n",
    "        print('Name ' + full_name + ' was not found. Adding to the skipped names dataset.')\n",
    "        skip_df = pd.concat([skip_df, data.iloc[[i]]])\n",
    "        continue\n",
    "\n",
    "    fm_names = fm_names_str.split(\", \")\n",
    "    print(\"Name list: {}\".format(fm_names))   \n",
    "        \n",
    "    #This section loops through the list of alternative names and goes directly to their pages on Hein\n",
    "    for fm_name in fm_names:\n",
    "        # Create the full name\n",
    "        full_name = fm_name + ' ' +  last_name\n",
    "        print(\"Looking for {}\".format(full_name))\n",
    "        # Check if the file exists\n",
    "        file_name = '{}_{}_papers.xlsx'.format(full_name, prof_id)\n",
    "        file_path = out_path / file_name\n",
    "        if file_path.exists():\n",
    "            print(\"The file {} already exists. Moving to the next name.\".format(file_name))\n",
    "            continue\n",
    "\n",
    "        #Link to Hein page\n",
    "        link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals'\n",
    "        #Direct the webdriver to the page\n",
    "        driver.get(link)\n",
    "        #This function waits for the webpage to load\n",
    "        webpage_wait('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]', driver)\n",
    "        #This gets the page HTML\n",
    "        soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "        #This find the stat table at the top of the page\n",
    "        table_rows = soup.findAll('td', {'style': 'text-align:right;'})\n",
    "    \n",
    "        # This is the name for the professor that is used on the page.\n",
    "        cur_page = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]').text\n",
    "        #If there is a table on the page and the page name has not already appeared in the scraped list.\n",
    "        if table_rows and cur_page not in page_name: \n",
    "            element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]')\n",
    "            table_element = element.text.split('\\n')\n",
    "            #If the table is empty, there is no data to scrape\n",
    "            if len(table_element) < 5:\n",
    "                print('No data available on Hein for {} {}'.format(fm_name, last_name))\n",
    "                continue\n",
    "            #If the table is full, this section rearranges the data into a better format\n",
    "            else:    \n",
    "                print(\"Scraping the page\")                \n",
    "                #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\n",
    "                #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \n",
    "                #papers. The xpaths change when the page scrolls.\n",
    "                title_index = 3\n",
    "                stats_index = 4\n",
    "                topic_index = 0\n",
    "                scroll_num = 0\n",
    "                #This gets the page source\n",
    "                soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "                #This section gets the paper topics\n",
    "                topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "                element = title_index\n",
    "                df = pd.DataFrame(columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed'])\n",
    "                #This while loop will continue until there are no more papers on the page\n",
    "                while element:\n",
    "                    #Data stream is a list of the data in the paper data box (for example, authors, topics, journal)\n",
    "                    data_stream = []\n",
    "                    #This funciton returns a dictionary with various fields for each variable in the data box\n",
    "                    #Sometimes some of the variables are missing (for example, there are papers without a journal listed)\n",
    "                    #In this case, the dictionary returns an empty value for these variables\n",
    "                    data_dict = get_paper_data(last_name, prof_id, title_index, scroll_num, driver)\n",
    "                    #This section gets the paper stats box. This is the box that says how many citations the paper\n",
    "                    #has received\n",
    "                    if scroll_num == 0:\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                    elif scroll_num > 0:\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                    #This section extracts the data from the paper stats box\n",
    "                    for elm in element:\n",
    "                        cited_text = elm.text\n",
    "                    article_citations = 'na'\n",
    "                    case_citations = 'na'\n",
    "                    accessed = 'na'\n",
    "                    if not isinstance(cited_text, list):\n",
    "                        cited_text = cited_text.split('\\n')\n",
    "                        #This section finds the value for each paper stat\n",
    "                        for stat in cited_text:\n",
    "                            if 'Article' in stat:\n",
    "                                article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                            if 'Case' in stat:\n",
    "                                case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                            if 'Accessed' in stat:\n",
    "                                accessed = int(re.search(r'\\d+', stat).group())\n",
    "                    #The values are appended to the data_stream list\n",
    "                    data_stream.append(article_citations)\n",
    "                    data_stream.append(case_citations)\n",
    "                    data_stream.append(accessed)\n",
    "                    #This line adds the output from the function get_paper_data to the data_stream list\n",
    "                    data_stream = list(data_dict.values()) + data_stream\n",
    "                    #The data_stream list is used to add a line of data to the overall paper dataframe for this author\n",
    "                    df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed']), sort=False)\n",
    "                    #The indices are augmented to get the next paper\n",
    "                    stats_index +=4\n",
    "                    title_index += 4\n",
    "                    page_name.append(cur_page)\n",
    "                    #Check that next paper exists:\n",
    "                    if scroll_num == 0:\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                    #If the page has scrolled, the xpath we need to check has changed\n",
    "                    if scroll_num > 0:\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                    element = driver.find_elements_by_xpath(x_path_title)\n",
    "                    #If we can't find a next paper, it could be because we need to scroll again\n",
    "                    #This section attempts to scroll the page. \n",
    "                    if not element:\n",
    "                        scroll_num +=1\n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\n",
    "                        num_papers = int(box_element[0].text.split(' ')[0])\n",
    "                        #If there are more than 100 papers, we know there are still paper left to scrape\n",
    "                        if num_papers > 100*scroll_num:\n",
    "                            time.sleep(15)\n",
    "                            title_index = 3\n",
    "                            stats_index = 4\n",
    "                            topic_index = 0\n",
    "                            x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                            element = driver.find_elements_by_xpath(x_path_title)\n",
    "                #This line saves the Excel file of papers\n",
    "                df.to_excel(out_path / '{}_{}_papers.xlsx'.format(full_name, prof_id), index=False)\n",
    "                time.sleep(3)\n",
    "            #If we reach this point, all the pages for that author have been scraped\n",
    "            print('No remaining pages to scrape for {}.'.format(fm_name + ' ' + last_name))\n",
    "        else:\n",
    "            print(\"Either no data was found for {} or the page has already been scraped. Moving to the next name.\".format(full_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A. Benjamin Spencer_250_papers.xlsx\n",
      "Aaron-Andrew P. Bruhl_30_papers.xlsx\n",
      "Adam Badawi_10_papers.xlsx\n",
      "Adam J. Hirsch_116_papers.xlsx\n",
      "Adam M. Gershowitz_91_papers.xlsx\n",
      "Adam M. Samaha_238_papers.xlsx\n",
      "Adam Steinman_256_papers.xlsx\n",
      "Ajay K. Mehrotra_177_papers.xlsx\n",
      "Alan Sykes_260_papers.xlsx\n",
      "Alex Stein_255_papers.xlsx\n",
      "Alexandra Natapoff_192_papers.xlsx\n",
      "Alfred Brophy_26_papers.xlsx\n",
      "Alice Ristroph_225_papers.xlsx\n",
      "Allison K. Hoffman_117_papers.xlsx\n",
      "Amanda L. Tyler_269_papers.xlsx\n",
      "Amy J. Schmitz_239_papers.xlsx\n",
      "Amy Landers_150_papers.xlsx\n",
      "Andrei Marmor_168_papers.xlsx\n",
      "Andrew C.W. Lund_160_papers.xlsx\n",
      "Andrew Coan_51_papers.xlsx\n",
      "Andrew Gold_94_papers.xlsx\n",
      "Andrew Hessick_113_papers.xlsx\n",
      "Andrew Keane Woods_279_papers.xlsx\n",
      "Andrew Kull_148_papers.xlsx\n",
      "Andrew S. Gold_94_papers.xlsx\n",
      "Angela Banks_13_papers.xlsx\n",
      "Angela Onwuachi-Willig_201_papers.xlsx\n",
      "Ann Bartow_15_papers.xlsx\n",
      "Anne O'Connell_196_papers.xlsx\n",
      "Anu Bradford_22_papers.xlsx\n",
      "Anupam Chander_43_papers.xlsx\n",
      "Atiba R. Ellis_67_papers.xlsx\n",
      "Barry Cushman_55_papers.xlsx\n",
      "Bernadette Meyler_180_papers.xlsx\n",
      "Bernard E. Harcourt_104_papers.xlsx\n",
      "Beth Simmons_246_papers.xlsx\n",
      "Blake Hudson_124_papers.xlsx\n",
      "Brad Snyder_249_papers.xlsx\n",
      "Brandon Garrett_88_papers.xlsx\n",
      "Brett Frischmann_83_papers.xlsx\n",
      "Brian Galle_85_papers.xlsx\n",
      "Bruce A. Markell_167_papers.xlsx\n",
      "Bryant Garth_89_papers.xlsx\n",
      "C. Scott Hemphill_110_papers.xlsx\n",
      "Carissa Hessick_114_papers.xlsx\n",
      "Catherine Fisk_74_papers.xlsx\n",
      "Cathleen Kaveny_141_papers.xlsx\n",
      "Charles Brower_27_papers.xlsx\n",
      "Charles C. Jalloh_131_papers.xlsx\n",
      "Charlotte Ku_147_papers.xlsx\n",
      "Christine Cimini_48_papers.xlsx\n",
      "Christine Hurt_125_papers.xlsx\n",
      "Christopher Buccafusco_32_papers.xlsx\n",
      "Christopher M. Bruner_31_papers.xlsx\n",
      "Christopher M. Holman_119_papers.xlsx\n",
      "Christopher Roederer_230_papers.xlsx\n",
      "Christopher Serkin_241_papers.xlsx\n",
      "Christopher Sprigman_252_papers.xlsx\n",
      "Christopher Tomlins_264_papers.xlsx\n",
      "Cristina Rodriguez_229_papers.xlsx\n",
      "Curtis J. Milhaupt_181_papers.xlsx\n",
      "Dale Carpenter_40_papers.xlsx\n",
      "Dalie Jimenez_134_papers.xlsx\n",
      "Daniel Katz_140_papers.xlsx\n",
      "Darian M. Ibrahim_128_papers.xlsx\n",
      "Darien Shanske_243_papers.xlsx\n",
      "Darrell A. H. Miller_182_papers.xlsx\n",
      "Darren Hutchinson_126_papers.xlsx\n",
      "Dave Owen_204_papers.xlsx\n",
      "David Alan Sklansky_248_papers.xlsx\n",
      "David Fagundes_70_papers.xlsx\n",
      "David Franklyn_82_papers.xlsx\n",
      "David Gamage_86_papers.xlsx\n",
      "David Hasen_106_papers.xlsx\n",
      "David Hoffman_118_papers.xlsx\n",
      "David Hyman_127_papers.xlsx\n",
      "David L. Schwartz_240_papers.xlsx\n",
      "David M. Studdert_258_papers.xlsx\n",
      "David Marcus_166_papers.xlsx\n",
      "David Orentlicher_202_papers.xlsx\n",
      "David Schwartz_240_papers.xlsx\n",
      "Dayna Bowen Matthew_171_papers.xlsx\n",
      "Deborah Hellman_109_papers.xlsx\n",
      "Deborah Tuerkheimer_267_papers.xlsx\n",
      "Del Jr. Wright_281_papers.xlsx\n",
      "Dhammika Dharmapala_60_papers.xlsx\n",
      "Dorothy Roberts_227_papers.xlsx\n",
      "Douglas NeJaime_193_papers.xlsx\n",
      "Dylan C. Penningroth_213_papers.xlsx\n",
      "Eduardo Penalver_212_papers.xlsx\n",
      "Edward R. Morrison_188_papers.xlsx\n",
      "Edward Rock_228_papers.xlsx\n",
      "Eleanor Brown_28_papers.xlsx\n",
      "Elizabeth Trujillo_266_papers.xlsx\n",
      "Ellen Pryor_218_papers.xlsx\n",
      "Ellen Yaroshefsky_283_papers.xlsx\n",
      "Emily E. Kadens_137_papers.xlsx\n",
      "Emily Hammond_103_papers.xlsx\n",
      "Emily Kadens_137_papers.xlsx\n",
      "Eric A. Chiappinelli_46_papers.xlsx\n",
      "Eric Talley_261_papers.xlsx\n",
      "Erik Luna_159_papers.xlsx\n",
      "Erin Ryan_234_papers.xlsx\n",
      "Eugene Kontorovich_145_papers.xlsx\n",
      "Evan Criddle_54_papers.xlsx\n",
      "Fatma Marouf_169_papers.xlsx\n",
      "Francine Lipman_155_papers.xlsx\n",
      "Franita Tolson_263_papers.xlsx\n",
      "Frank Partnoy_210_papers.xlsx\n",
      "Frank Pasquale_211_papers.xlsx\n",
      "Gerald Torres_265_papers.xlsx\n",
      "Gideon Yaffe_282_papers.xlsx\n",
      "Gina Warren_275_papers.xlsx\n",
      "Glynn Lunney_161_papers.xlsx\n",
      "Grant M. Hayden_108_papers.xlsx\n",
      "Grayson McCouch_173_papers.xlsx\n",
      "Gregg Polsky_215_papers.xlsx\n",
      "Gregory Shaffer_242_papers.xlsx\n",
      "H. Powell_216_papers.xlsx\n",
      "Herbert Hovenkamp_122_papers.xlsx\n",
      "Hilary J. Allen_5_papers.xlsx\n",
      "Hillary A. Sale_236_papers.xlsx\n",
      "Intisar Rabb_221_papers.xlsx\n",
      "Irene Calboli_36_papers.xlsx\n",
      "Jacqueline D. Lipton_157_papers.xlsx\n",
      "Jacqueline Lipton_157_papers.xlsx\n",
      "James Gathii_90_papers.xlsx\n",
      "James Park_208_papers.xlsx\n",
      "James Salzman_237_papers.xlsx\n",
      "Janine Young Kim_143_papers.xlsx\n",
      "Jason J. Czarnezki_56_papers.xlsx\n",
      "Jason Mazzone_172_papers.xlsx\n",
      "Jean Stefancic_254_papers.xlsx\n",
      "Jeanne C. Fromer_84_papers.xlsx\n",
      "Jedediah Purdy_220_papers.xlsx\n",
      "Jennifer Chacon_42_papers.xlsx\n",
      "Jennifer Hendricks_111_papers.xlsx\n",
      "Jenny (I) Carroll_41_papers.xlsx\n",
      "Jenny E. Carroll_41_papers.xlsx\n",
      "Jessica Clarke_49_papers.xlsx\n",
      "Jessica Silbey_244_papers.xlsx\n",
      "Jill Horwitz_121_papers.xlsx\n",
      "Jill Wieber Lens_154_papers.xlsx\n",
      "Jim Chen_45_papers.xlsx\n",
      "Jim Rossi_232_papers.xlsx\n",
      "Joanna Grossman_98_papers.xlsx\n",
      "Jody S. Kraus_146_papers.xlsx\n",
      "Jonathan C. Lipson_156_papers.xlsx\n",
      "Jordan Paradise_206_papers.xlsx\n",
      "Jorge Contreras_52_papers.xlsx\n",
      "Joshua B. Fischman_73_papers.xlsx\n",
      "Joshua Blank_20_papers.xlsx\n",
      "Julie Andersen Hill_115_papers.xlsx\n",
      "June Carbone_38_papers.xlsx\n",
      "Justin Driver_64_papers.xlsx\n",
      "Justin McCrary_175_papers.xlsx\n",
      "Kaaryn Gustafson_99_papers.xlsx\n",
      "Kalyani Robbins_226_papers.xlsx\n",
      "Karen C. Burke_33_papers.xlsx\n",
      "Karima Bennoune_17_papers.xlsx\n",
      "Katharine Van Tassel_271_papers.xlsx\n",
      "Kathryn Zeiler_286_papers.xlsx\n",
      "Kenneth Ayotte_8_papers.xlsx\n",
      "Kevin Woodson_280_papers.xlsx\n",
      "Kim Talus_262_papers.xlsx\n",
      "Kimberly Ferzan_72_papers.xlsx\n",
      "Kristen Stilt_257_papers.xlsx\n",
      "Kristin Johnson_135_papers.xlsx\n",
      "Kurt Lash_151_papers.xlsx\n",
      "L. Song Richardson_224_papers.xlsx\n",
      "Lan Cao_37_papers.xlsx\n",
      "Lee Epstein_68_papers.xlsx\n",
      "Lee R. Epstein_68_papers.xlsx\n",
      "Leigh Osofsky_203_papers.xlsx\n",
      "Lenese Herbert_112_papers.xlsx\n",
      "Leslie Griffin_96_papers.xlsx\n",
      "Lisa T. Alexander_4_papers.xlsx\n",
      "Lolita Buckner Inniss_129_papers.xlsx\n",
      "Lucy Jewel_133_papers.xlsx\n",
      "Luis Chiesa_47_papers.xlsx\n",
      "Madhavi Sunder_259_papers.xlsx\n",
      "Margo A. Bagley_11_papers.xlsx\n",
      "Mary L. Dudziak_65_papers.xlsx\n",
      "Mary-Rose Papandrea_205_papers.xlsx\n",
      "Mathew D. McCubbins_176_papers.xlsx\n",
      "Matthew Adler_1_papers.xlsx\n",
      "Matthew Spitzer_251_papers.xlsx\n",
      "Melissa F. Wasserman_276_papers.xlsx\n",
      "Melissa J. Durkee_66_papers.xlsx\n",
      "Melissa Murray_191_papers.xlsx\n",
      "Michael Doran_63_papers.xlsx\n",
      "Michael Frakes_80_papers.xlsx\n",
      "Michael Hatfield_107_papers.xlsx\n",
      "Michael Kang_138_papers.xlsx\n",
      "Michael Simkovic_245_papers.xlsx\n",
      "Michele Goodwin_95_papers.xlsx\n",
      "Miranda Perry Fleischer_76_papers.xlsx\n",
      "Mitchell N. Berman_18_papers.xlsx\n",
      "Nancy Welsh_277_papers.xlsx\n",
      "Nathaniel Persily_214_papers.xlsx\n",
      "Nicholas A. Mirkay_185_papers.xlsx\n",
      "Nita A. Farahany_71_papers.xlsx\n",
      "Nita Farahany_71_papers.xlsx\n",
      "Nuno Garoupa_87_papers.xlsx\n",
      "Olufunmilayo B. Arewa_7_papers.xlsx\n",
      "Oren Bar-Gill_14_papers.xlsx\n",
      "Orin S. Kerr_142_papers.xlsx\n",
      "Owen L. Anderson_6_papers.xlsx\n",
      "Pamela R. Metzger_178_papers.xlsx\n",
      "Patricia A. McCoy_174_papers.xlsx\n",
      "Paul B. Miller_183_papers.xlsx\n",
      "Paul Butler_34_papers.xlsx\n",
      "Paul Caron_39_papers.xlsx\n",
      "Paul Ohm_198_papers.xlsx\n",
      "Paul Stancil_253_papers.xlsx\n",
      "Peter K. Yu_285_papers.xlsx\n",
      "Philip Hackney_101_papers.xlsx\n",
      "Rachel D. Godsil_93_papers.xlsx\n",
      "Rafael (I) Pardo_207_papers.xlsx\n",
      "Rafael I. Pardo_207_papers.xlsx\n",
      "Rebecca M. Kysar_149_papers.xlsx\n",
      "Rebecca Tushnet_268_papers.xlsx\n",
      "Renee Knake_144_papers.xlsx\n",
      "Richard Albert_3_papers.xlsx\n",
      "Richard Brooks_25_papers.xlsx\n",
      "Richard Delgado_59_papers.xlsx\n",
      "Robert H. II Jerry_132_papers.xlsx\n",
      "Robert J. MacCoun_163_papers.xlsx\n",
      "Robert J. Rhee_223_papers.xlsx\n",
      "Robert Jackson_130_papers.xlsx\n",
      "Robert Miller_184_papers.xlsx\n",
      "Robin Wilson_278_papers.xlsx\n",
      "RonNell Andersen Jones_136_papers.xlsx\n",
      "Rose Cuison Villazor_273_papers.xlsx\n",
      "Ruqaiijah Yearby_284_papers.xlsx\n",
      "Ruth Mason_170_papers.xlsx\n",
      "Ruth Okediji_199_papers.xlsx\n",
      "Ryan Vacca_270_papers.xlsx\n",
      "Sahar Aziz_9_papers.xlsx\n",
      "Sam Halabi_102_papers.xlsx\n",
      "Samuel Bray_23_papers.xlsx\n",
      "Samuel Moyn_190_papers.xlsx\n",
      "Sandi B. Zellmer_287_papers.xlsx\n",
      "Sarah C. Haan_100_papers.xlsx\n",
      "Sarah Lawsky_152_papers.xlsx\n",
      "Scott Dodson_62_papers.xlsx\n",
      "Scott Pryor_219_papers.xlsx\n",
      "Seema Mohapatra_187_papers.xlsx\n",
      "Seth Davis_58_papers.xlsx\n",
      "Shalanda H. Baker_12_papers.xlsx\n",
      "Sheila Foster_79_papers.xlsx\n",
      "Shi-Ling Hsu_123_papers.xlsx\n",
      "Shu-Yi Oei_197_papers.xlsx\n",
      "Shubha Ghosh_92_papers.xlsx\n",
      "Sonia Katyal_139_papers.xlsx\n",
      "Srividhya Ragavan_222_papers.xlsx\n",
      "Steve Vladeck_274_papers.xlsx\n",
      "Steven Davidoff_57_papers.xlsx\n",
      "Susan Fortney_78_papers.xlsx\n",
      "Susan Franck_81_papers.xlsx\n",
      "Susan Morse_189_papers.xlsx\n",
      "Suzette Malveaux_165_papers.xlsx\n",
      "Teri Dobbins Baxter_16_papers.xlsx\n",
      "Thomas Brennan_24_papers.xlsx\n",
      "Thomas Main_164_papers.xlsx\n",
      "Thomas Mitchell_186_papers.xlsx\n",
      "Thomas W. Mitchell_186_papers.xlsx\n",
      "Timothy Lytton_162_papers.xlsx\n",
      "Tomiko Brown-Nagin_29_papers.xlsx\n",
      "Troy Rule_233_papers.xlsx\n",
      "Urska Velikonja_272_papers.xlsx\n",
      "Victor Flatt_75_papers.xlsx\n",
      "Victor Fleischer_77_papers.xlsx\n",
      "Victoria Nourse_195_papers.xlsx\n",
      "Victoria Shannon Sahani_235_papers.xlsx\n",
      "W. Edward Afield_2_papers.xlsx\n",
      "Wendell E. Pritchett_217_papers.xlsx\n",
      "William Boyd_21_papers.xlsx\n",
      "William S. Dodge_61_papers.xlsx\n",
      "William W. Buzbee_35_papers.xlsx\n",
      "Woodrow Hartzog_105_papers.xlsx\n",
      "Xuan-Thao Nguyen_194_papers.xlsx\n",
      "Yoon-Ho Alex Lee_153_papers.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Stack the output files\n",
    "files = os.listdir(out_path)\n",
    "stacked_output = pd.DataFrame()\n",
    "for f in files:\n",
    "    print(f)\n",
    "    data = pd.read_excel(out_path / f, 'Sheet1')\n",
    "    data[\"file\"] = f\n",
    "    stacked_output = stacked_output.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate the data by ID so that we can see if all of the IDs are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_output.sort_values(by = [\"ID\"], inplace = True)\n",
    "stacked_output.to_excel(out_path / '_stacked_output.xlsx', index=False)\n",
    "skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}