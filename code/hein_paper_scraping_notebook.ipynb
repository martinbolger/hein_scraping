{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "web_scraping",
   "display_name": "web_scraping",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names\n",
    "from modules.data_manipulation_functions import remove_commas, check_files, concat_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "alt_names = pd.read_excel(intr_path / \"alt_names.xlsx\")\n",
    "data = alt_names\n",
    "data_type = \"lateral\"\n",
    "\n",
    "# Initilization\n",
    "page_name = []\n",
    "err_fm_names = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matthew Adler\n",
      "Name list: ['Matthew', 'Matthew D.']\n",
      "No remaining pages to scrape for Matthew Adler.\n",
      "Bernard Harcourt\n",
      "Name list: ['Bernard E.']\n",
      "No remaining pages to scrape for Bernard E. Harcourt.\n",
      "Justin McCrary\n",
      "Name list: ['Justin']\n",
      "No remaining pages to scrape for Justin McCrary.\n",
      "Eric Talley\n",
      "Name list: ['Eric', 'Eric L.']\n",
      "No remaining pages to scrape for Eric Talley.\n",
      "Nita Farahany\n",
      "Name list: ['Nita A.', 'Nita']\n",
      "No remaining pages to scrape for Nita A. Farahany.\n",
      "No remaining pages to scrape for Nita Farahany.\n",
      "Jody Kraus\n",
      "Name list: ['Jody S.', 'Jody']\n",
      "No remaining pages to scrape for Jody S. Kraus.\n",
      "Anu Bradford\n",
      "Name list: ['Anu']\n",
      "No remaining pages to scrape for Anu Bradford.\n",
      "Nita Farahany\n",
      "Name list: ['Nita A.', 'Nita']\n",
      "No remaining pages to scrape for Nita A. Farahany.\n",
      "No remaining pages to scrape for Nita Farahany.\n",
      "Jody Kraus\n",
      "Name list: ['Jody S.', 'Jody']\n",
      "No remaining pages to scrape for Jody S. Kraus.\n",
      "Anu Bradford\n",
      "Name list: ['Anu']\n",
      "No remaining pages to scrape for Anu Bradford.\n",
      "Darrell Miller\n",
      "Name list: ['Darrell A. H.', 'Darrell A.H.']\n",
      "No remaining pages to scrape for Darrell A. H. Miller.\n",
      "Mathew McCubbins\n",
      "Name list: ['Mathew D.']\n",
      "No remaining pages to scrape for Mathew D. McCubbins.\n",
      "Brandon Garrett\n",
      "Name list: ['Brandon']\n",
      "No remaining pages to scrape for Brandon Garrett.\n",
      "H. Powell\n",
      "Name list: ['H.']\n",
      "No remaining pages to scrape for H. Powell.\n",
      "Michael Frakes\n",
      "Name list: ['Michael']\n",
      "No remaining pages to scrape for Michael Frakes.\n"
     ]
    }
   ],
   "source": [
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    #This line gets the school URLs from the dataframe\n",
    "    if data_type == \"lateral\":\n",
    "        school_url = [data['Short URL Origin'][i], data['Short URL Destination'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "        new_school = data['Destination School'][i]\n",
    "    elif data_type == \"control\":\n",
    "        school_url = [data['Short URL Origin'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "\n",
    "    fm_names = data['fm_names'][i].split(\", \")\n",
    "    print(\"Name list: {}\".format(fm_names))\n",
    "    \n",
    "    # If there were no matching names, the name is added to the skipped names list and the loop moves onto the next name\n",
    "    # if not fm_names:\n",
    "    #     print('Name ' + full_name + ' was not found')\n",
    "    #     skip_df = skip_df.append(pd.DataFrame([[full_name, school, new_school, title]], columns = ['Full Name', 'School', 'New School', 'Title']), sort=False)\n",
    "        \n",
    "    #This section loops through the list of alternative names and goes directly to their pages on Hein\n",
    "    for fm_name in fm_names:\n",
    "        #Link to Hein page\n",
    "        link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals'\n",
    "        #Direct the webdriver to the page\n",
    "        driver.get(link)\n",
    "        #This function waits for the webpage to load\n",
    "        webpage_wait('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]', driver)\n",
    "        #This gets the page HTML\n",
    "        soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "        #This find the stat table at the top of the page\n",
    "        table_rows = soup.findAll('td', {'style': 'text-align:right;'})\n",
    "        #This gives the full name\n",
    "        full_name = fm_name + ' ' +  last_name\n",
    "        #This function checks the similar names list on the Hein page to append additional names\n",
    "        # fm_names, err_fm_names = similar_names(fm_names, err_fm_names, fm_name, last_name)\n",
    "\n",
    "        cur_page = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]').text\n",
    "        #If there is a table on the page\n",
    "        if table_rows and cur_page not in page_name: \n",
    "            element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]')\n",
    "            table_element = element.text.split('\\n')\n",
    "            #If the table is empty, there is no data to scrape\n",
    "            if len(table_element) < 5:\n",
    "                print('No data available on Hein for {} {}'.format(fm_name, last_name))\n",
    "            #If the table is full, this section rearranges the data into a better format\n",
    "            else:                    \n",
    "                #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\n",
    "                #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \n",
    "                #papers. The xpaths change when the page scrolls.\n",
    "                title_index = 3\n",
    "                stats_index = 4\n",
    "                topic_index = 0\n",
    "                scroll_num = 0\n",
    "                #This gets the page source\n",
    "                soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "                #This section gets the paper topics\n",
    "                topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "                element = title_index\n",
    "                page_name = []\n",
    "                df = pd.DataFrame(columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed'])\n",
    "                #This while loop will continue until there are no more papers on the page\n",
    "                while element:\n",
    "                    #Data stream is a list of the data in the paper data box (for example, authors, topics, journal)\n",
    "                    data_stream = []\n",
    "                    #This funciton returns a dictionary with various fields for each variable in the data box\n",
    "                    #Sometimes some of the variables are missing (for example, there are papers without a journal listed)\n",
    "                    #In this case, the dictionary returns an empty value for these variables\n",
    "                    data_dict = get_paper_data(last_name, prof_id, title_index, scroll_num, driver)\n",
    "                    #This section gets the paper stats box. This is the box that says how many citations the paper\n",
    "                    #has received\n",
    "                    if scroll_num == 0:\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                    elif scroll_num > 0:\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                    #This section extracts the data from the paper stats box\n",
    "                    for elm in element:\n",
    "                        cited_text = elm.text\n",
    "                    article_citations = 'na'\n",
    "                    case_citations = 'na'\n",
    "                    accessed = 'na'\n",
    "                    if not isinstance(cited_text, list):\n",
    "                        cited_text = cited_text.split('\\n')\n",
    "                        #This section finds the value for each paper stat\n",
    "                        for stat in cited_text:\n",
    "                            if 'Article' in stat:\n",
    "                                article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                            if 'Case' in stat:\n",
    "                                case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                            if 'Accessed' in stat:\n",
    "                                accessed = int(re.search(r'\\d+', stat).group())\n",
    "                    #The values are appended to the data_stream list\n",
    "                    data_stream.append(article_citations)\n",
    "                    data_stream.append(case_citations)\n",
    "                    data_stream.append(accessed)\n",
    "                    #This line adds the output from the function get_paper_data to the data_stream list\n",
    "                    data_stream = list(data_dict.values()) + data_stream\n",
    "                    #The data_stream list is used to add a line of data to the overall paper dataframe for this author\n",
    "                    df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed']), sort=False)\n",
    "                    #The indices are augmented to get the next paper\n",
    "                    stats_index +=4\n",
    "                    title_index += 4\n",
    "                    page_name.append(cur_page)\n",
    "                    #Check that next paper exists:\n",
    "                    if scroll_num == 0:\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                    #If the page has scrolled, the xpath we need to check has changed\n",
    "                    if scroll_num > 0:\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                    element = driver.find_elements_by_xpath(x_path_title)\n",
    "                    #If we can't find a next paper, it could be because we need to scroll again\n",
    "                    #This section attempts to scroll the page. \n",
    "                    if not element:\n",
    "                        scroll_num +=1\n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\n",
    "                        num_papers = int(box_element[0].text.split(' ')[0])\n",
    "                        #If there are more than 100 papers, we know there are still paper left to scrape\n",
    "                        if num_papers > 100*scroll_num:\n",
    "                            time.sleep(15)\n",
    "                            title_index = 3\n",
    "                            stats_index = 4\n",
    "                            topic_index = 0\n",
    "                            x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                            element = driver.find_elements_by_xpath(x_path_title)\n",
    "                #This line saves the Excel file of papers\n",
    "                df.to_excel(out_path / '{}_{}_papers.xlsx'.format(full_name, prof_id), index=False)\n",
    "                time.sleep(3)\n",
    "            #If we reach this point, all the pages for that author have been scraped\n",
    "            print('No remaining pages to scrape for {}.'.format(fm_name + ' ' + last_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Anu Bradford_22_papers.xlsx\nBernard E. Harcourt_104_papers.xlsx\nBrandon Garrett_88_papers.xlsx\nDarrell A. H. Miller_182_papers.xlsx\nEric Talley_261_papers.xlsx\nH. Powell_216_papers.xlsx\nJody S. Kraus_146_papers.xlsx\nJustin McCrary_175_papers.xlsx\nMathew D. McCubbins_176_papers.xlsx\nMatthew Adler_1_papers.xlsx\nMichael Frakes_80_papers.xlsx\nNita A. Farahany_71_papers.xlsx\nNita Farahany_71_papers.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Stack the output files\n",
    "files = os.listdir(out_path)\n",
    "stacked_output = pd.DataFrame()\n",
    "for f in files:\n",
    "    print(f)\n",
    "    data = pd.read_excel(out_path / f, 'Sheet1')\n",
    "    data[\"file\"] = f\n",
    "    stacked_output = stacked_output.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate the data by ID so that we can see if all of the IDs are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_output.sort_values(by = [\"ID\"], inplace = True)\n",
    "stacked_output.to_excel(out_path / '_stacked_output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}