{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "web_scraping",
   "display_name": "web_scraping",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names\n",
    "from modules.data_manipulation_functions import remove_commas, check_files, concat_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already exists. Names that have already been scraped will be skipped\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "input_data = pd.read_excel(intr_path / \"hein_scraping_input_data.xlsx\")\n",
    "data = input_data\n",
    "data_type = \"lateral\"\n",
    "\n",
    "# Create the list of scraped pages columns\n",
    "scraped_pages_columns = [\"links\", \"file_names\", \"professor_names\", \"id\"]\n",
    "# Load the list of scraped pages if it exists\n",
    "scraped_pages_file = out_path / \"_scraped_pages.xlsx\"\n",
    "if scraped_pages_file.exists():\n",
    "    print(\"Data already exists. Names that have already been scraped will be skipped\")\n",
    "    # Create the dataset of existing alt names.\n",
    "    df_scraped_pages = pd.read_excel(scraped_pages_file)\n",
    "else:\n",
    "    df_scraped_pages = pd.DataFrame(columns = scraped_pages_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                links  \\\n",
       "0   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "1   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "2   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "3   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "4   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "5   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "6   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "7   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "8   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "9   https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "10  https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "11  https://heinonline-org.proxy01.its.virginia.ed...   \n",
       "\n",
       "                             file_names      professor_names  id  \n",
       "0           Matthew Adler_1_papers.xlsx    Adler, Matthew D.   1  \n",
       "1        Matthew D. Adler_1_papers.xlsx    Adler, Matthew D.   1  \n",
       "2        W. Edward Afield_2_papers.xlsx    Afield, W. Edward   2  \n",
       "3          Richard Albert_3_papers.xlsx      Albert, Richard   3  \n",
       "4       Lisa T. Alexander_4_papers.xlsx   Alexander, Lisa T.   4  \n",
       "5         Hilary J. Allen_5_papers.xlsx     Allen, Hilary J.   5  \n",
       "6        Owen L. Anderson_6_papers.xlsx    Anderson, Owen L.   6  \n",
       "7   Olufunmilayo B. Arewa_7_papers.xlsx  Arewa, Olufunmilayo   7  \n",
       "8          Kenneth Ayotte_8_papers.xlsx   Ayotte, Kenneth M.   8  \n",
       "9       Kenneth M. Ayotte_8_papers.xlsx   Ayotte, Kenneth M.   8  \n",
       "10             Sahar Aziz_9_papers.xlsx          Aziz, Sahar   9  \n",
       "11          Sahar F. Aziz_9_papers.xlsx          Aziz, Sahar   9  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>links</th>\n      <th>file_names</th>\n      <th>professor_names</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Matthew Adler_1_papers.xlsx</td>\n      <td>Adler, Matthew D.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Matthew D. Adler_1_papers.xlsx</td>\n      <td>Adler, Matthew D.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>W. Edward Afield_2_papers.xlsx</td>\n      <td>Afield, W. Edward</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Richard Albert_3_papers.xlsx</td>\n      <td>Albert, Richard</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Lisa T. Alexander_4_papers.xlsx</td>\n      <td>Alexander, Lisa T.</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Hilary J. Allen_5_papers.xlsx</td>\n      <td>Allen, Hilary J.</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Owen L. Anderson_6_papers.xlsx</td>\n      <td>Anderson, Owen L.</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Olufunmilayo B. Arewa_7_papers.xlsx</td>\n      <td>Arewa, Olufunmilayo</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Kenneth Ayotte_8_papers.xlsx</td>\n      <td>Ayotte, Kenneth M.</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Kenneth M. Ayotte_8_papers.xlsx</td>\n      <td>Ayotte, Kenneth M.</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Sahar Aziz_9_papers.xlsx</td>\n      <td>Aziz, Sahar</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>https://heinonline-org.proxy01.its.virginia.ed...</td>\n      <td>Sahar F. Aziz_9_papers.xlsx</td>\n      <td>Aziz, Sahar</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "df_scraped_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matthew Adler\n",
      "Name list: ['Matthew', 'Matthew D.']\n",
      "Looking for Matthew Adler\n",
      "Scraping the page\n",
      "Done scraping for Matthew Adler.\n",
      "Looking for Matthew D. Adler\n",
      "A file for Matthew D. Adler already exists. Moving to the next name.\n",
      "Edward Afield\n",
      "Name list: ['W. Edward']\n",
      "Looking for W. Edward Afield\n",
      "Scraping the page\n",
      "Done scraping for W. Edward Afield.\n",
      "Richard Albert\n",
      "Name list: ['Richard']\n",
      "Looking for Richard Albert\n",
      "Scraping the page\n",
      "Done scraping for Richard Albert.\n",
      "Lisa Alexander\n",
      "Name list: ['Lisa T.']\n",
      "Looking for Lisa T. Alexander\n",
      "Scraping the page\n",
      "Done scraping for Lisa T. Alexander.\n",
      "Hilary Allen\n",
      "Name list: ['Hilary J.']\n",
      "Looking for Hilary J. Allen\n",
      "Scraping the page\n",
      "Done scraping for Hilary J. Allen.\n",
      "Owen Anderson\n",
      "Name list: ['Owen L.']\n",
      "Looking for Owen L. Anderson\n",
      "Scraping the page\n",
      "Done scraping for Owen L. Anderson.\n",
      "Olufunmilayo Arewa\n",
      "Name list: ['Olufunmilayo B.']\n",
      "Looking for Olufunmilayo B. Arewa\n",
      "Scraping the page\n",
      "Done scraping for Olufunmilayo B. Arewa.\n",
      "Kenneth Ayotte\n",
      "Name list: ['Kenneth', 'Kenneth M.']\n",
      "Looking for Kenneth Ayotte\n",
      "Scraping the page\n",
      "Done scraping for Kenneth Ayotte.\n",
      "Looking for Kenneth M. Ayotte\n",
      "A file for Kenneth M. Ayotte already exists. Moving to the next name.\n",
      "Sahar Aziz\n",
      "Name list: ['Sahar', 'Sahar F.']\n",
      "Looking for Sahar Aziz\n",
      "Scraping the page\n",
      "Done scraping for Sahar Aziz.\n",
      "Looking for Sahar F. Aziz\n",
      "A file for Sahar F. Aziz already exists. Moving to the next name.\n",
      "Adam Badawi\n",
      "Name list: ['Adam', 'Adam B.']\n",
      "Looking for Adam Badawi\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-e06188a05585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#Direct the webdriver to the page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;31m#This function waits for the webpage to load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mwebpage_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             return self.request_encode_body(\n\u001b[0m\u001b[0;32m     80\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initilization\n",
    "# Page name is a list of the name for all of the pages that we have scraped.\n",
    "# This is the name that actually appears on the webpage. This helps prevent\n",
    "# us from having to rescrape pages multiple times.\n",
    "err_fm_names = []\n",
    "skip_df = pd.DataFrame()\n",
    "\n",
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    df_scraped_pages.to_excel(scraped_pages_file, index = False)\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    #This line gets the school URLs from the dataframe\n",
    "    if data_type == \"lateral\":\n",
    "        school_url = [data['Short URL Origin'][i], data['Short URL Destination'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "        new_school = data['Destination School'][i]\n",
    "    elif data_type == \"control\":\n",
    "        school_url = [data['Short URL Origin'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "\n",
    "    # If there were no matching names, the value is nan. This means that the value does not equal itself.\n",
    "    #  The name is added to the skipped names list and the loop moves onto the next name. \n",
    "    fm_names_str = data['fm_names'][i]\n",
    "    if fm_names_str != fm_names_str:\n",
    "        print('Name ' + full_name + ' was not found. Adding to the skipped names dataset.')\n",
    "        skip_df = pd.concat([skip_df, data.iloc[[i]]])\n",
    "        continue\n",
    "\n",
    "    fm_names = fm_names_str.split(\", \")\n",
    "    print(\"Name list: {}\".format(fm_names))   \n",
    "        \n",
    "    #This section loops through the list of alternative names and goes directly to their pages on Hein\n",
    "    for fm_name in fm_names:\n",
    "        # Create the full name\n",
    "        full_name = fm_name + ' ' +  last_name\n",
    "        print(\"Looking for {}\".format(full_name))\n",
    "        # Check if the file exists\n",
    "        file_name = '{}_{}_papers.xlsx'.format(full_name, prof_id)            \n",
    "\n",
    "        #Link to Hein page\n",
    "        link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals'\n",
    "\n",
    "        # CHECK DATA: This is the first spot where we check the data. We want to see if the link that we are\n",
    "        # going to has already been added to the list of scraped pages. This is helpful when rerunning the code.\n",
    "        if not df_scraped_pages.query('@link == links').empty:\n",
    "            print(\"The link {} has already been scraped. Moving to the next name.\".format(link))\n",
    "            continue\n",
    "        #Direct the webdriver to the page\n",
    "        driver.get(link)\n",
    "        #This function waits for the webpage to load\n",
    "        webpage_wait('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]', driver)\n",
    "        #This gets the page HTML\n",
    "        soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "        #This find the stat table at the top of the page\n",
    "        table_rows = soup.findAll('td', {'style': 'text-align:right;'})\n",
    "    \n",
    "        # This is the name for the professor that is used on the page.\n",
    "        cur_page = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]').text\n",
    "\n",
    "        # CHECK DATA: This is the second spot where we check the data to see if the page has\n",
    "        # already been scraped. In order to check to see if the link has already been scraped, \n",
    "        # we look for the professor name on the page (which may be different from the name in our list) \n",
    "        # and the professor ID. This is helpful when two names point to the same page.\n",
    "        if not df_scraped_pages.query('@cur_page == professor_names and @prof_id == id').empty:\n",
    "            print(\"A file for {} already exists. Moving to the next name.\".format(full_name))\n",
    "            # Add the link to the data so that we know to skip it in future runs\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            continue\n",
    "        #If there is a table on the page and the page name has not already appeared in the scraped list.\n",
    "        if table_rows: \n",
    "            element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]')\n",
    "            table_element = element.text.split('\\n')\n",
    "            #If the table is empty, there is no data to scrape\n",
    "            if len(table_element) < 5:\n",
    "                print('No data available on Hein for {} {}'.format(fm_name, last_name))\n",
    "                continue\n",
    "            #If the table is full, this section rearranges the data into a better format\n",
    "            print(\"Scraping the page\")                \n",
    "            #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\n",
    "            #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \n",
    "            #papers. The xpaths change when the page scrolls.\n",
    "            title_index = 3\n",
    "            stats_index = 4\n",
    "            topic_index = 0\n",
    "            scroll_num = 0\n",
    "            #This gets the page source\n",
    "            soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "            #This section gets the paper topics\n",
    "            topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "            element = title_index\n",
    "            df = pd.DataFrame(columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed'])\n",
    "            #This while loop will continue until there are no more papers on the page\n",
    "            while element:\n",
    "                #Data stream is a list of the data in the paper data box (for example, authors, topics, journal)\n",
    "                data_stream = []\n",
    "                #This funciton returns a dictionary with various fields for each variable in the data box\n",
    "                #Sometimes some of the variables are missing (for example, there are papers without a journal listed)\n",
    "                #In this case, the dictionary returns an empty value for these variables\n",
    "                data_dict = get_paper_data(last_name, prof_id, title_index, scroll_num, driver)\n",
    "                #This section gets the paper stats box. This is the box that says how many citations the paper\n",
    "                #has received\n",
    "                if scroll_num == 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                elif scroll_num > 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                #This section extracts the data from the paper stats box\n",
    "                for elm in element:\n",
    "                    cited_text = elm.text\n",
    "                article_citations = 'na'\n",
    "                case_citations = 'na'\n",
    "                accessed = 'na'\n",
    "                if not isinstance(cited_text, list):\n",
    "                    cited_text = cited_text.split('\\n')\n",
    "                    #This section finds the value for each paper stat\n",
    "                    for stat in cited_text:\n",
    "                        if 'Article' in stat:\n",
    "                            article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Case' in stat:\n",
    "                            case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Accessed' in stat:\n",
    "                            accessed = int(re.search(r'\\d+', stat).group())\n",
    "                #The values are appended to the data_stream list\n",
    "                data_stream.append(article_citations)\n",
    "                data_stream.append(case_citations)\n",
    "                data_stream.append(accessed)\n",
    "                #This line adds the output from the function get_paper_data to the data_stream list\n",
    "                data_stream = list(data_dict.values()) + data_stream\n",
    "                #The data_stream list is used to add a line of data to the overall paper dataframe for this author\n",
    "                df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed']), sort=False)\n",
    "                #The indices are augmented to get the next paper\n",
    "                stats_index +=4\n",
    "                title_index += 4\n",
    "                #Check that next paper exists:\n",
    "                if scroll_num == 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                #If the page has scrolled, the xpath we need to check has changed\n",
    "                if scroll_num > 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                element = driver.find_elements_by_xpath(x_path_title)\n",
    "                #If we can't find a next paper, it could be because we need to scroll again\n",
    "                #This section attempts to scroll the page. \n",
    "                if not element:\n",
    "                    scroll_num +=1\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\n",
    "                    num_papers = int(box_element[0].text.split(' ')[0])\n",
    "                    #If there are more than 100 papers, we know there are still paper left to scrape\n",
    "                    if num_papers > 100*scroll_num:\n",
    "                        time.sleep(15)\n",
    "                        title_index = 3\n",
    "                        stats_index = 4\n",
    "                        topic_index = 0\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                        element = driver.find_elements_by_xpath(x_path_title)\n",
    "            #This line saves the Excel file of papers\n",
    "            df.to_excel(out_path / '{}_{}_papers.xlsx'.format(full_name, prof_id), index=False)\n",
    "            # We have created a file, so we need to append the link and the file name to the list of scraped pages\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            time.sleep(3)\n",
    "            #If we reach this point, all the pages for that author have been scraped\n",
    "            print('Done scraping for {}.'.format(fm_name + ' ' + last_name))\n",
    "        else:\n",
    "            print(\"No data was found for {}. Moving to the next name.\".format(full_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A. Benjamin Spencer_250_papers.xlsx\n",
      "Aaron-Andrew P. Bruhl_30_papers.xlsx\n",
      "Adam Badawi_10_papers.xlsx\n",
      "Adam J. Hirsch_116_papers.xlsx\n",
      "Adam M. Gershowitz_91_papers.xlsx\n",
      "Adam M. Samaha_238_papers.xlsx\n",
      "Adam Steinman_256_papers.xlsx\n",
      "Ajay K. Mehrotra_177_papers.xlsx\n",
      "Alan Sykes_260_papers.xlsx\n",
      "Alex Stein_255_papers.xlsx\n",
      "Alexandra Natapoff_192_papers.xlsx\n",
      "Alfred Brophy_26_papers.xlsx\n",
      "Alice Ristroph_225_papers.xlsx\n",
      "Allison K. Hoffman_117_papers.xlsx\n",
      "Amanda L. Tyler_269_papers.xlsx\n",
      "Amy J. Schmitz_239_papers.xlsx\n",
      "Amy Landers_150_papers.xlsx\n",
      "Andrei Marmor_168_papers.xlsx\n",
      "Andrew C.W. Lund_160_papers.xlsx\n",
      "Andrew Coan_51_papers.xlsx\n",
      "Andrew Gold_94_papers.xlsx\n",
      "Andrew Hessick_113_papers.xlsx\n",
      "Andrew Keane Woods_279_papers.xlsx\n",
      "Andrew Kull_148_papers.xlsx\n",
      "Andrew S. Gold_94_papers.xlsx\n",
      "Angela Banks_13_papers.xlsx\n",
      "Angela Onwuachi-Willig_201_papers.xlsx\n",
      "Ann Bartow_15_papers.xlsx\n",
      "Anne O'Connell_196_papers.xlsx\n",
      "Anu Bradford_22_papers.xlsx\n",
      "Anupam Chander_43_papers.xlsx\n",
      "Atiba R. Ellis_67_papers.xlsx\n",
      "Barry Cushman_55_papers.xlsx\n",
      "Bernadette Meyler_180_papers.xlsx\n",
      "Bernard E. Harcourt_104_papers.xlsx\n",
      "Beth Simmons_246_papers.xlsx\n",
      "Blake Hudson_124_papers.xlsx\n",
      "Brad Snyder_249_papers.xlsx\n",
      "Brandon Garrett_88_papers.xlsx\n",
      "Brett Frischmann_83_papers.xlsx\n",
      "Brian Galle_85_papers.xlsx\n",
      "Bruce A. Markell_167_papers.xlsx\n",
      "Bryant Garth_89_papers.xlsx\n",
      "C. Scott Hemphill_110_papers.xlsx\n",
      "Carissa Hessick_114_papers.xlsx\n",
      "Catherine Fisk_74_papers.xlsx\n",
      "Cathleen Kaveny_141_papers.xlsx\n",
      "Charles Brower_27_papers.xlsx\n",
      "Charles C. Jalloh_131_papers.xlsx\n",
      "Charlotte Ku_147_papers.xlsx\n",
      "Christine Cimini_48_papers.xlsx\n",
      "Christine Hurt_125_papers.xlsx\n",
      "Christopher Buccafusco_32_papers.xlsx\n",
      "Christopher M. Bruner_31_papers.xlsx\n",
      "Christopher M. Holman_119_papers.xlsx\n",
      "Christopher Roederer_230_papers.xlsx\n",
      "Christopher Serkin_241_papers.xlsx\n",
      "Christopher Sprigman_252_papers.xlsx\n",
      "Christopher Tomlins_264_papers.xlsx\n",
      "Cristina Rodriguez_229_papers.xlsx\n",
      "Curtis J. Milhaupt_181_papers.xlsx\n",
      "Dale Carpenter_40_papers.xlsx\n",
      "Dalie Jimenez_134_papers.xlsx\n",
      "Daniel Katz_140_papers.xlsx\n",
      "Darian M. Ibrahim_128_papers.xlsx\n",
      "Darien Shanske_243_papers.xlsx\n",
      "Darrell A. H. Miller_182_papers.xlsx\n",
      "Darren Hutchinson_126_papers.xlsx\n",
      "Dave Owen_204_papers.xlsx\n",
      "David Alan Sklansky_248_papers.xlsx\n",
      "David Fagundes_70_papers.xlsx\n",
      "David Franklyn_82_papers.xlsx\n",
      "David Gamage_86_papers.xlsx\n",
      "David Hasen_106_papers.xlsx\n",
      "David Hoffman_118_papers.xlsx\n",
      "David Hyman_127_papers.xlsx\n",
      "David L. Schwartz_240_papers.xlsx\n",
      "David M. Studdert_258_papers.xlsx\n",
      "David Marcus_166_papers.xlsx\n",
      "David Orentlicher_202_papers.xlsx\n",
      "David Schwartz_240_papers.xlsx\n",
      "Dayna Bowen Matthew_171_papers.xlsx\n",
      "Deborah Hellman_109_papers.xlsx\n",
      "Deborah Tuerkheimer_267_papers.xlsx\n",
      "Del Jr. Wright_281_papers.xlsx\n",
      "Dhammika Dharmapala_60_papers.xlsx\n",
      "Dorothy Roberts_227_papers.xlsx\n",
      "Douglas NeJaime_193_papers.xlsx\n",
      "Dylan C. Penningroth_213_papers.xlsx\n",
      "Eduardo Penalver_212_papers.xlsx\n",
      "Edward R. Morrison_188_papers.xlsx\n",
      "Edward Rock_228_papers.xlsx\n",
      "Eleanor Brown_28_papers.xlsx\n",
      "Elizabeth Trujillo_266_papers.xlsx\n",
      "Ellen Pryor_218_papers.xlsx\n",
      "Ellen Yaroshefsky_283_papers.xlsx\n",
      "Emily E. Kadens_137_papers.xlsx\n",
      "Emily Hammond_103_papers.xlsx\n",
      "Emily Kadens_137_papers.xlsx\n",
      "Eric A. Chiappinelli_46_papers.xlsx\n",
      "Eric Talley_261_papers.xlsx\n",
      "Erik Luna_159_papers.xlsx\n",
      "Erin Ryan_234_papers.xlsx\n",
      "Eugene Kontorovich_145_papers.xlsx\n",
      "Evan Criddle_54_papers.xlsx\n",
      "Fatma Marouf_169_papers.xlsx\n",
      "Francine Lipman_155_papers.xlsx\n",
      "Franita Tolson_263_papers.xlsx\n",
      "Frank Partnoy_210_papers.xlsx\n",
      "Frank Pasquale_211_papers.xlsx\n",
      "Gerald Torres_265_papers.xlsx\n",
      "Gideon Yaffe_282_papers.xlsx\n",
      "Gina Warren_275_papers.xlsx\n",
      "Glynn Lunney_161_papers.xlsx\n",
      "Grant M. Hayden_108_papers.xlsx\n",
      "Grayson McCouch_173_papers.xlsx\n",
      "Gregg Polsky_215_papers.xlsx\n",
      "Gregory Shaffer_242_papers.xlsx\n",
      "H. Powell_216_papers.xlsx\n",
      "Herbert Hovenkamp_122_papers.xlsx\n",
      "Hilary J. Allen_5_papers.xlsx\n",
      "Hillary A. Sale_236_papers.xlsx\n",
      "Intisar Rabb_221_papers.xlsx\n",
      "Irene Calboli_36_papers.xlsx\n",
      "Jacqueline D. Lipton_157_papers.xlsx\n",
      "Jacqueline Lipton_157_papers.xlsx\n",
      "James Gathii_90_papers.xlsx\n",
      "James Park_208_papers.xlsx\n",
      "James Salzman_237_papers.xlsx\n",
      "Janine Young Kim_143_papers.xlsx\n",
      "Jason J. Czarnezki_56_papers.xlsx\n",
      "Jason Mazzone_172_papers.xlsx\n",
      "Jean Stefancic_254_papers.xlsx\n",
      "Jeanne C. Fromer_84_papers.xlsx\n",
      "Jedediah Purdy_220_papers.xlsx\n",
      "Jennifer Chacon_42_papers.xlsx\n",
      "Jennifer Hendricks_111_papers.xlsx\n",
      "Jenny (I) Carroll_41_papers.xlsx\n",
      "Jenny E. Carroll_41_papers.xlsx\n",
      "Jessica Clarke_49_papers.xlsx\n",
      "Jessica Silbey_244_papers.xlsx\n",
      "Jill Horwitz_121_papers.xlsx\n",
      "Jill Wieber Lens_154_papers.xlsx\n",
      "Jim Chen_45_papers.xlsx\n",
      "Jim Rossi_232_papers.xlsx\n",
      "Joanna Grossman_98_papers.xlsx\n",
      "Jody S. Kraus_146_papers.xlsx\n",
      "Jonathan C. Lipson_156_papers.xlsx\n",
      "Jordan Paradise_206_papers.xlsx\n",
      "Jorge Contreras_52_papers.xlsx\n",
      "Joshua B. Fischman_73_papers.xlsx\n",
      "Joshua Blank_20_papers.xlsx\n",
      "Julie Andersen Hill_115_papers.xlsx\n",
      "June Carbone_38_papers.xlsx\n",
      "Justin Driver_64_papers.xlsx\n",
      "Justin McCrary_175_papers.xlsx\n",
      "Kaaryn Gustafson_99_papers.xlsx\n",
      "Kalyani Robbins_226_papers.xlsx\n",
      "Karen C. Burke_33_papers.xlsx\n",
      "Karima Bennoune_17_papers.xlsx\n",
      "Katharine Van Tassel_271_papers.xlsx\n",
      "Kathryn Zeiler_286_papers.xlsx\n",
      "Kenneth Ayotte_8_papers.xlsx\n",
      "Kevin Woodson_280_papers.xlsx\n",
      "Kim Talus_262_papers.xlsx\n",
      "Kimberly Ferzan_72_papers.xlsx\n",
      "Kristen Stilt_257_papers.xlsx\n",
      "Kristin Johnson_135_papers.xlsx\n",
      "Kurt Lash_151_papers.xlsx\n",
      "L. Song Richardson_224_papers.xlsx\n",
      "Lan Cao_37_papers.xlsx\n",
      "Lee Epstein_68_papers.xlsx\n",
      "Lee R. Epstein_68_papers.xlsx\n",
      "Leigh Osofsky_203_papers.xlsx\n",
      "Lenese Herbert_112_papers.xlsx\n",
      "Leslie Griffin_96_papers.xlsx\n",
      "Lisa T. Alexander_4_papers.xlsx\n",
      "Lolita Buckner Inniss_129_papers.xlsx\n",
      "Lucy Jewel_133_papers.xlsx\n",
      "Luis Chiesa_47_papers.xlsx\n",
      "Madhavi Sunder_259_papers.xlsx\n",
      "Margo A. Bagley_11_papers.xlsx\n",
      "Mary L. Dudziak_65_papers.xlsx\n",
      "Mary-Rose Papandrea_205_papers.xlsx\n",
      "Mathew D. McCubbins_176_papers.xlsx\n",
      "Matthew Adler_1_papers.xlsx\n",
      "Matthew Spitzer_251_papers.xlsx\n",
      "Melissa F. Wasserman_276_papers.xlsx\n",
      "Melissa J. Durkee_66_papers.xlsx\n",
      "Melissa Murray_191_papers.xlsx\n",
      "Michael Doran_63_papers.xlsx\n",
      "Michael Frakes_80_papers.xlsx\n",
      "Michael Hatfield_107_papers.xlsx\n",
      "Michael Kang_138_papers.xlsx\n",
      "Michael Simkovic_245_papers.xlsx\n",
      "Michele Goodwin_95_papers.xlsx\n",
      "Miranda Perry Fleischer_76_papers.xlsx\n",
      "Mitchell N. Berman_18_papers.xlsx\n",
      "Nancy Welsh_277_papers.xlsx\n",
      "Nathaniel Persily_214_papers.xlsx\n",
      "Nicholas A. Mirkay_185_papers.xlsx\n",
      "Nita A. Farahany_71_papers.xlsx\n",
      "Nita Farahany_71_papers.xlsx\n",
      "Nuno Garoupa_87_papers.xlsx\n",
      "Olufunmilayo B. Arewa_7_papers.xlsx\n",
      "Oren Bar-Gill_14_papers.xlsx\n",
      "Orin S. Kerr_142_papers.xlsx\n",
      "Owen L. Anderson_6_papers.xlsx\n",
      "Pamela R. Metzger_178_papers.xlsx\n",
      "Patricia A. McCoy_174_papers.xlsx\n",
      "Paul B. Miller_183_papers.xlsx\n",
      "Paul Butler_34_papers.xlsx\n",
      "Paul Caron_39_papers.xlsx\n",
      "Paul Ohm_198_papers.xlsx\n",
      "Paul Stancil_253_papers.xlsx\n",
      "Peter K. Yu_285_papers.xlsx\n",
      "Philip Hackney_101_papers.xlsx\n",
      "Rachel D. Godsil_93_papers.xlsx\n",
      "Rafael (I) Pardo_207_papers.xlsx\n",
      "Rafael I. Pardo_207_papers.xlsx\n",
      "Rebecca M. Kysar_149_papers.xlsx\n",
      "Rebecca Tushnet_268_papers.xlsx\n",
      "Renee Knake_144_papers.xlsx\n",
      "Richard Albert_3_papers.xlsx\n",
      "Richard Brooks_25_papers.xlsx\n",
      "Richard Delgado_59_papers.xlsx\n",
      "Robert H. II Jerry_132_papers.xlsx\n",
      "Robert J. MacCoun_163_papers.xlsx\n",
      "Robert J. Rhee_223_papers.xlsx\n",
      "Robert Jackson_130_papers.xlsx\n",
      "Robert Miller_184_papers.xlsx\n",
      "Robin Wilson_278_papers.xlsx\n",
      "RonNell Andersen Jones_136_papers.xlsx\n",
      "Rose Cuison Villazor_273_papers.xlsx\n",
      "Ruqaiijah Yearby_284_papers.xlsx\n",
      "Ruth Mason_170_papers.xlsx\n",
      "Ruth Okediji_199_papers.xlsx\n",
      "Ryan Vacca_270_papers.xlsx\n",
      "Sahar Aziz_9_papers.xlsx\n",
      "Sam Halabi_102_papers.xlsx\n",
      "Samuel Bray_23_papers.xlsx\n",
      "Samuel Moyn_190_papers.xlsx\n",
      "Sandi B. Zellmer_287_papers.xlsx\n",
      "Sarah C. Haan_100_papers.xlsx\n",
      "Sarah Lawsky_152_papers.xlsx\n",
      "Scott Dodson_62_papers.xlsx\n",
      "Scott Pryor_219_papers.xlsx\n",
      "Seema Mohapatra_187_papers.xlsx\n",
      "Seth Davis_58_papers.xlsx\n",
      "Shalanda H. Baker_12_papers.xlsx\n",
      "Sheila Foster_79_papers.xlsx\n",
      "Shi-Ling Hsu_123_papers.xlsx\n",
      "Shu-Yi Oei_197_papers.xlsx\n",
      "Shubha Ghosh_92_papers.xlsx\n",
      "Sonia Katyal_139_papers.xlsx\n",
      "Srividhya Ragavan_222_papers.xlsx\n",
      "Steve Vladeck_274_papers.xlsx\n",
      "Steven Davidoff_57_papers.xlsx\n",
      "Susan Fortney_78_papers.xlsx\n",
      "Susan Franck_81_papers.xlsx\n",
      "Susan Morse_189_papers.xlsx\n",
      "Suzette Malveaux_165_papers.xlsx\n",
      "Teri Dobbins Baxter_16_papers.xlsx\n",
      "Thomas Brennan_24_papers.xlsx\n",
      "Thomas Main_164_papers.xlsx\n",
      "Thomas Mitchell_186_papers.xlsx\n",
      "Thomas W. Mitchell_186_papers.xlsx\n",
      "Timothy Lytton_162_papers.xlsx\n",
      "Tomiko Brown-Nagin_29_papers.xlsx\n",
      "Troy Rule_233_papers.xlsx\n",
      "Urska Velikonja_272_papers.xlsx\n",
      "Victor Flatt_75_papers.xlsx\n",
      "Victor Fleischer_77_papers.xlsx\n",
      "Victoria Nourse_195_papers.xlsx\n",
      "Victoria Shannon Sahani_235_papers.xlsx\n",
      "W. Edward Afield_2_papers.xlsx\n",
      "Wendell E. Pritchett_217_papers.xlsx\n",
      "William Boyd_21_papers.xlsx\n",
      "William S. Dodge_61_papers.xlsx\n",
      "William W. Buzbee_35_papers.xlsx\n",
      "Woodrow Hartzog_105_papers.xlsx\n",
      "Xuan-Thao Nguyen_194_papers.xlsx\n",
      "Yoon-Ho Alex Lee_153_papers.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Stack the output files\n",
    "files = os.listdir(out_path)\n",
    "stacked_output = pd.DataFrame()\n",
    "for f in files:\n",
    "    print(f)\n",
    "    data = pd.read_excel(out_path / f, 'Sheet1')\n",
    "    data[\"file\"] = f\n",
    "    stacked_output = stacked_output.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate the data by ID so that we can see if all of the IDs are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_output.sort_values(by = [\"ID\"], inplace = True)\n",
    "stacked_output.to_excel(out_path / '_stacked_output.xlsx', index=False)\n",
    "skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}