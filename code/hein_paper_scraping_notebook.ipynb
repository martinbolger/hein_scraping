{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "web_scraping",
   "display_name": "web_scraping",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "import modules.hein_scraping_functions\n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already exists. Names that have already been scraped will be skipped\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "input_data = pd.read_excel(intr_path / \"hein_scraping_input_data.xlsx\")\n",
    "data = input_data\n",
    "data_type = \"lateral\"\n",
    "\n",
    "# Create the list of scraped pages columns\n",
    "scraped_pages_columns = [\"links\", \"file_names\", \"professor_names\", \"id\"]\n",
    "# Load the list of scraped pages if it exists\n",
    "scraped_pages_file = out_path / \"_scraped_pages.xlsx\"\n",
    "if scraped_pages_file.exists():\n",
    "    print(\"Data already exists. Names that have already been scraped will be skipped\")\n",
    "    # Create the dataset of existing alt names.\n",
    "    df_scraped_pages = pd.read_excel(scraped_pages_file)\n",
    "else:\n",
    "    df_scraped_pages = pd.DataFrame(columns = scraped_pages_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      ID  FirstName     LastName  multi_obs Short URL Destination  \\\n",
       "0      1    Matthew        Adler      False              duke.edu   \n",
       "1      2     Edward       Afield      False               gsu.edu   \n",
       "2      3    Richard       Albert      False            utexas.edu   \n",
       "3      4       Lisa    Alexander      False              tamu.edu   \n",
       "4      5     Hilary        Allen      False          american.edu   \n",
       "..   ...        ...          ...        ...                   ...   \n",
       "318  283      Ellen  Yaroshefsky      False           hofstra.edu   \n",
       "319  284  Ruqaiijah       Yearby      False               slu.edu   \n",
       "320  285      Peter           Yu      False              tamu.edu   \n",
       "321  286    Kathryn       Zeiler      False                bu.edu   \n",
       "322  287      Sandi      Zellmer      False               umt.edu   \n",
       "\n",
       "    Short URL Origin  Lateral  LateralYear  \\\n",
       "0          upenn.edu        1         2012   \n",
       "1    avemarialaw.edu        1         2016   \n",
       "2             bc.edu        1         2017   \n",
       "3           wisc.edu        1         2016   \n",
       "4        suffolk.edu        1         2018   \n",
       "..               ...      ...          ...   \n",
       "318           yu.edu        1         2016   \n",
       "319         case.edu        1         2018   \n",
       "320        drake.edu        1         2015   \n",
       "321   georgetown.edu        1         2015   \n",
       "322          unl.edu        1         2018   \n",
       "\n",
       "                                     Origin School  \\\n",
       "0            University of Pennsylvania Law School   \n",
       "1                          Ave Maria School of Law   \n",
       "2                        Boston College Law School   \n",
       "3               University of Wisconsin Law School   \n",
       "4                    Suffolk University Law School   \n",
       "..                                             ...   \n",
       "318                             Cardozo Law School   \n",
       "319  Case Western Reserve University School of Law   \n",
       "320                    Drake University Law School   \n",
       "321               Georgetown University Law Center   \n",
       "322          University of Nebraska College of Law   \n",
       "\n",
       "                                    Destination School  alt_url  \\\n",
       "0                        Duke University School of Law        0   \n",
       "1                         Georgia State College of Law        0   \n",
       "2                    University of Texas School of Law        0   \n",
       "3                   Texas A&M University School of Law        0   \n",
       "4        American University Washington College of Law        0   \n",
       "..                                                 ...      ...   \n",
       "318  Maurice A. Deane School of Law at Hofstra Univ...        0   \n",
       "319               Saint Louis University School of Law        0   \n",
       "320                 Texas A&M University School of Law        0   \n",
       "321                    Boston University School of Law        0   \n",
       "322  University of Montana Alexander Blewett III Sc...        0   \n",
       "\n",
       "                fm_names  ID_counts  \n",
       "0    Matthew, Matthew D.          1  \n",
       "1              W. Edward          1  \n",
       "2                Richard          1  \n",
       "3                Lisa T.          1  \n",
       "4              Hilary J.          1  \n",
       "..                   ...        ...  \n",
       "318                Ellen          1  \n",
       "319            Ruqaiijah          1  \n",
       "320             Peter K.          1  \n",
       "321              Kathryn          1  \n",
       "322     Sandi B., Sandra          1  \n",
       "\n",
       "[323 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>multi_obs</th>\n      <th>Short URL Destination</th>\n      <th>Short URL Origin</th>\n      <th>Lateral</th>\n      <th>LateralYear</th>\n      <th>Origin School</th>\n      <th>Destination School</th>\n      <th>alt_url</th>\n      <th>fm_names</th>\n      <th>ID_counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Matthew</td>\n      <td>Adler</td>\n      <td>False</td>\n      <td>duke.edu</td>\n      <td>upenn.edu</td>\n      <td>1</td>\n      <td>2012</td>\n      <td>University of Pennsylvania Law School</td>\n      <td>Duke University School of Law</td>\n      <td>0</td>\n      <td>Matthew, Matthew D.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Edward</td>\n      <td>Afield</td>\n      <td>False</td>\n      <td>gsu.edu</td>\n      <td>avemarialaw.edu</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>Ave Maria School of Law</td>\n      <td>Georgia State College of Law</td>\n      <td>0</td>\n      <td>W. Edward</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Richard</td>\n      <td>Albert</td>\n      <td>False</td>\n      <td>utexas.edu</td>\n      <td>bc.edu</td>\n      <td>1</td>\n      <td>2017</td>\n      <td>Boston College Law School</td>\n      <td>University of Texas School of Law</td>\n      <td>0</td>\n      <td>Richard</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Lisa</td>\n      <td>Alexander</td>\n      <td>False</td>\n      <td>tamu.edu</td>\n      <td>wisc.edu</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>University of Wisconsin Law School</td>\n      <td>Texas A&amp;M University School of Law</td>\n      <td>0</td>\n      <td>Lisa T.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Hilary</td>\n      <td>Allen</td>\n      <td>False</td>\n      <td>american.edu</td>\n      <td>suffolk.edu</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>Suffolk University Law School</td>\n      <td>American University Washington College of Law</td>\n      <td>0</td>\n      <td>Hilary J.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>318</th>\n      <td>283</td>\n      <td>Ellen</td>\n      <td>Yaroshefsky</td>\n      <td>False</td>\n      <td>hofstra.edu</td>\n      <td>yu.edu</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>Cardozo Law School</td>\n      <td>Maurice A. Deane School of Law at Hofstra Univ...</td>\n      <td>0</td>\n      <td>Ellen</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>319</th>\n      <td>284</td>\n      <td>Ruqaiijah</td>\n      <td>Yearby</td>\n      <td>False</td>\n      <td>slu.edu</td>\n      <td>case.edu</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>Case Western Reserve University School of Law</td>\n      <td>Saint Louis University School of Law</td>\n      <td>0</td>\n      <td>Ruqaiijah</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>320</th>\n      <td>285</td>\n      <td>Peter</td>\n      <td>Yu</td>\n      <td>False</td>\n      <td>tamu.edu</td>\n      <td>drake.edu</td>\n      <td>1</td>\n      <td>2015</td>\n      <td>Drake University Law School</td>\n      <td>Texas A&amp;M University School of Law</td>\n      <td>0</td>\n      <td>Peter K.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>321</th>\n      <td>286</td>\n      <td>Kathryn</td>\n      <td>Zeiler</td>\n      <td>False</td>\n      <td>bu.edu</td>\n      <td>georgetown.edu</td>\n      <td>1</td>\n      <td>2015</td>\n      <td>Georgetown University Law Center</td>\n      <td>Boston University School of Law</td>\n      <td>0</td>\n      <td>Kathryn</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>287</td>\n      <td>Sandi</td>\n      <td>Zellmer</td>\n      <td>False</td>\n      <td>umt.edu</td>\n      <td>unl.edu</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>University of Nebraska College of Law</td>\n      <td>University of Montana Alexander Blewett III Sc...</td>\n      <td>0</td>\n      <td>Sandi B., Sandra</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>323 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matthew Adler\n",
      "Name list: ['Matthew', 'Matthew D.']\n",
      "Looking for Matthew Adler\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Adler%2C Matthew&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Matthew D. Adler\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Adler%2C Matthew D.&collection=journals has already been scraped. Moving to the next name.\n",
      "Edward Afield\n",
      "Name list: ['W. Edward']\n",
      "Looking for W. Edward Afield\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Afield%2C W. Edward&collection=journals has already been scraped. Moving to the next name.\n",
      "Richard Albert\n",
      "Name list: ['Richard']\n",
      "Looking for Richard Albert\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Albert%2C Richard&collection=journals has already been scraped. Moving to the next name.\n",
      "Lisa Alexander\n",
      "Name list: ['Lisa T.']\n",
      "Looking for Lisa T. Alexander\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Alexander%2C Lisa T.&collection=journals has already been scraped. Moving to the next name.\n",
      "Hilary Allen\n",
      "Name list: ['Hilary J.']\n",
      "Looking for Hilary J. Allen\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Allen%2C Hilary J.&collection=journals has already been scraped. Moving to the next name.\n",
      "Owen Anderson\n",
      "Name list: ['Owen L.']\n",
      "Looking for Owen L. Anderson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Anderson%2C Owen L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Olufunmilayo Arewa\n",
      "Name list: ['Olufunmilayo B.']\n",
      "Looking for Olufunmilayo B. Arewa\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Arewa%2C Olufunmilayo B.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kenneth Ayotte\n",
      "Name list: ['Kenneth', 'Kenneth M.']\n",
      "Looking for Kenneth Ayotte\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Ayotte%2C Kenneth&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Kenneth M. Ayotte\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Ayotte%2C Kenneth M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Sahar Aziz\n",
      "Name list: ['Sahar', 'Sahar F.']\n",
      "Looking for Sahar Aziz\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Aziz%2C Sahar&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Sahar F. Aziz\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Aziz%2C Sahar F.&collection=journals has already been scraped. Moving to the next name.\n",
      "Adam Badawi\n",
      "Name list: ['Adam', 'Adam B.']\n",
      "Looking for Adam Badawi\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Badawi%2C Adam&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Adam B. Badawi\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Badawi%2C Adam B.&collection=journals has already been scraped. Moving to the next name.\n",
      "Margo Bagley\n",
      "Name list: ['Margo A.']\n",
      "Looking for Margo A. Bagley\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bagley%2C Margo A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Shalanda Baker\n",
      "Name list: ['Shalanda H.', 'Shalanda Helen', 'Shalanda']\n",
      "Looking for Shalanda H. Baker\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Baker%2C Shalanda H.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Shalanda Helen Baker\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Baker%2C Shalanda Helen&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Shalanda Baker\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Baker%2C Shalanda&collection=journals has already been scraped. Moving to the next name.\n",
      "Angela Banks\n",
      "Name list: ['Angela']\n",
      "Looking for Angela Banks\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Banks%2C Angela&collection=journals has already been scraped. Moving to the next name.\n",
      "Oren Bar-Gill\n",
      "Name list: ['Oren']\n",
      "Looking for Oren Bar-Gill\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bar-Gill%2C Oren&collection=journals has already been scraped. Moving to the next name.\n",
      "Ann Bartow\n",
      "Name list: ['Ann']\n",
      "Looking for Ann Bartow\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bartow%2C Ann&collection=journals has already been scraped. Moving to the next name.\n",
      "Teri Baxter\n",
      "Name list: ['Teri Dobbins']\n",
      "Looking for Teri Dobbins Baxter\n",
      "Scraping the page\n",
      "Done scraping for Teri Dobbins Baxter.\n",
      "Teri Baxter\n",
      "Name list: ['Teri Dobbins']\n",
      "Looking for Teri Dobbins Baxter\n",
      "Scraping the page\n",
      "Done scraping for Teri Dobbins Baxter.\n",
      "Teri Dobbins\n",
      "Name list: ['Teri']\n",
      "Looking for Teri Dobbins\n",
      "Scraping the page\n",
      "Done scraping for Teri Dobbins.\n",
      "Teri Dobbins\n",
      "Name list: ['Teri']\n",
      "Looking for Teri Dobbins\n",
      "Scraping the page\n",
      "Done scraping for Teri Dobbins.\n",
      "Karima Bennoune\n",
      "Name list: ['Karima']\n",
      "Looking for Karima Bennoune\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bennoune%2C Karima&collection=journals has already been scraped. Moving to the next name.\n",
      "Mitchell Berman\n",
      "Name list: ['Mitchell N.']\n",
      "Looking for Mitchell N. Berman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Berman%2C Mitchell N.&collection=journals has already been scraped. Moving to the next name.\n",
      "Khaled Beydoun\n",
      "Name list: ['Khaled', 'Khaled A.', 'Khaled Ali']\n",
      "Looking for Khaled Beydoun\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Beydoun%2C Khaled&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Khaled A. Beydoun\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Beydoun%2C Khaled A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Khaled Ali Beydoun\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Beydoun%2C Khaled Ali&collection=journals has already been scraped. Moving to the next name.\n",
      "Joshua Blank\n",
      "Name list: ['Joshua']\n",
      "Looking for Joshua Blank\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Blank%2C Joshua&collection=journals has already been scraped. Moving to the next name.\n",
      "William Boyd\n",
      "Name list: ['William']\n",
      "Looking for William Boyd\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Boyd%2C William&collection=journals has already been scraped. Moving to the next name.\n",
      "Anu Bradford\n",
      "Name list: ['Anu']\n",
      "Looking for Anu Bradford\n",
      "Scraping the page\n",
      "Done scraping for Anu Bradford.\n",
      "Anu Bradford\n",
      "Name list: ['Anu']\n",
      "Looking for Anu Bradford\n",
      "Scraping the page\n",
      "Done scraping for Anu Bradford.\n",
      "Anu Piilola\n",
      "Name list: ['Anu']\n",
      "Looking for Anu Piilola\n",
      "Scraping the page\n",
      "Done scraping for Anu Piilola.\n",
      "Anu Piilola\n",
      "Name list: ['Anu']\n",
      "Looking for Anu Piilola\n",
      "Scraping the page\n",
      "Done scraping for Anu Piilola.\n",
      "Samuel Bray\n",
      "Name list: ['Samuel', 'Samuel L.']\n",
      "Looking for Samuel Bray\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bray%2C Samuel&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Samuel L. Bray\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bray%2C Samuel L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Thomas Brennan\n",
      "Name list: ['Thomas', 'Thomas J.']\n",
      "Looking for Thomas Brennan\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brennan%2C Thomas&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Thomas J. Brennan\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brennan%2C Thomas J.&collection=journals has already been scraped. Moving to the next name.\n",
      "Richard Brooks\n",
      "Name list: ['Richard']\n",
      "Looking for Richard Brooks\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brooks%2C Richard&collection=journals has already been scraped. Moving to the next name.\n",
      "Alfred Brophy\n",
      "Name list: ['Alfred', 'Alfred L.']\n",
      "Looking for Alfred Brophy\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brophy%2C Alfred&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Alfred L. Brophy\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brophy%2C Alfred L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Charles Brower\n",
      "Name list: ['Charles']\n",
      "Looking for Charles Brower\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brower%2C Charles&collection=journals has already been scraped. Moving to the next name.\n",
      "Eleanor Brown\n",
      "Name list: ['Eleanor', 'Eleanor Marie', 'Eleanor Marie Lawrence']\n",
      "Looking for Eleanor Brown\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brown%2C Eleanor&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Eleanor Marie Brown\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brown%2C Eleanor Marie&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Eleanor Marie Lawrence Brown\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brown%2C Eleanor Marie Lawrence&collection=journals has already been scraped. Moving to the next name.\n",
      "Tomiko Brown-Nagin\n",
      "Name list: ['Tomiko']\n",
      "Looking for Tomiko Brown-Nagin\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Brown-Nagin%2C Tomiko&collection=journals has already been scraped. Moving to the next name.\n",
      "Aaron Bruhl\n",
      "Name list: ['Aaron-Andrew P.']\n",
      "Looking for Aaron-Andrew P. Bruhl\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bruhl%2C Aaron-Andrew P.&collection=journals has already been scraped. Moving to the next name.\n",
      "Christopher Bruner\n",
      "Name list: ['Christopher M.']\n",
      "Looking for Christopher M. Bruner\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Bruner%2C Christopher M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Christopher Buccafusco\n",
      "Name list: ['Christopher', 'Christopher J.']\n",
      "Looking for Christopher Buccafusco\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Buccafusco%2C Christopher&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Christopher J. Buccafusco\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Buccafusco%2C Christopher J.&collection=journals has already been scraped. Moving to the next name.\n",
      "Karen Burke\n",
      "Name list: ['Karen C.']\n",
      "Looking for Karen C. Burke\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Burke%2C Karen C.&collection=journals has already been scraped. Moving to the next name.\n",
      "Paul Butler\n",
      "Name list: ['Paul']\n",
      "Looking for Paul Butler\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Butler%2C Paul&collection=journals has already been scraped. Moving to the next name.\n",
      "William Buzbee\n",
      "Name list: ['William W.', 'William Wade']\n",
      "Looking for William W. Buzbee\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Buzbee%2C William W.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for William Wade Buzbee\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Buzbee%2C William Wade&collection=journals has already been scraped. Moving to the next name.\n",
      "Irene Calboli\n",
      "Name list: ['Irene']\n",
      "Looking for Irene Calboli\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Calboli%2C Irene&collection=journals has already been scraped. Moving to the next name.\n",
      "Lan Cao\n",
      "Name list: ['Lan']\n",
      "Looking for Lan Cao\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Cao%2C Lan&collection=journals has already been scraped. Moving to the next name.\n",
      "June Carbone\n",
      "Name list: ['June']\n",
      "Looking for June Carbone\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Carbone%2C June&collection=journals has already been scraped. Moving to the next name.\n",
      "Paul Caron\n",
      "Name list: ['Paul', 'Paul L.']\n",
      "Looking for Paul Caron\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Caron%2C Paul&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Paul L. Caron\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Caron%2C Paul L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Dale Carpenter\n",
      "Name list: ['Dale']\n",
      "Looking for Dale Carpenter\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Carpenter%2C Dale&collection=journals has already been scraped. Moving to the next name.\n",
      "Jenny Carroll\n",
      "Name list: ['Jenny E.', 'Jenny (I)']\n",
      "Looking for Jenny E. Carroll\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Carroll%2C Jenny E.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Jenny (I) Carroll\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Carroll%2C Jenny (I)&collection=journals has already been scraped. Moving to the next name.\n",
      "Jennifer Chacon\n",
      "Name list: ['Jennifer', 'Jennifer M.']\n",
      "Looking for Jennifer Chacon\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chacon%2C Jennifer&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Jennifer M. Chacon\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chacon%2C Jennifer M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Anupam Chander\n",
      "Name list: ['Anupam']\n",
      "Looking for Anupam Chander\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chander%2C Anupam&collection=journals has already been scraped. Moving to the next name.\n",
      "Stewart Chang\n",
      "Name list: ['Stewart', 'Stewart L.', 'Stewart']\n",
      "Looking for Stewart Chang\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chang%2C Stewart&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Stewart L. Chang\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chang%2C Stewart L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Stewart Chang\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chang%2C Stewart&collection=journals has already been scraped. Moving to the next name.\n",
      "Jim Chen\n",
      "Name list: ['Jim']\n",
      "Looking for Jim Chen\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chen%2C Jim&collection=journals has already been scraped. Moving to the next name.\n",
      "Eric Chiappinelli\n",
      "Name list: ['Eric A.']\n",
      "Looking for Eric A. Chiappinelli\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Chiappinelli%2C Eric A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Luis Aponte\n",
      "Name list: ['Luis Ernesto Chiesa', 'Luis E. Chiesa', 'Luis E.', 'Luis Ernesto', 'Luis Ernesto Chiesa']\n",
      "Looking for Luis Ernesto Chiesa Aponte\n",
      "Scraping the page\n",
      "Done scraping for Luis Ernesto Chiesa Aponte.\n",
      "Looking for Luis E. Chiesa Aponte\n",
      "Scraping the page\n",
      "Done scraping for Luis E. Chiesa Aponte.\n",
      "Looking for Luis E. Aponte\n",
      "No data was found for Luis E. Aponte. Moving to the next name.\n",
      "Looking for Luis Ernesto Aponte\n",
      "No data was found for Luis Ernesto Aponte. Moving to the next name.\n",
      "Looking for Luis Ernesto Chiesa Aponte\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-390f712e041e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m#Direct the webdriver to the page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;31m#This function waits for the webpage to load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mwebpage_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             return self.request_encode_body(\n\u001b[0m\u001b[0;32m     80\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initilization\n",
    "# Page name is a list of the name for all of the pages that we have scraped.\n",
    "# This is the name that actually appears on the webpage. This helps prevent\n",
    "# us from having to rescrape pages multiple times.\n",
    "err_fm_names = []\n",
    "skip_df = pd.DataFrame()\n",
    "\n",
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    # Export the updated dataframe of skipped names and scraped pages\n",
    "    skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "    df_scraped_pages.to_excel(scraped_pages_file, index = False)\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    # Create the multiple observation variable\n",
    "    multi_obs = data[\"multi_obs\"][i]\n",
    "    # Create the index variable for the name. This is used to distinguish \n",
    "    # the file names if we have mutliple last names.\n",
    "    last_name_index = data[\"ID_counts\"][i]\n",
    "    # Get the alt url value\n",
    "    alt_url = data[\"alt_url\"][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "\n",
    "    # If there were no matching names, the value is nan. This means that the value does not equal itself.\n",
    "    #  The name is added to the skipped names list and the loop moves onto the next name. \n",
    "    fm_names_str = data['fm_names'][i]\n",
    "    if fm_names_str != fm_names_str:\n",
    "        print('Name ' + full_name + ' was not found. Adding to the skipped names dataset.')\n",
    "        skip_df = pd.concat([skip_df, data.iloc[[i]]])\n",
    "        continue\n",
    "\n",
    "    fm_names = fm_names_str.split(\", \")\n",
    "    print(\"Name list: {}\".format(fm_names))   \n",
    "        \n",
    "    #This section loops through the list of alternative names and goes directly to their pages on Hein\n",
    "    for first_name_index, fm_name in enumerate(fm_names):\n",
    "        # Create the full name\n",
    "        full_name = fm_name + ' ' +  last_name      \n",
    "\n",
    "        #Link to Hein page\n",
    "        if alt_url == 0:\n",
    "            links = ['https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals']\n",
    "        elif alt_url == 1:\n",
    "            links = ['https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals', 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%20 ' + fm_name + '&collection=journals']  \n",
    "\n",
    "        for link_index, link in enumerate(links):\n",
    "\n",
    "            print(\"Looking for {}\".format(full_name))\n",
    "            if alt_url == 1:\n",
    "                # Check if the file exists\n",
    "                file_name = '{}_{}_{}_{}_{}_papers.xlsx'.format(full_name, prof_id, first_name_index, last_name_index, link_index)  \n",
    "            elif alt_url == 0:\n",
    "                # Check if the file exists\n",
    "                file_name = '{}_{}_{}_papers.xlsx'.format(full_name, prof_id, last_name_index)  \n",
    "\n",
    "            # CHECK DATA: This is the first spot where we check the data. We want to see if the link that we are\n",
    "            # going to scrape has already been added to the list of scraped pages. This is helpful when \n",
    "            # rerunning the code.\n",
    "            if not df_scraped_pages.query('@link == links').empty and not multi_obs:\n",
    "                print(\"The link {} has already been scraped. Moving to the next name.\".format(link))\n",
    "                continue\n",
    "            #Direct the webdriver to the page\n",
    "            driver.get(link)\n",
    "            #This function waits for the webpage to load\n",
    "            webpage_wait('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]', driver)\n",
    "            \n",
    "            # Make sure that the data exists on the page. Otherwise, we will skip the page.\n",
    "            try:\n",
    "                no_data_text = driver.find_element_by_xpath('//*[@id=\"luceneres\"]/b').text\n",
    "                if no_data_text == \"No matching results found\":\n",
    "                    data_exists = False\n",
    "            except NoSuchElementException:\n",
    "                data_exists = True\n",
    "\n",
    "            # This is the name for the professor that is used on the page.\n",
    "            cur_page = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]').text\n",
    "\n",
    "            # CHECK DATA: This is the second spot where we check the data to see if the page has\n",
    "            # already been scraped. In order to check to see if the link has already been scraped, \n",
    "            # we look for the professor name on the page (which may be different from the name in our list) \n",
    "            # and the professor ID. This is helpful when two names point to the same page.\n",
    "            if not df_scraped_pages.query('@cur_page == professor_names and @prof_id == id').empty and not multi_obs: \n",
    "                print(\"A file for {} already exists. Moving to the next name.\".format(full_name))\n",
    "                # Add the link to the data so that we know to skip it in future runs\n",
    "                values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "                dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "                df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "                continue\n",
    "            #If there is a table on the page and the page name has not already appeared in the scraped list.\n",
    "            if data_exists: \n",
    "                element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]')\n",
    "                table_element = element.text.split('\\n')\n",
    "                #If the table is empty, there is no data to scrape\n",
    "                if len(table_element) < 5:\n",
    "                    print('No data available on Hein for {} {}'.format(fm_name, last_name))\n",
    "                    continue\n",
    "                #If the table is full, this section rearranges the data into a better format\n",
    "                print(\"Scraping the page\")                \n",
    "                #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\n",
    "                #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \n",
    "                #papers. The xpaths change when the page scrolls.\n",
    "                title_index = 3\n",
    "                stats_index = 4\n",
    "                topic_index = 0\n",
    "                scroll_num = 0\n",
    "                #This gets the page source\n",
    "                soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "                #This section gets the paper topics\n",
    "                topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "                element = title_index\n",
    "                df = pd.DataFrame(columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Subjects', 'Type', 'Cited (articles)', 'Cited (cases)', 'Accessed'])\n",
    "                #This while loop will continue until there are no more papers on the page\n",
    "                while element:\n",
    "                    #Data stream is a list of the data in the paper data box (for example, authors, topics, journal)\n",
    "                    data_stream = []\n",
    "                    #This funciton returns a dictionary with various fields for each variable in the data box\n",
    "                    #Sometimes some of the variables are missing (for example, there are papers without a journal listed)\n",
    "                    #In this case, the dictionary returns an empty value for these variables\n",
    "                    data_dict = get_paper_data(last_name, prof_id, title_index, scroll_num, driver)\n",
    "                    #This section gets the paper stats box. This is the box that says how many citations the paper\n",
    "                    #has received\n",
    "                    if scroll_num == 0:\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                    elif scroll_num > 0:\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                    #This section extracts the data from the paper stats box\n",
    "                    for elm in element:\n",
    "                        cited_text = elm.text\n",
    "                    article_citations = 'na'\n",
    "                    case_citations = 'na'\n",
    "                    accessed = 'na'\n",
    "                    if not isinstance(cited_text, list):\n",
    "                        cited_text = cited_text.split('\\n')\n",
    "                        #This section finds the value for each paper stat\n",
    "                        for stat in cited_text:\n",
    "                            if 'Article' in stat:\n",
    "                                article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                            if 'Case' in stat:\n",
    "                                case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                            if 'Accessed' in stat:\n",
    "                                accessed = int(re.search(r'\\d+', stat).group())\n",
    "                    #The values are appended to the data_stream list\n",
    "                    data_stream.append(article_citations)\n",
    "                    data_stream.append(case_citations)\n",
    "                    data_stream.append(accessed)\n",
    "                    #This line adds the output from the function get_paper_data to the data_stream list\n",
    "                    data_stream = list(data_dict.values()) + data_stream\n",
    "                    #The data_stream list is used to add a line of data to the overall paper dataframe for this author\n",
    "                    df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Subjects', 'Type', 'Cited (articles)', 'Cited (cases)', 'Accessed']), sort=False)\n",
    "                    #The indices are augmented to get the next paper\n",
    "                    stats_index +=4\n",
    "                    title_index += 4\n",
    "                    #Check that next paper exists:\n",
    "                    if scroll_num == 0:\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                    #If the page has scrolled, the xpath we need to check has changed\n",
    "                    if scroll_num > 0:\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                    element = driver.find_elements_by_xpath(x_path_title)\n",
    "                    #If we can't find a next paper, it could be because we need to scroll again\n",
    "                    #This section attempts to scroll the page. \n",
    "                    if not element:\n",
    "                        scroll_num +=1\n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\n",
    "                        num_papers = int(box_element[0].text.split(' ')[0])\n",
    "                        #If there are more than 100 papers, we know there are still paper left to scrape\n",
    "                        if num_papers > 100*scroll_num:\n",
    "                            time.sleep(15)\n",
    "                            title_index = 3\n",
    "                            stats_index = 4\n",
    "                            topic_index = 0\n",
    "                            x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                            element = driver.find_elements_by_xpath(x_path_title)\n",
    "                #This line saves the Excel file of papers\n",
    "                df.to_excel(out_path / file_name, index=False)\n",
    "                # We have created a file, so we need to append the link and the file name to the list of scraped pages\n",
    "                values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "                dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "                df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "                time.sleep(3)\n",
    "                #If we reach this point, all the pages for that author have been scraped\n",
    "                print('Done scraping for {}.'.format(fm_name + ' ' + last_name))\n",
    "            else:\n",
    "                print(\"No data was found for {}. Moving to the next name.\".format(full_name))\n",
    "# Export the updated dataframe of skipped names and scraped pages\n",
    "skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "df_scraped_pages.to_excel(scraped_pages_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}