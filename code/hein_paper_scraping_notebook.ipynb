{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "web_scraping",
   "display_name": "web_scraping",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names\n",
    "from modules.data_manipulation_functions import remove_commas, check_files, concat_function, list_to_comma_separated_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "input_data = pd.read_excel(intr_path / \"hein_scraping_input_data.xlsx\")\n",
    "data = input_data\n",
    "data_type = \"control\"\n",
    "\n",
    "# Create the list of scraped pages columns\n",
    "scraped_pages_columns = [\"links\", \"file_names\", \"professor_names\", \"id\"]\n",
    "# Load the list of scraped pages if it exists\n",
    "scraped_pages_file = out_path / \"_scraped_pages.xlsx\"\n",
    "if scraped_pages_file.exists():\n",
    "    print(\"Data already exists. Names that have already been scraped will be skipped\")\n",
    "    # Create the dataset of existing alt names.\n",
    "    df_scraped_pages = pd.read_excel(scraped_pages_file)\n",
    "else:\n",
    "    df_scraped_pages = pd.DataFrame(columns = scraped_pages_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [links, file_names, professor_names, id]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>links</th>\n      <th>file_names</th>\n      <th>professor_names</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_scraped_pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Howard Abrams\n",
      "Name list: ['Howard E.']\n",
      "Looking for Howard E. Abrams\n",
      "Scraping the page\n",
      "['A Reevaluation of the Terminable Interest Rule [article]', 'Tax Law Review, Vol. 39, Issue 1 (Fall 1983), pp. 1-30', 'Abrams, Howard E. (Cited 83 times)', '39 Tax L. Rev. 1 (1983-1984)', 'Subjects: Law', 'Topics: Interest, Estates and Trusts']\n",
      "['Systemic Coercion: Unconstitutional Conditions in the Criminal Law [article]', 'Criminal Law', 'Journal of Criminal Law and Criminology, Vol. 72, Issue 1 (Spring 1981), pp. 128-164', 'Abrams, Howard E. (Cited 83 times)', '72 J. Crim. L. & Criminology 128 (1981)', 'Subjects: Law, Criminal Justice, Government and Politics', 'Topics: Criminal Law and Procedure, Plea Bargaining, Due Process, Fourteenth Amendment']\n",
      "['Rethinking Tax Transitions: A Reply to Dr. Shachar [comments]', 'Colloquy: Tax Transitions', 'Harvard Law Review , Vol. 98, Issue 8 (June 1985), pp. 1809-1819', 'Abrams, Howard E. (Cited 83 times)', '98 Harv. L. Rev. 1809 (June 1985)', 'Subjects: Economics, Government and Politics', 'Topics: Antitrust, Economics, Government, Legislation']\n",
      "['Taxation of Carried Interests: The Reform that Did Not Happen [article]', 'Loyola University Chicago Law Journal, Vol. 40, Issue 2 (Winter 2009), pp. 197-228', 'Abrams, Howard E. (Cited 83 times)', '40 Loy. U. Chi. L.J. 197 (2008-2009)', 'Subjects: Law, Sociology, Government and Politics, Finance, Business', 'Topics: Law and Society, Politics, Taxation--Federal, Interest, Business Organizations']\n",
      "['The Section 734(b) Basis Adjustment Needs Repair [article]', 'Tax Lawyer, Vol. 57, Issue 2 (Winter 2004), pp. 343-370', 'Abrams, Howard E. (Cited 83 times)', '57 Tax Law. 343 (2003-2004)', 'Subjects: Business, Law, Government and Politics, Finance', 'Topics: Business Organizations, Taxation--Federal']\n",
      "['Long-Awaited Regulations under Section 752 Provide Wrong Answers [article]', 'Tax Law Review, Vol. 44, Issue 4 (Summer 1989), pp. 627-640', 'Abrams, Howard E. (Cited 83 times)', '44 Tax L. Rev. 627 (1988-1989)', 'Subjects: Business, Finance', 'Topics: Business Organizations, Debts, Liability, Limited Liability Companies']\n",
      "['Economic Analysis and Unconstitutional Conditions: A Reply to Professor Epstein [comments]', 'San Diego Law Review, Vol. 27, Issue 2 (March-April 1990), pp. 359-394', 'Abrams, Howard E. (Cited 83 times)', '27 San Diego L. Rev. 359 (1990)', 'Subjects: Business, Law, Government and Politics, Criminal Justice', 'Topics: Business Organizations, First Amendment, Constitutional Law, Generally, Criminal Law and Procedure, Politics, Causation']\n",
      "['Rethinking Generation - Skipping Transfers [article]', 'Southwestern Law Journal, Vol. 40, Issue 5 (February 1987), pp. 1145-1168', 'Abrams, Howard E. (Cited 83 times)', '40 SW L.J. 1145 (1986-1987)', 'Subjects: Law', 'Topics: Estates and Trusts, Future Interests']\n",
      "['Simple Distributions from Leveraged Partnerships [article]', 'Pittsburgh Tax Review, Vol. 1, Issue 2 (Spring 2004), pp. 131-154', 'Abrams, Howard E. (Cited 83 times)', '1 Pitt. Tax Rev. 131 (2003-2004)', 'Subjects: Business, Law', 'Topics: Business Organizations, Estates and Trusts']\n",
      "['Dispositions and Partial Dispositions of a Partnership Interest [article]', 'Journal of Passthrough Entities, Vol. 11, Issue 1 (January-February 2008), pp. 31-40', 'Abrams, Howard E. (Cited 83 times)', '11 J. Passthrough Entities 31 (2008)', 'Subjects: Business', 'Topics: Interest, Business Organizations']\n",
      "['Wherefore Code Sec. 736 [article]', 'Taxes: The Tax Magazine, Vol. 96, Issue 3 (March 2018), pp. 119-133', 'Abrams, Howard E. (Cited 83 times)', '96 Taxes 119 (2018)', 'Subjects: Business, Finance, Law, Government and Politics', 'Topics: Business Organizations, Interest, Sales, Taxation--Federal']\n",
      "['Partnership Inequalities: The Consequence of Book/Tax Disparities [article]', 'Taxes: The Tax Magazine, Vol. 92, Issue 3 (March 2014), pp. 111-128', 'Abrams, Howard E. (Cited 83 times)', '92 Taxes 111 (2014)', 'Subjects: Business, Law, Finance', 'Topics: Business Organizations, Securities Law, Debts']\n",
      "['Partnership Disguised Sales: Traps Getting in and Tips on Getting out [chapter]', 'Partnerships, Real Estate, Individual & Enforcement: Chapter 6', 'Major Tax Planning, Vol. 68, pp. 6-1-6-20', 'Abrams, Howard E. (Cited 83 times)', '68 Major Tax Plan. 6-1 (2016)']\n",
      "['Reverse Allocations: More than Meets the Eye [article]', 'Journal of Passthrough Entities, Vol. 5, Issue 5 (September-October 2002), pp. 35-48', 'Abrams, Howard E. (Cited 83 times)', '5 J. Passthrough Entities 35 (2002)', 'Subjects: Business, Finance, Law', 'Topics: Business Organizations, Debts, Securities Law']\n",
      "['New Changes to the at-Risk Rules [article]', 'Journal of Passthrough Entities, Vol. 6, Issue 5 (September-October 2003), pp. 37-48', 'Abrams, Howard E. (Cited 83 times)', '6 J. Passthrough Entities 37 (2003)', 'Subjects: Housing and Real Estate, Finance', 'Topics: Liability, Real Estate']\n",
      "Done scraping for Howard E. Abrams.\n"
     ]
    }
   ],
   "source": [
    "# Initilization\n",
    "# Page name is a list of the name for all of the pages that we have scraped.\n",
    "# This is the name that actually appears on the webpage. This helps prevent\n",
    "# us from having to rescrape pages multiple times.\n",
    "err_fm_names = []\n",
    "skip_df = pd.DataFrame()\n",
    "\n",
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    # Export the updated dataframe of skipped names and scraped pages\n",
    "    skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "    df_scraped_pages.to_excel(scraped_pages_file, index = False)\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    # Create the multiple observation variable\n",
    "    multi_obs = data[\"multi_obs\"][i]\n",
    "    # Create the index variable for the name. This is used to distinguish \n",
    "    # the file names if we have mutliple last names.\n",
    "    name_index = data[\"ID_counts\"][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "\n",
    "    # If there were no matching names, the value is nan. This means that the value does not equal itself.\n",
    "    #  The name is added to the skipped names list and the loop moves onto the next name. \n",
    "    fm_names_str = data['fm_names'][i]\n",
    "    if fm_names_str != fm_names_str:\n",
    "        print('Name ' + full_name + ' was not found. Adding to the skipped names dataset.')\n",
    "        skip_df = pd.concat([skip_df, data.iloc[[i]]])\n",
    "        continue\n",
    "\n",
    "    fm_names = fm_names_str.split(\", \")\n",
    "    print(\"Name list: {}\".format(fm_names))   \n",
    "        \n",
    "    #This section loops through the list of alternative names and goes directly to their pages on Hein\n",
    "    for fm_name in fm_names:\n",
    "        # Create the full name\n",
    "        full_name = fm_name + ' ' +  last_name\n",
    "        print(\"Looking for {}\".format(full_name))\n",
    "        # Check if the file exists\n",
    "        file_name = '{}_{}_{}_papers.xlsx'.format(full_name, prof_id, name_index)            \n",
    "\n",
    "        #Link to Hein page\n",
    "        link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals'\n",
    "\n",
    "        # CHECK DATA: This is the first spot where we check the data. We want to see if the link that we are\n",
    "        # going to scrape has already been added to the list of scraped pages. This is helpful when \n",
    "        # rerunning the code.\n",
    "        if not df_scraped_pages.query('@link == links').empty and not multi_obs:\n",
    "            print(\"The link {} has already been scraped. Moving to the next name.\".format(link))\n",
    "            continue\n",
    "        #Direct the webdriver to the page\n",
    "        driver.get(link)\n",
    "        #This function waits for the webpage to load\n",
    "        webpage_wait('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]', driver)\n",
    "        \n",
    "        # Make sure that the data exists on the page. Otherwise, we will skip the page.\n",
    "        try:\n",
    "            no_data_text = driver.find_element_by_xpath('//*[@id=\"luceneres\"]/b').text\n",
    "            if no_data_text == \"No matching results found\":\n",
    "                data_exists = False\n",
    "        except NoSuchElementException:\n",
    "            data_exists = True\n",
    "\n",
    "        # This is the name for the professor that is used on the page.\n",
    "        cur_page = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]').text\n",
    "\n",
    "        # CHECK DATA: This is the second spot where we check the data to see if the page has\n",
    "        # already been scraped. In order to check to see if the link has already been scraped, \n",
    "        # we look for the professor name on the page (which may be different from the name in our list) \n",
    "        # and the professor ID. This is helpful when two names point to the same page.\n",
    "        if not df_scraped_pages.query('@cur_page == professor_names and @prof_id == id').empty and not multi_obs: \n",
    "            print(\"A file for {} already exists. Moving to the next name.\".format(full_name))\n",
    "            # Add the link to the data so that we know to skip it in future runs\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            continue\n",
    "        #If there is a table on the page and the page name has not already appeared in the scraped list.\n",
    "        if data_exists: \n",
    "            element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]')\n",
    "            table_element = element.text.split('\\n')\n",
    "            #If the table is empty, there is no data to scrape\n",
    "            if len(table_element) < 5:\n",
    "                print('No data available on Hein for {} {}'.format(fm_name, last_name))\n",
    "                continue\n",
    "            #If the table is full, this section rearranges the data into a better format\n",
    "            print(\"Scraping the page\")                \n",
    "            #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\n",
    "            #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \n",
    "            #papers. The xpaths change when the page scrolls.\n",
    "            title_index = 3\n",
    "            stats_index = 4\n",
    "            topic_index = 0\n",
    "            scroll_num = 0\n",
    "            #This gets the page source\n",
    "            soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "            #This section gets the paper topics\n",
    "            topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "            element = title_index\n",
    "            df = pd.DataFrame(columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Subjects', 'Cited (articles)', 'Cited (cases)', 'Accessed'])\n",
    "            #This while loop will continue until there are no more papers on the page\n",
    "            while element:\n",
    "                #Data stream is a list of the data in the paper data box (for example, authors, topics, journal)\n",
    "                data_stream = []\n",
    "                #This funciton returns a dictionary with various fields for each variable in the data box\n",
    "                #Sometimes some of the variables are missing (for example, there are papers without a journal listed)\n",
    "                #In this case, the dictionary returns an empty value for these variables\n",
    "                data_dict = get_paper_data(last_name, prof_id, title_index, scroll_num, driver)\n",
    "                #This section gets the paper stats box. This is the box that says how many citations the paper\n",
    "                #has received\n",
    "                if scroll_num == 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                elif scroll_num > 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                #This section extracts the data from the paper stats box\n",
    "                for elm in element:\n",
    "                    cited_text = elm.text\n",
    "                article_citations = 'na'\n",
    "                case_citations = 'na'\n",
    "                accessed = 'na'\n",
    "                if not isinstance(cited_text, list):\n",
    "                    cited_text = cited_text.split('\\n')\n",
    "                    #This section finds the value for each paper stat\n",
    "                    for stat in cited_text:\n",
    "                        if 'Article' in stat:\n",
    "                            article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Case' in stat:\n",
    "                            case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Accessed' in stat:\n",
    "                            accessed = int(re.search(r'\\d+', stat).group())\n",
    "                #The values are appended to the data_stream list\n",
    "                data_stream.append(article_citations)\n",
    "                data_stream.append(case_citations)\n",
    "                data_stream.append(accessed)\n",
    "                #This line adds the output from the function get_paper_data to the data_stream list\n",
    "                data_stream = list(data_dict.values()) + data_stream\n",
    "                #The data_stream list is used to add a line of data to the overall paper dataframe for this author\n",
    "                df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Subjects', 'Cited (articles)', 'Cited (cases)', 'Accessed']), sort=False)\n",
    "                #The indices are augmented to get the next paper\n",
    "                stats_index +=4\n",
    "                title_index += 4\n",
    "                #Check that next paper exists:\n",
    "                if scroll_num == 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                #If the page has scrolled, the xpath we need to check has changed\n",
    "                if scroll_num > 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                element = driver.find_elements_by_xpath(x_path_title)\n",
    "                #If we can't find a next paper, it could be because we need to scroll again\n",
    "                #This section attempts to scroll the page. \n",
    "                if not element:\n",
    "                    scroll_num +=1\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\n",
    "                    num_papers = int(box_element[0].text.split(' ')[0])\n",
    "                    #If there are more than 100 papers, we know there are still paper left to scrape\n",
    "                    if num_papers > 100*scroll_num:\n",
    "                        time.sleep(15)\n",
    "                        title_index = 3\n",
    "                        stats_index = 4\n",
    "                        topic_index = 0\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                        element = driver.find_elements_by_xpath(x_path_title)\n",
    "            #This line saves the Excel file of papers\n",
    "            df.to_excel(out_path / file_name, index=False)\n",
    "            # We have created a file, so we need to append the link and the file name to the list of scraped pages\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            time.sleep(3)\n",
    "            #If we reach this point, all the pages for that author have been scraped\n",
    "            print('Done scraping for {}.'.format(fm_name + ' ' + last_name))\n",
    "        else:\n",
    "            print(\"No data was found for {}. Moving to the next name.\".format(full_name))\n",
    "# Export the updated dataframe of skipped names and scraped pages\n",
    "skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "df_scraped_pages.to_excel(scraped_pages_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}