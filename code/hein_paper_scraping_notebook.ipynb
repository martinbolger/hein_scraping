{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "web_scraping",
   "display_name": "web_scraping",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names\n",
    "from modules.data_manipulation_functions import remove_commas, check_files, concat_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already exists. Names that have already been scraped will be skipped\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "input_data = pd.read_excel(intr_path / \"hein_scraping_input_data.xlsx\")\n",
    "data = input_data\n",
    "data_type = \"control\"\n",
    "\n",
    "# Create the list of scraped pages columns\n",
    "scraped_pages_columns = [\"links\", \"file_names\", \"professor_names\", \"id\"]\n",
    "# Load the list of scraped pages if it exists\n",
    "scraped_pages_file = out_path / \"_scraped_pages.xlsx\"\n",
    "if scraped_pages_file.exists():\n",
    "    print(\"Data already exists. Names that have already been scraped will be skipped\")\n",
    "    # Create the dataset of existing alt names.\n",
    "    df_scraped_pages = pd.read_excel(scraped_pages_file)\n",
    "else:\n",
    "    df_scraped_pages = pd.DataFrame(columns = scraped_pages_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       ID FirstName   LastName  multi_obs  Short URL Destination  \\\n",
       "0    1001    Howard     Abrams      False                    NaN   \n",
       "1    1002     Bryan    Adamson      False                    NaN   \n",
       "2    1003  Nicholas     Allard      False                    NaN   \n",
       "3    1004     Scott     Altman      False                    NaN   \n",
       "4    1005   Michael   Ambrosio      False                    NaN   \n",
       "..    ...       ...        ...        ...                    ...   \n",
       "288  1275  Patricia     Wilson      False                    NaN   \n",
       "289  1276    Jarrod       Wong      False                    NaN   \n",
       "290  1277        Mo      Zhang      False                    NaN   \n",
       "291  1278    Donald    Zillman      False                    NaN   \n",
       "292  1279      Adam  Zimmerman      False                    NaN   \n",
       "\n",
       "     Short URL Origin  Lateral  LateralYear  Origin School  \\\n",
       "0                 NaN      NaN          NaN            NaN   \n",
       "1                 NaN      NaN          NaN            NaN   \n",
       "2                 NaN      NaN          NaN            NaN   \n",
       "3                 NaN      NaN          NaN            NaN   \n",
       "4                 NaN      NaN          NaN            NaN   \n",
       "..                ...      ...          ...            ...   \n",
       "288               NaN      NaN          NaN            NaN   \n",
       "289               NaN      NaN          NaN            NaN   \n",
       "290               NaN      NaN          NaN            NaN   \n",
       "291               NaN      NaN          NaN            NaN   \n",
       "292               NaN      NaN          NaN            NaN   \n",
       "\n",
       "     Destination School               fm_names  ID_counts  \n",
       "0                   NaN              Howard E.          1  \n",
       "1                   NaN        Bryan, Bryan L.          1  \n",
       "2                   NaN            Nicholas W.          1  \n",
       "3                   NaN                  Scott          1  \n",
       "4                   NaN    Michael, Michael P.          1  \n",
       "..                  ...                    ...        ...  \n",
       "288                 NaN  Patricia, Patricia A.          1  \n",
       "289                 NaN                 Jarrod          1  \n",
       "290                 NaN                     Mo          1  \n",
       "291                 NaN      Donald, Donald N.          1  \n",
       "292                 NaN       Adam S., Adam S.          1  \n",
       "\n",
       "[293 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>multi_obs</th>\n      <th>Short URL Destination</th>\n      <th>Short URL Origin</th>\n      <th>Lateral</th>\n      <th>LateralYear</th>\n      <th>Origin School</th>\n      <th>Destination School</th>\n      <th>fm_names</th>\n      <th>ID_counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>Howard</td>\n      <td>Abrams</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Howard E.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1002</td>\n      <td>Bryan</td>\n      <td>Adamson</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Bryan, Bryan L.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1003</td>\n      <td>Nicholas</td>\n      <td>Allard</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Nicholas W.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1004</td>\n      <td>Scott</td>\n      <td>Altman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Scott</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1005</td>\n      <td>Michael</td>\n      <td>Ambrosio</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Michael, Michael P.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>1275</td>\n      <td>Patricia</td>\n      <td>Wilson</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Patricia, Patricia A.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>289</th>\n      <td>1276</td>\n      <td>Jarrod</td>\n      <td>Wong</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Jarrod</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>290</th>\n      <td>1277</td>\n      <td>Mo</td>\n      <td>Zhang</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Mo</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>291</th>\n      <td>1278</td>\n      <td>Donald</td>\n      <td>Zillman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Donald, Donald N.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>1279</td>\n      <td>Adam</td>\n      <td>Zimmerman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Adam S., Adam S.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>293 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Shaughnessy%2C Joan M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Joanna Bailey\n",
      "Name list: ['Joanna Shepherd', 'Joanna', 'Joanna M.', 'Joanna Shepherd']\n",
      "Looking for Joanna Shepherd Bailey\n",
      "Scraping the page\n",
      "Done scraping for Joanna Shepherd Bailey.\n",
      "Looking for Joanna Bailey\n",
      "No data was found for Joanna Bailey. Moving to the next name.\n",
      "Looking for Joanna M. Bailey\n",
      "No data was found for Joanna M. Bailey. Moving to the next name.\n",
      "Looking for Joanna Shepherd Bailey\n",
      "Scraping the page\n",
      "Done scraping for Joanna Shepherd Bailey.\n",
      "Joanna Shepherd\n",
      "Name list: ['Joanna', 'Joanna', 'Joanna M.', 'Joanna Shepherd']\n",
      "Looking for Joanna Shepherd\n",
      "Scraping the page\n",
      "Done scraping for Joanna Shepherd.\n",
      "Looking for Joanna Shepherd\n",
      "Scraping the page\n",
      "Done scraping for Joanna Shepherd.\n",
      "Looking for Joanna M. Shepherd\n",
      "Scraping the page\n",
      "Done scraping for Joanna M. Shepherd.\n",
      "Looking for Joanna Shepherd Shepherd\n",
      "No data was found for Joanna Shepherd Shepherd. Moving to the next name.\n",
      "Nadav Shoked\n",
      "Name list: ['Nadav']\n",
      "Looking for Nadav Shoked\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Shoked%2C Nadav&collection=journals has already been scraped. Moving to the next name.\n",
      "Mark Sidel\n",
      "Name list: ['Mark']\n",
      "Looking for Mark Sidel\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sidel%2C Mark&collection=journals has already been scraped. Moving to the next name.\n",
      "Peter Siegelman\n",
      "Name list: ['Peter']\n",
      "Looking for Peter Siegelman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Siegelman%2C Peter&collection=journals has already been scraped. Moving to the next name.\n",
      "Daniel Simmons\n",
      "Name list: ['Daniel L.']\n",
      "Looking for Daniel L. Simmons\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Simmons%2C Daniel L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Robert Sitkoff\n",
      "Name list: ['Robert', 'Robert H.']\n",
      "Looking for Robert Sitkoff\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sitkoff%2C Robert&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Robert H. Sitkoff\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sitkoff%2C Robert H.&collection=journals has already been scraped. Moving to the next name.\n",
      "David Skover\n",
      "Name list: ['David', 'David M.']\n",
      "Looking for David Skover\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Skover%2C David&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for David M. Skover\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Skover%2C David M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Lawrence Solum\n",
      "Name list: ['Lawrence Byard']\n",
      "Looking for Lawrence Byard Solum\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Solum%2C Lawrence Byard&collection=journals has already been scraped. Moving to the next name.\n",
      "Sarah Song\n",
      "Name list: ['Sarah']\n",
      "Looking for Sarah Song\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Song%2C Sarah&collection=journals has already been scraped. Moving to the next name.\n",
      "Nancy Soonpaa\n",
      "Name list: ['Nancy', 'Nancy J.']\n",
      "Looking for Nancy Soonpaa\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Soonpaa%2C Nancy&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Nancy J. Soonpaa\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Soonpaa%2C Nancy J.&collection=journals has already been scraped. Moving to the next name.\n",
      "Ann Southworth\n",
      "Name list: ['Ann']\n",
      "Looking for Ann Southworth\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Southworth%2C Ann&collection=journals has already been scraped. Moving to the next name.\n",
      "Mark Spiegel\n",
      "Name list: ['Mark']\n",
      "Looking for Mark Spiegel\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Spiegel%2C Mark&collection=journals has already been scraped. Moving to the next name.\n",
      "John Sprankling\n",
      "Name list: ['John G.']\n",
      "Looking for John G. Sprankling\n",
      "Scraping the page\n",
      "Done scraping for John G. Sprankling.\n",
      "Richard Squire\n",
      "Name list: ['Richard']\n",
      "Looking for Richard Squire\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Squire%2C Richard&collection=journals has already been scraped. Moving to the next name.\n",
      "Barbara Stark\n",
      "Name list: ['Barbara']\n",
      "Looking for Barbara Stark\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stark%2C Barbara&collection=journals has already been scraped. Moving to the next name.\n",
      "Craig Stern\n",
      "Name list: ['Craig', 'Craig A.']\n",
      "Looking for Craig Stern\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stern%2C Craig&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Craig A. Stern\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stern%2C Craig A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Robert Strassfeld\n",
      "Name list: ['Robert N.', 'Robert']\n",
      "Looking for Robert N. Strassfeld\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Strassfeld%2C Robert N.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Robert Strassfeld\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Strassfeld%2C Robert&collection=journals has already been scraped. Moving to the next name.\n",
      "Charles Sullivan\n",
      "Name list: ['Charles', 'Charles A.']\n",
      "Looking for Charles Sullivan\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sullivan%2C Charles&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Charles A. Sullivan\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sullivan%2C Charles A.&collection=journals has already been scraped. Moving to the next name.\n",
      "William Tabb\n",
      "Name list: ['William Murray', 'William M.']\n",
      "Looking for William Murray Tabb\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tabb%2C William Murray&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for William M. Tabb\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tabb%2C William M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Shauhin Talesh\n",
      "Name list: ['Shauhin', 'Shauhin A.', 'Shauhin', 'Shauhin A.']\n",
      "Looking for Shauhin Talesh\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Talesh%2C Shauhin&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Shauhin A. Talesh\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Talesh%2C Shauhin A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Shauhin Talesh\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Talesh%2C Shauhin&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Shauhin A. Talesh\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Talesh%2C Shauhin A.&collection=journals has already been scraped. Moving to the next name.\n",
      "David Thronson\n",
      "Name list: ['David', 'David B.']\n",
      "Looking for David Thronson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Thronson%2C David&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for David B. Thronson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Thronson%2C David B.&collection=journals has already been scraped. Moving to the next name.\n",
      "Emerson Tiller\n",
      "Name list: ['Emerson H.', 'Emerson']\n",
      "Looking for Emerson H. Tiller\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tiller%2C Emerson H.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Emerson Tiller\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tiller%2C Emerson&collection=journals has already been scraped. Moving to the next name.\n",
      "Lisa Tripp\n",
      "Name list: ['Lisa']\n",
      "Looking for Lisa Tripp\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tripp%2C Lisa&collection=journals has already been scraped. Moving to the next name.\n",
      "Kevin Tu\n",
      "Name list: ['Kevin V.']\n",
      "Looking for Kevin V. Tu\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tu%2C Kevin V.&collection=journals has already been scraped. Moving to the next name.\n",
      "Christian Turner\n",
      "Name list: ['Christian', 'Christian']\n",
      "Looking for Christian Turner\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Turner%2C Christian&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Christian Turner\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Turner%2C Christian&collection=journals has already been scraped. Moving to the next name.\n",
      "Aaron Twerski\n",
      "Name list: ['Aaron D.', 'Aaron D. ']\n",
      "Looking for Aaron D. Twerski\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Twerski%2C Aaron D.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Aaron D.  Twerski\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Twerski%2C Aaron D. &collection=journals has already been scraped. Moving to the next name.\n",
      "Manuel Utset\n",
      "Name list: ['Manuel A.']\n",
      "Looking for Manuel A. Utset\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Utset%2C Manuel A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Stephen Utz\n",
      "Name list: ['Stephen', 'Stephen G.']\n",
      "Looking for Stephen Utz\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Utz%2C Stephen&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Stephen G. Utz\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Utz%2C Stephen G.&collection=journals has already been scraped. Moving to the next name.\n",
      "Rachel VanLandingham\n",
      "Name list: ['Rachel', 'Rachel E.']\n",
      "Looking for Rachel VanLandingham\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=VanLandingham%2C Rachel&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Rachel E. VanLandingham\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=VanLandingham%2C Rachel E.&collection=journals has already been scraped. Moving to the next name.\n",
      "David Walker\n",
      "Name list: ['David I.', 'David I.']\n",
      "Looking for David I. Walker\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Walker%2C David I.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for David I. Walker\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Walker%2C David I.&collection=journals has already been scraped. Moving to the next name.\n",
      "Cynthia Ward\n",
      "Name list: ['Cynthia', 'Cynthia V.']\n",
      "Looking for Cynthia Ward\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Ward%2C Cynthia&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Cynthia V. Ward\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Ward%2C Cynthia V.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kathryn Tongue\n",
      "Name list: ['Kathryn', 'Kathryn A.', 'Kathryn A,', 'Kathryn', 'Kathryn A.', 'Kathryn Tongue']\n",
      "Looking for Kathryn Tongue\n",
      "Scraping the page\n",
      "Done scraping for Kathryn Tongue.\n",
      "Looking for Kathryn A. Tongue\n",
      "Scraping the page\n",
      "Done scraping for Kathryn A. Tongue.\n",
      "Looking for Kathryn A, Tongue\n",
      "Scraping the page\n",
      "Done scraping for Kathryn A, Tongue.\n",
      "Looking for Kathryn Tongue\n",
      "Scraping the page\n",
      "Done scraping for Kathryn Tongue.\n",
      "Looking for Kathryn A. Tongue\n",
      "Scraping the page\n",
      "Done scraping for Kathryn A. Tongue.\n",
      "Looking for Kathryn Tongue Tongue\n",
      "No data was found for Kathryn Tongue Tongue. Moving to the next name.\n",
      "Kathryn Watts\n",
      "Name list: ['Kathryn', 'Kathryn A.', 'Kathryn', 'Kathryn A.', 'Kathryn Tongue']\n",
      "Looking for Kathryn Watts\n",
      "Scraping the page\n",
      "Done scraping for Kathryn Watts.\n",
      "Looking for Kathryn A. Watts\n",
      "Scraping the page\n",
      "Done scraping for Kathryn A. Watts.\n",
      "Looking for Kathryn Watts\n",
      "Scraping the page\n",
      "Done scraping for Kathryn Watts.\n",
      "Looking for Kathryn A. Watts\n",
      "Scraping the page\n",
      "Done scraping for Kathryn A. Watts.\n",
      "Looking for Kathryn Tongue Watts\n",
      "Scraping the page\n",
      "Done scraping for Kathryn Tongue Watts.\n",
      "Louise Weinberg\n",
      "Name list: ['Louise']\n",
      "Looking for Louise Weinberg\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Weinberg%2C Louise&collection=journals has already been scraped. Moving to the next name.\n",
      "Richard Weisberg\n",
      "Name list: ['Richard']\n",
      "Looking for Richard Weisberg\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Weisberg%2C Richard&collection=journals has already been scraped. Moving to the next name.\n",
      "Deborah Weissman\n",
      "Name list: ['Deborah', 'Deborah M.']\n",
      "Looking for Deborah Weissman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Weissman%2C Deborah&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Deborah M. Weissman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Weissman%2C Deborah M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Bret Wells\n",
      "Name list: ['Bret']\n",
      "Looking for Bret Wells\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wells%2C Bret&collection=journals has already been scraped. Moving to the next name.\n",
      "Keith Werhan\n",
      "Name list: ['Keith', 'Keith M.']\n",
      "Looking for Keith Werhan\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Werhan%2C Keith&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Keith M. Werhan\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Werhan%2C Keith M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Ahmed White\n",
      "Name list: ['Ahmed', 'Ahmed A.']\n",
      "Looking for Ahmed White\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=White%2C Ahmed&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Ahmed A. White\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=White%2C Ahmed A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Michaela White\n",
      "Name list: ['Michaela', 'Michaela M.']\n",
      "Looking for Michaela White\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=White%2C Michaela&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Michaela M. White\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=White%2C Michaela M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kelli Alces\n",
      "Name list: ['Kelli A.', 'Kelli Alces', 'Kelli A.']\n",
      "Looking for Kelli A. Alces\n",
      "Scraping the page\n",
      "Done scraping for Kelli A. Alces.\n",
      "Looking for Kelli Alces Alces\n",
      "No data was found for Kelli Alces Alces. Moving to the next name.\n",
      "Looking for Kelli A. Alces\n",
      "Scraping the page\n",
      "Done scraping for Kelli A. Alces.\n",
      "Kelli Alces\n",
      "Name list: ['Kelli A.']\n",
      "Looking for Kelli A. Alces\n",
      "Scraping the page\n",
      "Done scraping for Kelli A. Alces.\n",
      "Kelli Williams\n",
      "Name list: ['Kelli Alces', 'Kelli Alces', 'Kelli A.']\n",
      "Looking for Kelli Alces Williams\n",
      "Scraping the page\n",
      "Done scraping for Kelli Alces Williams.\n",
      "Looking for Kelli Alces Williams\n",
      "Scraping the page\n",
      "Done scraping for Kelli Alces Williams.\n",
      "Looking for Kelli A. Williams\n",
      "No data was found for Kelli A. Williams. Moving to the next name.\n",
      "Kelli Williams\n",
      "Name list: ['Kelli Alces']\n",
      "Looking for Kelli Alces Williams\n",
      "Scraping the page\n",
      "Done scraping for Kelli Alces Williams.\n",
      "Sean Williams\n",
      "Name list: ['Sean Hannon', 'Sean Hannon']\n",
      "Looking for Sean Hannon Williams\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Williams%2C Sean Hannon&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Sean Hannon Williams\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Williams%2C Sean Hannon&collection=journals has already been scraped. Moving to the next name.\n",
      "Arthur Wilmarth\n",
      "Name list: ['Arthur E. Jr.']\n",
      "Looking for Arthur E. Jr. Wilmarth\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wilmarth%2C Arthur E. Jr.&collection=journals has already been scraped. Moving to the next name.\n",
      "Patricia Wilson\n",
      "Name list: ['Patricia', 'Patricia A.']\n",
      "Looking for Patricia Wilson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wilson%2C Patricia&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Patricia A. Wilson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wilson%2C Patricia A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Jarrod Wong\n",
      "Name list: ['Jarrod']\n",
      "Looking for Jarrod Wong\n",
      "Scraping the page\n",
      "Done scraping for Jarrod Wong.\n",
      "Mo Zhang\n",
      "Name list: ['Mo']\n",
      "Looking for Mo Zhang\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zhang%2C Mo&collection=journals has already been scraped. Moving to the next name.\n",
      "Donald Zillman\n",
      "Name list: ['Donald', 'Donald N.']\n",
      "Looking for Donald Zillman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zillman%2C Donald&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Donald N. Zillman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zillman%2C Donald N.&collection=journals has already been scraped. Moving to the next name.\n",
      "Adam Zimmerman\n",
      "Name list: ['Adam S.', 'Adam S.']\n",
      "Looking for Adam S. Zimmerman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zimmerman%2C Adam S.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Adam S. Zimmerman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zimmerman%2C Adam S.&collection=journals has already been scraped. Moving to the next name.\n"
     ]
    }
   ],
   "source": [
    "# Initilization\n",
    "# Page name is a list of the name for all of the pages that we have scraped.\n",
    "# This is the name that actually appears on the webpage. This helps prevent\n",
    "# us from having to rescrape pages multiple times.\n",
    "err_fm_names = []\n",
    "skip_df = pd.DataFrame()\n",
    "\n",
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    # Export the updated dataframe of skipped names and scraped pages\n",
    "    skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "    df_scraped_pages.to_excel(scraped_pages_file, index = False)\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    # Create the multiple observation variable\n",
    "    multi_obs = data[\"multi_obs\"][i]\n",
    "    # Create the index variable for the name. This is used to distinguish \n",
    "    # the file names if we have mutliple last names.\n",
    "    name_index = data[\"ID_counts\"][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "\n",
    "    # If there were no matching names, the value is nan. This means that the value does not equal itself.\n",
    "    #  The name is added to the skipped names list and the loop moves onto the next name. \n",
    "    fm_names_str = data['fm_names'][i]\n",
    "    if fm_names_str != fm_names_str:\n",
    "        print('Name ' + full_name + ' was not found. Adding to the skipped names dataset.')\n",
    "        skip_df = pd.concat([skip_df, data.iloc[[i]]])\n",
    "        continue\n",
    "\n",
    "    fm_names = fm_names_str.split(\", \")\n",
    "    print(\"Name list: {}\".format(fm_names))   \n",
    "        \n",
    "    #This section loops through the list of alternative names and goes directly to their pages on Hein\n",
    "    for fm_name in fm_names:\n",
    "        # Create the full name\n",
    "        full_name = fm_name + ' ' +  last_name\n",
    "        print(\"Looking for {}\".format(full_name))\n",
    "        # Check if the file exists\n",
    "        file_name = '{}_{}_{}_papers.xlsx'.format(full_name, prof_id, name_index)            \n",
    "\n",
    "        #Link to Hein page\n",
    "        link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals'\n",
    "\n",
    "        # CHECK DATA: This is the first spot where we check the data. We want to see if the link that we are\n",
    "        # going to scrape has already been added to the list of scraped pages. This is helpful when \n",
    "        # rerunning the code.\n",
    "        if not df_scraped_pages.query('@link == links').empty and not multi_obs:\n",
    "            print(\"The link {} has already been scraped. Moving to the next name.\".format(link))\n",
    "            continue\n",
    "        #Direct the webdriver to the page\n",
    "        driver.get(link)\n",
    "        #This function waits for the webpage to load\n",
    "        webpage_wait('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]', driver)\n",
    "        \n",
    "        # Make sure that the data exists on the page. Otherwise, we will skip the page.\n",
    "        try:\n",
    "            no_data_text = driver.find_element_by_xpath('//*[@id=\"luceneres\"]/b').text\n",
    "            if no_data_text == \"No matching results found\":\n",
    "                data_exists = False\n",
    "        except NoSuchElementException:\n",
    "            data_exists = True\n",
    "\n",
    "        # This is the name for the professor that is used on the page.\n",
    "        cur_page = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]').text\n",
    "\n",
    "        # CHECK DATA: This is the second spot where we check the data to see if the page has\n",
    "        # already been scraped. In order to check to see if the link has already been scraped, \n",
    "        # we look for the professor name on the page (which may be different from the name in our list) \n",
    "        # and the professor ID. This is helpful when two names point to the same page.\n",
    "        if not df_scraped_pages.query('@cur_page == professor_names and @prof_id == id').empty and not multi_obs: \n",
    "            print(\"A file for {} already exists. Moving to the next name.\".format(full_name))\n",
    "            # Add the link to the data so that we know to skip it in future runs\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            continue\n",
    "        #If there is a table on the page and the page name has not already appeared in the scraped list.\n",
    "        if data_exists: \n",
    "            element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]')\n",
    "            table_element = element.text.split('\\n')\n",
    "            #If the table is empty, there is no data to scrape\n",
    "            if len(table_element) < 5:\n",
    "                print('No data available on Hein for {} {}'.format(fm_name, last_name))\n",
    "                continue\n",
    "            #If the table is full, this section rearranges the data into a better format\n",
    "            print(\"Scraping the page\")                \n",
    "            #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\n",
    "            #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \n",
    "            #papers. The xpaths change when the page scrolls.\n",
    "            title_index = 3\n",
    "            stats_index = 4\n",
    "            topic_index = 0\n",
    "            scroll_num = 0\n",
    "            #This gets the page source\n",
    "            soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "            #This section gets the paper topics\n",
    "            topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "            element = title_index\n",
    "            df = pd.DataFrame(columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed'])\n",
    "            #This while loop will continue until there are no more papers on the page\n",
    "            while element:\n",
    "                #Data stream is a list of the data in the paper data box (for example, authors, topics, journal)\n",
    "                data_stream = []\n",
    "                #This funciton returns a dictionary with various fields for each variable in the data box\n",
    "                #Sometimes some of the variables are missing (for example, there are papers without a journal listed)\n",
    "                #In this case, the dictionary returns an empty value for these variables\n",
    "                data_dict = get_paper_data(last_name, prof_id, title_index, scroll_num, driver)\n",
    "                #This section gets the paper stats box. This is the box that says how many citations the paper\n",
    "                #has received\n",
    "                if scroll_num == 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                elif scroll_num > 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                #This section extracts the data from the paper stats box\n",
    "                for elm in element:\n",
    "                    cited_text = elm.text\n",
    "                article_citations = 'na'\n",
    "                case_citations = 'na'\n",
    "                accessed = 'na'\n",
    "                if not isinstance(cited_text, list):\n",
    "                    cited_text = cited_text.split('\\n')\n",
    "                    #This section finds the value for each paper stat\n",
    "                    for stat in cited_text:\n",
    "                        if 'Article' in stat:\n",
    "                            article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Case' in stat:\n",
    "                            case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Accessed' in stat:\n",
    "                            accessed = int(re.search(r'\\d+', stat).group())\n",
    "                #The values are appended to the data_stream list\n",
    "                data_stream.append(article_citations)\n",
    "                data_stream.append(case_citations)\n",
    "                data_stream.append(accessed)\n",
    "                #This line adds the output from the function get_paper_data to the data_stream list\n",
    "                data_stream = list(data_dict.values()) + data_stream\n",
    "                #The data_stream list is used to add a line of data to the overall paper dataframe for this author\n",
    "                df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed']), sort=False)\n",
    "                #The indices are augmented to get the next paper\n",
    "                stats_index +=4\n",
    "                title_index += 4\n",
    "                #Check that next paper exists:\n",
    "                if scroll_num == 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                #If the page has scrolled, the xpath we need to check has changed\n",
    "                if scroll_num > 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                element = driver.find_elements_by_xpath(x_path_title)\n",
    "                #If we can't find a next paper, it could be because we need to scroll again\n",
    "                #This section attempts to scroll the page. \n",
    "                if not element:\n",
    "                    scroll_num +=1\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\n",
    "                    num_papers = int(box_element[0].text.split(' ')[0])\n",
    "                    #If there are more than 100 papers, we know there are still paper left to scrape\n",
    "                    if num_papers > 100*scroll_num:\n",
    "                        time.sleep(15)\n",
    "                        title_index = 3\n",
    "                        stats_index = 4\n",
    "                        topic_index = 0\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                        element = driver.find_elements_by_xpath(x_path_title)\n",
    "            #This line saves the Excel file of papers\n",
    "            df.to_excel(out_path / file_name, index=False)\n",
    "            # We have created a file, so we need to append the link and the file name to the list of scraped pages\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            time.sleep(3)\n",
    "            #If we reach this point, all the pages for that author have been scraped\n",
    "            print('Done scraping for {}.'.format(fm_name + ' ' + last_name))\n",
    "        else:\n",
    "            print(\"No data was found for {}. Moving to the next name.\".format(full_name))\n",
    "# Export the updated dataframe of skipped names and scraped pages\n",
    "skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "df_scraped_pages.to_excel(scraped_pages_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}