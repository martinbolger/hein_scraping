{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "web_scraping",
   "display_name": "web_scraping",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names\n",
    "from modules.data_manipulation_functions import remove_commas, check_files, concat_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already exists. Names that have already been scraped will be skipped\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "input_data = pd.read_excel(intr_path / \"hein_scraping_input_data.xlsx\")\n",
    "data = input_data\n",
    "data_type = \"lateral\"\n",
    "\n",
    "# Create the list of scraped pages columns\n",
    "scraped_pages_columns = [\"links\", \"file_names\", \"professor_names\", \"id\"]\n",
    "# Load the list of scraped pages if it exists\n",
    "scraped_pages_file = out_path / \"_scraped_pages.xlsx\"\n",
    "if scraped_pages_file.exists():\n",
    "    print(\"Data already exists. Names that have already been scraped will be skipped\")\n",
    "    # Create the dataset of existing alt names.\n",
    "    df_scraped_pages = pd.read_excel(scraped_pages_file)\n",
    "else:\n",
    "    df_scraped_pages = pd.DataFrame(columns = scraped_pages_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      ID Short URL Destination Short URL Origin  FirstName     LastName  \\\n",
       "0      1              duke.edu        upenn.edu    Matthew        Adler   \n",
       "1      2               gsu.edu  avemarialaw.edu     Edward       Afield   \n",
       "2      3            utexas.edu           bc.edu    Richard       Albert   \n",
       "3      4              tamu.edu         wisc.edu       Lisa    Alexander   \n",
       "4      5          american.edu      suffolk.edu     Hilary        Allen   \n",
       "..   ...                   ...              ...        ...          ...   \n",
       "291  283           hofstra.edu           yu.edu      Ellen  Yaroshefsky   \n",
       "292  284               slu.edu         case.edu  Ruqaiijah       Yearby   \n",
       "293  285              tamu.edu        drake.edu      Peter           Yu   \n",
       "294  286                bu.edu   georgetown.edu    Kathryn       Zeiler   \n",
       "295  287               umt.edu          unl.edu      Sandi      Zellmer   \n",
       "\n",
       "     Lateral  LateralYear                                  Origin School  \\\n",
       "0          1         2012          University of Pennsylvania Law School   \n",
       "1          1         2016                        Ave Maria School of Law   \n",
       "2          1         2017                      Boston College Law School   \n",
       "3          1         2016             University of Wisconsin Law School   \n",
       "4          1         2018                  Suffolk University Law School   \n",
       "..       ...          ...                                            ...   \n",
       "291        1         2016                             Cardozo Law School   \n",
       "292        1         2018  Case Western Reserve University School of Law   \n",
       "293        1         2015                    Drake University Law School   \n",
       "294        1         2015               Georgetown University Law Center   \n",
       "295        1         2018          University of Nebraska College of Law   \n",
       "\n",
       "                                    Destination School  multi_obs  \\\n",
       "0                        Duke University School of Law      False   \n",
       "1                         Georgia State College of Law      False   \n",
       "2                    University of Texas School of Law      False   \n",
       "3                   Texas A&M University School of Law      False   \n",
       "4        American University Washington College of Law      False   \n",
       "..                                                 ...        ...   \n",
       "291  Maurice A. Deane School of Law at Hofstra Univ...      False   \n",
       "292               Saint Louis University School of Law      False   \n",
       "293                 Texas A&M University School of Law      False   \n",
       "294                    Boston University School of Law      False   \n",
       "295  University of Montana Alexander Blewett III Sc...      False   \n",
       "\n",
       "                    fm_names  \n",
       "0        Matthew, Matthew D.  \n",
       "1                  W. Edward  \n",
       "2                    Richard  \n",
       "3                    Lisa T.  \n",
       "4                  Hilary J.  \n",
       "..                       ...  \n",
       "291                    Ellen  \n",
       "292  Ruqaiijah, Ruqaiijah A.  \n",
       "293                 Peter K.  \n",
       "294                  Kathryn  \n",
       "295         Sandi B., Sandra  \n",
       "\n",
       "[296 rows x 11 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Short URL Destination</th>\n      <th>Short URL Origin</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>Lateral</th>\n      <th>LateralYear</th>\n      <th>Origin School</th>\n      <th>Destination School</th>\n      <th>multi_obs</th>\n      <th>fm_names</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>duke.edu</td>\n      <td>upenn.edu</td>\n      <td>Matthew</td>\n      <td>Adler</td>\n      <td>1</td>\n      <td>2012</td>\n      <td>University of Pennsylvania Law School</td>\n      <td>Duke University School of Law</td>\n      <td>False</td>\n      <td>Matthew, Matthew D.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>gsu.edu</td>\n      <td>avemarialaw.edu</td>\n      <td>Edward</td>\n      <td>Afield</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>Ave Maria School of Law</td>\n      <td>Georgia State College of Law</td>\n      <td>False</td>\n      <td>W. Edward</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>utexas.edu</td>\n      <td>bc.edu</td>\n      <td>Richard</td>\n      <td>Albert</td>\n      <td>1</td>\n      <td>2017</td>\n      <td>Boston College Law School</td>\n      <td>University of Texas School of Law</td>\n      <td>False</td>\n      <td>Richard</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>tamu.edu</td>\n      <td>wisc.edu</td>\n      <td>Lisa</td>\n      <td>Alexander</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>University of Wisconsin Law School</td>\n      <td>Texas A&amp;M University School of Law</td>\n      <td>False</td>\n      <td>Lisa T.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>american.edu</td>\n      <td>suffolk.edu</td>\n      <td>Hilary</td>\n      <td>Allen</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>Suffolk University Law School</td>\n      <td>American University Washington College of Law</td>\n      <td>False</td>\n      <td>Hilary J.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>291</th>\n      <td>283</td>\n      <td>hofstra.edu</td>\n      <td>yu.edu</td>\n      <td>Ellen</td>\n      <td>Yaroshefsky</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>Cardozo Law School</td>\n      <td>Maurice A. Deane School of Law at Hofstra Univ...</td>\n      <td>False</td>\n      <td>Ellen</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>284</td>\n      <td>slu.edu</td>\n      <td>case.edu</td>\n      <td>Ruqaiijah</td>\n      <td>Yearby</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>Case Western Reserve University School of Law</td>\n      <td>Saint Louis University School of Law</td>\n      <td>False</td>\n      <td>Ruqaiijah, Ruqaiijah A.</td>\n    </tr>\n    <tr>\n      <th>293</th>\n      <td>285</td>\n      <td>tamu.edu</td>\n      <td>drake.edu</td>\n      <td>Peter</td>\n      <td>Yu</td>\n      <td>1</td>\n      <td>2015</td>\n      <td>Drake University Law School</td>\n      <td>Texas A&amp;M University School of Law</td>\n      <td>False</td>\n      <td>Peter K.</td>\n    </tr>\n    <tr>\n      <th>294</th>\n      <td>286</td>\n      <td>bu.edu</td>\n      <td>georgetown.edu</td>\n      <td>Kathryn</td>\n      <td>Zeiler</td>\n      <td>1</td>\n      <td>2015</td>\n      <td>Georgetown University Law Center</td>\n      <td>Boston University School of Law</td>\n      <td>False</td>\n      <td>Kathryn</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>287</td>\n      <td>umt.edu</td>\n      <td>unl.edu</td>\n      <td>Sandi</td>\n      <td>Zellmer</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>University of Nebraska College of Law</td>\n      <td>University of Montana Alexander Blewett III Sc...</td>\n      <td>False</td>\n      <td>Sandi B., Sandra</td>\n    </tr>\n  </tbody>\n</table>\n<p>296 rows Ã— 11 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tion=edit&search_name=Roederer%2C Christopher J.&collection=journals has already been scraped. Moving to the next name.\n",
      "Benjamin van Rooij\n",
      "Name list: ['Benjamin', 'Benjamin']\n",
      "Looking for Benjamin van Rooij\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=van Rooij%2C Benjamin&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Benjamin van Rooij\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=van Rooij%2C Benjamin&collection=journals has already been scraped. Moving to the next name.\n",
      "Jim Rossi\n",
      "Name list: ['Jim']\n",
      "Looking for Jim Rossi\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Rossi%2C Jim&collection=journals has already been scraped. Moving to the next name.\n",
      "Troy Rule\n",
      "Name list: ['Troy', 'Troy A.']\n",
      "Looking for Troy Rule\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Rule%2C Troy&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Troy A. Rule\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Rule%2C Troy A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Erin Ryan\n",
      "Name list: ['Erin']\n",
      "Looking for Erin Ryan\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Ryan%2C Erin&collection=journals has already been scraped. Moving to the next name.\n",
      "Victoria Sahani\n",
      "Name list: ['Victoria Shannon']\n",
      "Looking for Victoria Shannon Sahani\n",
      "Scraping the page\n",
      "Done scraping for Victoria Shannon Sahani.\n",
      "Victoria Shannon\n",
      "Name list: ['Victoria', 'Victoria A.']\n",
      "Looking for Victoria Shannon\n",
      "Scraping the page\n",
      "Done scraping for Victoria Shannon.\n",
      "Looking for Victoria A. Shannon\n",
      "Scraping the page\n",
      "Done scraping for Victoria A. Shannon.\n",
      "Hillary Sale\n",
      "Name list: ['Hillary A.']\n",
      "Looking for Hillary A. Sale\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sale%2C Hillary A.&collection=journals has already been scraped. Moving to the next name.\n",
      "James Salzman\n",
      "Name list: ['James']\n",
      "Looking for James Salzman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Salzman%2C James&collection=journals has already been scraped. Moving to the next name.\n",
      "Adam Samaha\n",
      "Name list: ['Adam M.']\n",
      "Looking for Adam M. Samaha\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Samaha%2C Adam M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Amy Schmitz\n",
      "Name list: ['Amy J.']\n",
      "Looking for Amy J. Schmitz\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Schmitz%2C Amy J.&collection=journals has already been scraped. Moving to the next name.\n",
      "David Schwartz\n",
      "Name list: ['David', 'David L.']\n",
      "Looking for David Schwartz\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Schwartz%2C David&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for David L. Schwartz\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Schwartz%2C David L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Christopher Serkin\n",
      "Name list: ['Christopher']\n",
      "Looking for Christopher Serkin\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Serkin%2C Christopher&collection=journals has already been scraped. Moving to the next name.\n",
      "Gregory Shaffer\n",
      "Name list: ['Gregory']\n",
      "Looking for Gregory Shaffer\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Shaffer%2C Gregory&collection=journals has already been scraped. Moving to the next name.\n",
      "Darien Shanske\n",
      "Name list: ['Darien']\n",
      "Looking for Darien Shanske\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Shanske%2C Darien&collection=journals has already been scraped. Moving to the next name.\n",
      "Jessica Silbey\n",
      "Name list: ['Jessica']\n",
      "Looking for Jessica Silbey\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Silbey%2C Jessica&collection=journals has already been scraped. Moving to the next name.\n",
      "Michael Simkovic\n",
      "Name list: ['Michael', 'Michael N.']\n",
      "Looking for Michael Simkovic\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Simkovic%2C Michael&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Michael N. Simkovic\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Simkovic%2C Michael N.&collection=journals has already been scraped. Moving to the next name.\n",
      "Beth Simmons\n",
      "Name list: ['Beth', 'Beth A.']\n",
      "Looking for Beth Simmons\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Simmons%2C Beth&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Beth A. Simmons\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Simmons%2C Beth A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kenneth Simons\n",
      "Name list: ['Kenneth W.']\n",
      "Looking for Kenneth W. Simons\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Simons%2C Kenneth W.&collection=journals has already been scraped. Moving to the next name.\n",
      "David Sklansky\n",
      "Name list: ['David Alan', 'David A.']\n",
      "Looking for David Alan Sklansky\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sklansky%2C David Alan&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for David A. Sklansky\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sklansky%2C David A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Brad Snyder\n",
      "Name list: ['Brad']\n",
      "Looking for Brad Snyder\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Snyder%2C Brad&collection=journals has already been scraped. Moving to the next name.\n",
      "A. Spencer\n",
      "Name list: ['A. Benjamin']\n",
      "Looking for A. Benjamin Spencer\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Spencer%2C A. Benjamin&collection=journals has already been scraped. Moving to the next name.\n",
      "Matthew Spitzer\n",
      "Name list: ['Matthew', 'Matthew L.']\n",
      "Looking for Matthew Spitzer\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Spitzer%2C Matthew&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Matthew L. Spitzer\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Spitzer%2C Matthew L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Christopher Sprigman\n",
      "Name list: ['Christopher', 'Christopher Jon']\n",
      "Looking for Christopher Sprigman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sprigman%2C Christopher&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Christopher Jon Sprigman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sprigman%2C Christopher Jon&collection=journals has already been scraped. Moving to the next name.\n",
      "Paul Stancil\n",
      "Name list: ['Paul']\n",
      "Looking for Paul Stancil\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stancil%2C Paul&collection=journals has already been scraped. Moving to the next name.\n",
      "Jean Stefancic\n",
      "Name list: ['Jean']\n",
      "Looking for Jean Stefancic\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stefancic%2C Jean&collection=journals has already been scraped. Moving to the next name.\n",
      "Alex Stein\n",
      "Name list: ['Alex']\n",
      "Looking for Alex Stein\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stein%2C Alex&collection=journals has already been scraped. Moving to the next name.\n",
      "Adam Steinman\n",
      "Name list: ['Adam', 'Adam N.']\n",
      "Looking for Adam Steinman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Steinman%2C Adam&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Adam N. Steinman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Steinman%2C Adam N.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kristen Stilt\n",
      "Name list: ['Kristen', 'Kristen A.']\n",
      "Looking for Kristen Stilt\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stilt%2C Kristen&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Kristen A. Stilt\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Stilt%2C Kristen A.&collection=journals has already been scraped. Moving to the next name.\n",
      "David Studdert\n",
      "Name list: ['David M.', 'David']\n",
      "Looking for David M. Studdert\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Studdert%2C David M.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for David Studdert\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Studdert%2C David&collection=journals has already been scraped. Moving to the next name.\n",
      "Madhavi Sunder\n",
      "Name list: ['Madhavi']\n",
      "Looking for Madhavi Sunder\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sunder%2C Madhavi&collection=journals has already been scraped. Moving to the next name.\n",
      "Alan Sykes\n",
      "Name list: ['Alan', 'Alan O.']\n",
      "Looking for Alan Sykes\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sykes%2C Alan&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Alan O. Sykes\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Sykes%2C Alan O.&collection=journals has already been scraped. Moving to the next name.\n",
      "Eric Talley\n",
      "Name list: ['Eric', 'Eric L.']\n",
      "Looking for Eric Talley\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Talley%2C Eric&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Eric L. Talley\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Talley%2C Eric L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kim Talus\n",
      "Name list: ['Kim']\n",
      "Looking for Kim Talus\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Talus%2C Kim&collection=journals has already been scraped. Moving to the next name.\n",
      "Franita Tolson\n",
      "Name list: ['Franita']\n",
      "Looking for Franita Tolson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tolson%2C Franita&collection=journals has already been scraped. Moving to the next name.\n",
      "Christopher Tomlins\n",
      "Name list: ['Christopher', 'Christopher L.']\n",
      "Looking for Christopher Tomlins\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tomlins%2C Christopher&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Christopher L. Tomlins\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tomlins%2C Christopher L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Gerald Torres\n",
      "Name list: ['Gerald']\n",
      "Looking for Gerald Torres\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Torres%2C Gerald&collection=journals has already been scraped. Moving to the next name.\n",
      "Elizabeth Trujillo\n",
      "Name list: ['Elizabeth']\n",
      "Looking for Elizabeth Trujillo\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Trujillo%2C Elizabeth&collection=journals has already been scraped. Moving to the next name.\n",
      "Deborah Tuerkheimer\n",
      "Name list: ['Deborah']\n",
      "Looking for Deborah Tuerkheimer\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tuerkheimer%2C Deborah&collection=journals has already been scraped. Moving to the next name.\n",
      "Rebecca Tushnet\n",
      "Name list: ['Rebecca']\n",
      "Looking for Rebecca Tushnet\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tushnet%2C Rebecca&collection=journals has already been scraped. Moving to the next name.\n",
      "Amanda Tyler\n",
      "Name list: ['Amanda L.']\n",
      "Looking for Amanda L. Tyler\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Tyler%2C Amanda L.&collection=journals has already been scraped. Moving to the next name.\n",
      "Ryan Vacca\n",
      "Name list: ['Ryan', 'Ryan G.']\n",
      "Looking for Ryan Vacca\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Vacca%2C Ryan&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Ryan G. Vacca\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Vacca%2C Ryan G.&collection=journals has already been scraped. Moving to the next name.\n",
      "Katharine Van Tassel\n",
      "Name list: ['Katharine']\n",
      "Looking for Katharine Van Tassel\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Van Tassel%2C Katharine&collection=journals has already been scraped. Moving to the next name.\n",
      "Urska Velikonja\n",
      "Name list: ['Urska']\n",
      "Looking for Urska Velikonja\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Velikonja%2C Urska&collection=journals has already been scraped. Moving to the next name.\n",
      "Rose Cuison\n",
      "Name list: ['Rose Cruz']\n",
      "Looking for Rose Cruz Cuison\n",
      "Scraping the page\n",
      "Done scraping for Rose Cruz Cuison.\n",
      "Rose Villazor\n",
      "Name list: ['Rose Cuison']\n",
      "Looking for Rose Cuison Villazor\n",
      "Scraping the page\n",
      "Done scraping for Rose Cuison Villazor.\n",
      "Steve Vladeck\n",
      "Name list: ['Steve', 'Stephen']\n",
      "Looking for Steve Vladeck\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Vladeck%2C Steve&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Stephen Vladeck\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Vladeck%2C Stephen&collection=journals has already been scraped. Moving to the next name.\n",
      "Gina Warren\n",
      "Name list: ['Gina', 'Gina S.']\n",
      "Looking for Gina Warren\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Warren%2C Gina&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Gina S. Warren\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Warren%2C Gina S.&collection=journals has already been scraped. Moving to the next name.\n",
      "Melissa Wasserman\n",
      "Name list: ['Melissa F.', 'Melissa Feeney']\n",
      "Looking for Melissa F. Wasserman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wasserman%2C Melissa F.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Melissa Feeney Wasserman\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wasserman%2C Melissa Feeney&collection=journals has already been scraped. Moving to the next name.\n",
      "Nancy Welsh\n",
      "Name list: ['Nancy', 'Nancy A.']\n",
      "Looking for Nancy Welsh\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Welsh%2C Nancy&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Nancy A. Welsh\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Welsh%2C Nancy A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Robin Wilson\n",
      "Name list: ['Robin', 'Robin Fretwell']\n",
      "Looking for Robin Wilson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wilson%2C Robin&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Robin Fretwell Wilson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wilson%2C Robin Fretwell&collection=journals has already been scraped. Moving to the next name.\n",
      "Andrew Woods\n",
      "Name list: ['Andrew Keane', 'Andrew K.']\n",
      "Looking for Andrew Keane Woods\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Woods%2C Andrew Keane&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Andrew K. Woods\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Woods%2C Andrew K.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kevin Woodson\n",
      "Name list: ['Kevin']\n",
      "Looking for Kevin Woodson\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Woodson%2C Kevin&collection=journals has already been scraped. Moving to the next name.\n",
      "Del Wright\n",
      "Name list: ['Del Jr.']\n",
      "Looking for Del Jr. Wright\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Wright%2C Del Jr.&collection=journals has already been scraped. Moving to the next name.\n",
      "Gideon Yaffe\n",
      "Name list: ['Gideon']\n",
      "Looking for Gideon Yaffe\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Yaffe%2C Gideon&collection=journals has already been scraped. Moving to the next name.\n",
      "Ellen Yaroshefsky\n",
      "Name list: ['Ellen']\n",
      "Looking for Ellen Yaroshefsky\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Yaroshefsky%2C Ellen&collection=journals has already been scraped. Moving to the next name.\n",
      "Ruqaiijah Yearby\n",
      "Name list: ['Ruqaiijah', 'Ruqaiijah A.']\n",
      "Looking for Ruqaiijah Yearby\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Yearby%2C Ruqaiijah&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Ruqaiijah A. Yearby\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Yearby%2C Ruqaiijah A.&collection=journals has already been scraped. Moving to the next name.\n",
      "Peter Yu\n",
      "Name list: ['Peter K.']\n",
      "Looking for Peter K. Yu\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Yu%2C Peter K.&collection=journals has already been scraped. Moving to the next name.\n",
      "Kathryn Zeiler\n",
      "Name list: ['Kathryn']\n",
      "Looking for Kathryn Zeiler\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zeiler%2C Kathryn&collection=journals has already been scraped. Moving to the next name.\n",
      "Sandi Zellmer\n",
      "Name list: ['Sandi B.', 'Sandra']\n",
      "Looking for Sandi B. Zellmer\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zellmer%2C Sandi B.&collection=journals has already been scraped. Moving to the next name.\n",
      "Looking for Sandra Zellmer\n",
      "The link https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=Zellmer%2C Sandra&collection=journals has already been scraped. Moving to the next name.\n"
     ]
    }
   ],
   "source": [
    "# Initilization\n",
    "# Page name is a list of the name for all of the pages that we have scraped.\n",
    "# This is the name that actually appears on the webpage. This helps prevent\n",
    "# us from having to rescrape pages multiple times.\n",
    "err_fm_names = []\n",
    "skip_df = pd.DataFrame()\n",
    "\n",
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    # Export the updated dataframe of skipped names and scraped pages\n",
    "    skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "    df_scraped_pages.to_excel(scraped_pages_file, index = False)\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    #This line gets the school URLs from the dataframe\n",
    "    if data_type == \"lateral\":\n",
    "        school_url = [data['Short URL Origin'][i], data['Short URL Destination'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "        new_school = data['Destination School'][i]\n",
    "    elif data_type == \"control\":\n",
    "        school_url = [data['Short URL Origin'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "    # Create the multiple observation variable\n",
    "    multi_obs = data[\"multi_obs\"][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "\n",
    "    # If there were no matching names, the value is nan. This means that the value does not equal itself.\n",
    "    #  The name is added to the skipped names list and the loop moves onto the next name. \n",
    "    fm_names_str = data['fm_names'][i]\n",
    "    if fm_names_str != fm_names_str:\n",
    "        print('Name ' + full_name + ' was not found. Adding to the skipped names dataset.')\n",
    "        skip_df = pd.concat([skip_df, data.iloc[[i]]])\n",
    "        continue\n",
    "\n",
    "    fm_names = fm_names_str.split(\", \")\n",
    "    print(\"Name list: {}\".format(fm_names))   \n",
    "        \n",
    "    #This section loops through the list of alternative names and goes directly to their pages on Hein\n",
    "    for fm_name in fm_names:\n",
    "        # Create the full name\n",
    "        full_name = fm_name + ' ' +  last_name\n",
    "        print(\"Looking for {}\".format(full_name))\n",
    "        # Check if the file exists\n",
    "        file_name = '{}_{}_papers.xlsx'.format(full_name, prof_id)            \n",
    "\n",
    "        #Link to Hein page\n",
    "        link = 'https://heinonline-org.proxy01.its.virginia.edu/HOL/AuthorProfile?action=edit&search_name=' + last_name +  '%2C ' + fm_name + '&collection=journals'\n",
    "\n",
    "        # CHECK DATA: This is the first spot where we check the data. We want to see if the link that we are\n",
    "        # going to has already been added to the list of scraped pages. This is helpful when rerunning the code.\n",
    "        if not df_scraped_pages.query('@link == links').empty and not multi_obs:\n",
    "            print(\"The link {} has already been scraped. Moving to the next name.\".format(link))\n",
    "            continue\n",
    "        #Direct the webdriver to the page\n",
    "        driver.get(link)\n",
    "        #This function waits for the webpage to load\n",
    "        webpage_wait('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]', driver)\n",
    "        \n",
    "        # Make sure that the data exists on the page. Otherwise, we will skip the page.\n",
    "        try:\n",
    "            no_data_text = driver.find_element_by_xpath('//*[@id=\"luceneres\"]/b').text\n",
    "            if no_data_text == \"No matching results found\":\n",
    "                data_exists = False\n",
    "        except NoSuchElementException:\n",
    "            data_exists = True\n",
    "\n",
    "        # This is the name for the professor that is used on the page.\n",
    "        cur_page = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[1]/div[1]').text\n",
    "\n",
    "        # CHECK DATA: This is the second spot where we check the data to see if the page has\n",
    "        # already been scraped. In order to check to see if the link has already been scraped, \n",
    "        # we look for the professor name on the page (which may be different from the name in our list) \n",
    "        # and the professor ID. This is helpful when two names point to the same page.\n",
    "        if not df_scraped_pages.query('@cur_page == professor_names and @prof_id == id').empty and not multi_obs: \n",
    "            print(\"A file for {} already exists. Moving to the next name.\".format(full_name))\n",
    "            # Add the link to the data so that we know to skip it in future runs\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            continue\n",
    "        #If there is a table on the page and the page name has not already appeared in the scraped list.\n",
    "        if data_exists: \n",
    "            element = driver.find_element_by_xpath('//*[@id=\"page_content\"]/div[1]/div/div[2]')\n",
    "            table_element = element.text.split('\\n')\n",
    "            #If the table is empty, there is no data to scrape\n",
    "            if len(table_element) < 5:\n",
    "                print('No data available on Hein for {} {}'.format(fm_name, last_name))\n",
    "                continue\n",
    "            #If the table is full, this section rearranges the data into a better format\n",
    "            print(\"Scraping the page\")                \n",
    "            #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\n",
    "            #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \n",
    "            #papers. The xpaths change when the page scrolls.\n",
    "            title_index = 3\n",
    "            stats_index = 4\n",
    "            topic_index = 0\n",
    "            scroll_num = 0\n",
    "            #This gets the page source\n",
    "            soup=bs.BeautifulSoup(driver.page_source, 'lxml')\n",
    "            #This section gets the paper topics\n",
    "            topic_array = soup.findAll('div', {'class': 'topics'})\n",
    "            element = title_index\n",
    "            df = pd.DataFrame(columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed'])\n",
    "            #This while loop will continue until there are no more papers on the page\n",
    "            while element:\n",
    "                #Data stream is a list of the data in the paper data box (for example, authors, topics, journal)\n",
    "                data_stream = []\n",
    "                #This funciton returns a dictionary with various fields for each variable in the data box\n",
    "                #Sometimes some of the variables are missing (for example, there are papers without a journal listed)\n",
    "                #In this case, the dictionary returns an empty value for these variables\n",
    "                data_dict = get_paper_data(last_name, prof_id, title_index, scroll_num, driver)\n",
    "                #This section gets the paper stats box. This is the box that says how many citations the paper\n",
    "                #has received\n",
    "                if scroll_num == 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                elif scroll_num > 0:\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div')\n",
    "                #This section extracts the data from the paper stats box\n",
    "                for elm in element:\n",
    "                    cited_text = elm.text\n",
    "                article_citations = 'na'\n",
    "                case_citations = 'na'\n",
    "                accessed = 'na'\n",
    "                if not isinstance(cited_text, list):\n",
    "                    cited_text = cited_text.split('\\n')\n",
    "                    #This section finds the value for each paper stat\n",
    "                    for stat in cited_text:\n",
    "                        if 'Article' in stat:\n",
    "                            article_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Case' in stat:\n",
    "                            case_citations = int(re.search(r'\\d+', stat).group())\n",
    "                        if 'Accessed' in stat:\n",
    "                            accessed = int(re.search(r'\\d+', stat).group())\n",
    "                #The values are appended to the data_stream list\n",
    "                data_stream.append(article_citations)\n",
    "                data_stream.append(case_citations)\n",
    "                data_stream.append(accessed)\n",
    "                #This line adds the output from the function get_paper_data to the data_stream list\n",
    "                data_stream = list(data_dict.values()) + data_stream\n",
    "                #The data_stream list is used to add a line of data to the overall paper dataframe for this author\n",
    "                df = df.append(pd.DataFrame([data_stream], columns = ['Title', 'Author(s)', 'ID', 'Journal', 'BBCite', 'Topics', 'Cited (articles)', 'Cited (cases)', 'Accessed']), sort=False)\n",
    "                #The indices are augmented to get the next paper\n",
    "                stats_index +=4\n",
    "                title_index += 4\n",
    "                #Check that next paper exists:\n",
    "                if scroll_num == 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                #If the page has scrolled, the xpath we need to check has changed\n",
    "                if scroll_num > 0:\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                element = driver.find_elements_by_xpath(x_path_title)\n",
    "                #If we can't find a next paper, it could be because we need to scroll again\n",
    "                #This section attempts to scroll the page. \n",
    "                if not element:\n",
    "                    scroll_num +=1\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\n",
    "                    num_papers = int(box_element[0].text.split(' ')[0])\n",
    "                    #If there are more than 100 papers, we know there are still paper left to scrape\n",
    "                    if num_papers > 100*scroll_num:\n",
    "                        time.sleep(15)\n",
    "                        title_index = 3\n",
    "                        stats_index = 4\n",
    "                        topic_index = 0\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\n",
    "                        element = driver.find_elements_by_xpath(x_path_title)\n",
    "            #This line saves the Excel file of papers\n",
    "            df.to_excel(out_path / '{}_{}_papers.xlsx'.format(full_name, prof_id), index=False)\n",
    "            # We have created a file, so we need to append the link and the file name to the list of scraped pages\n",
    "            values_scraped_pages = [link, file_name, cur_page, prof_id]\n",
    "            dict_values_scraped_pages = dict(zip(scraped_pages_columns, values_scraped_pages))\n",
    "            df_scraped_pages = df_scraped_pages.append([dict_values_scraped_pages])\n",
    "            time.sleep(3)\n",
    "            #If we reach this point, all the pages for that author have been scraped\n",
    "            print('Done scraping for {}.'.format(fm_name + ' ' + last_name))\n",
    "        else:\n",
    "            print(\"No data was found for {}. Moving to the next name.\".format(full_name))\n",
    "# Export the updated dataframe of skipped names and scraped pages\n",
    "skip_df.to_excel(out_path / \"_skip_output.xlsx\", index = False)\n",
    "df_scraped_pages.to_excel(scraped_pages_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}