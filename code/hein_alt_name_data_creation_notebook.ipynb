{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "import modules.hein_scraping_functions\n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_bing, get_similar_names, check_similar_names, search_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()\n",
    "\n",
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "g_driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "s_driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")\n",
    "s_driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Short URL Destination Short URL Origin FirstName   LastName   ID  Lateral  \\\n",
       "0                duke.edu        upenn.edu   Matthew      Adler    1        1   \n",
       "1                 gsu.edu  avemarialaw.edu    Edward     Afield    2        1   \n",
       "2              utexas.edu           bc.edu   Richard     Albert    3        1   \n",
       "3                tamu.edu         wisc.edu      Lisa  Alexander    4        1   \n",
       "4            american.edu      suffolk.edu    Hilary      Allen    5        1   \n",
       "..                    ...              ...       ...        ...  ...      ...   \n",
       "294      northeastern.edu          wlu.edu  Victoria    Shannon  235        1   \n",
       "295           rutgers.edu      ucdavis.edu      Rose     Cuison  273        1   \n",
       "296           buffalo.edu         pace.edu      Luis     Aponte   47        1   \n",
       "297               gwu.edu          wfu.edu     Emily    Meazell  103        1   \n",
       "298               fsu.edu           ubc.ca  Shi-Ling       Hsu,  123        1   \n",
       "\n",
       "     LateralYear                                 Origin School  \\\n",
       "0           2012         University of Pennsylvania Law School   \n",
       "1           2016                       Ave Maria School of Law   \n",
       "2           2017                     Boston College Law School   \n",
       "3           2016            University of Wisconsin Law School   \n",
       "4           2018                 Suffolk University Law School   \n",
       "..           ...                                           ...   \n",
       "294         2017   Washington and Lee University School of Law   \n",
       "295         2017                        UC Davis School of Law   \n",
       "296         2013  Pace University Elisabeth Haub School of Law   \n",
       "297         2014          Wake Forest University School of Law   \n",
       "298         2012                University of British Columbia   \n",
       "\n",
       "                                   Destination School  multi_obs  \n",
       "0                       Duke University School of Law      False  \n",
       "1                        Georgia State College of Law      False  \n",
       "2                   University of Texas School of Law      False  \n",
       "3                  Texas A&M University School of Law      False  \n",
       "4       American University Washington College of Law      False  \n",
       "..                                                ...        ...  \n",
       "294  Arizona State Sandra Day O'Connor College of Law       True  \n",
       "295                                Rutgers Law Newark       True  \n",
       "296               University at Buffalo School of Law       True  \n",
       "297           George Washington University Law School       True  \n",
       "298           Florida State University College of Law       True  \n",
       "\n",
       "[299 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Short URL Destination</th>\n      <th>Short URL Origin</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>ID</th>\n      <th>Lateral</th>\n      <th>LateralYear</th>\n      <th>Origin School</th>\n      <th>Destination School</th>\n      <th>multi_obs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>duke.edu</td>\n      <td>upenn.edu</td>\n      <td>Matthew</td>\n      <td>Adler</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2012</td>\n      <td>University of Pennsylvania Law School</td>\n      <td>Duke University School of Law</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gsu.edu</td>\n      <td>avemarialaw.edu</td>\n      <td>Edward</td>\n      <td>Afield</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>Ave Maria School of Law</td>\n      <td>Georgia State College of Law</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>utexas.edu</td>\n      <td>bc.edu</td>\n      <td>Richard</td>\n      <td>Albert</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2017</td>\n      <td>Boston College Law School</td>\n      <td>University of Texas School of Law</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tamu.edu</td>\n      <td>wisc.edu</td>\n      <td>Lisa</td>\n      <td>Alexander</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2016</td>\n      <td>University of Wisconsin Law School</td>\n      <td>Texas A&amp;M University School of Law</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>american.edu</td>\n      <td>suffolk.edu</td>\n      <td>Hilary</td>\n      <td>Allen</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>Suffolk University Law School</td>\n      <td>American University Washington College of Law</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>294</th>\n      <td>northeastern.edu</td>\n      <td>wlu.edu</td>\n      <td>Victoria</td>\n      <td>Shannon</td>\n      <td>235</td>\n      <td>1</td>\n      <td>2017</td>\n      <td>Washington and Lee University School of Law</td>\n      <td>Arizona State Sandra Day O'Connor College of Law</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>rutgers.edu</td>\n      <td>ucdavis.edu</td>\n      <td>Rose</td>\n      <td>Cuison</td>\n      <td>273</td>\n      <td>1</td>\n      <td>2017</td>\n      <td>UC Davis School of Law</td>\n      <td>Rutgers Law Newark</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>buffalo.edu</td>\n      <td>pace.edu</td>\n      <td>Luis</td>\n      <td>Aponte</td>\n      <td>47</td>\n      <td>1</td>\n      <td>2013</td>\n      <td>Pace University Elisabeth Haub School of Law</td>\n      <td>University at Buffalo School of Law</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>gwu.edu</td>\n      <td>wfu.edu</td>\n      <td>Emily</td>\n      <td>Meazell</td>\n      <td>103</td>\n      <td>1</td>\n      <td>2014</td>\n      <td>Wake Forest University School of Law</td>\n      <td>George Washington University Law School</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>fsu.edu</td>\n      <td>ubc.ca</td>\n      <td>Shi-Ling</td>\n      <td>Hsu,</td>\n      <td>123</td>\n      <td>1</td>\n      <td>2012</td>\n      <td>University of British Columbia</td>\n      <td>Florida State University College of Law</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>299 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Data type: This is the type of data that we are using.\n",
    "# The control group members only have one university url because they\n",
    "# did not move. The lateral group members have two urls.\n",
    "data_type = \"lateral\"\n",
    "\n",
    "# Load the dataset from the working directory\n",
    "# The dataset in the working directory has already \n",
    "# been cleaned.\n",
    "data = pd.read_excel(work_path / \"{}.xlsx\".format(data_type))\n",
    "\n",
    "# Load the name modification dataset\n",
    "name_mod = pd.read_excel(input_path / \"name_mod_{}.xlsx\".format(data_type))\n",
    "\n",
    "# MERGE: Right join the name mod data to get the observations with a different last name\n",
    "alt_name_diff_last_name = pd.merge(data, name_mod[[\"ID\", \"diff_last_name\"]], how = \"right\", left_on = [\"ID\"], right_on = [\"ID\"])\n",
    "\n",
    "# Subset to the non-missing different last names\n",
    "alt_name_diff_last_name = alt_name_diff_last_name[alt_name_diff_last_name[\"diff_last_name\"].notnull()]\n",
    "# Create a copy of the alternate last name observations with a different last name\n",
    "alt_name_diff_last_name[\"LastName\"] = alt_name_diff_last_name[\"diff_last_name\"]\n",
    "# Drop the diff_last_name column\n",
    "alt_name_diff_last_name = alt_name_diff_last_name.drop([\"diff_last_name\"], axis = 1)\n",
    "\n",
    "# Add the observations onto the main dataset\n",
    "data = pd.concat([data, alt_name_diff_last_name], ignore_index = True)\n",
    "\n",
    "# Flag any observations that now show up multiple times\n",
    "data[\"multi_obs\"] = data[\"ID\"].duplicated(keep=False)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already exists. Names that have already been scraped will be skipped\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store the alternate name data\n",
    "alt_names_data = []\n",
    "# Create the alt-names dataset column list\n",
    "alt_names_columns = [\"ID\", \"FirstName\", \"LastName\", \"fm_names\", \"err_fm_names\", \"multi_obs\"]\n",
    "\n",
    "# Check to see if the file for the alternate names data already exists.\n",
    "# If it does, we only want to looks for the missing observations\n",
    "alt_names_file = intr_path / \"alt_names.xlsx\"\n",
    "if alt_names_file.exists():\n",
    "    print(\"Data already exists. Names that have already been scraped will be skipped\")\n",
    "    # Set the append flag to 1\n",
    "    append = 1\n",
    "    # Create the dataset of existing alt names.\n",
    "    df_existing_alt_names = pd.read_excel(alt_names_file)\n",
    "    # Complete a left outer join of the existing alt names and the lateral/control data to get \n",
    "    # a list of the names that we still need to scrape alt names for.\n",
    "    data = pd.merge(data, df_existing_alt_names[\"ID\"], how = \"outer\", left_on = \"ID\", right_on = \"ID\", indicator=True)\n",
    "    data = data[data['_merge'] == 'left_only']\n",
    "    data = data.drop([\"_merge\"], axis = 1)\n",
    "else:\n",
    "    # Set the append flag to zero because we won't have any data to append\n",
    "    append = 0\n",
    "# Data contains the observations that were not available in the existing alt_names file.\n",
    "# If there is no existing file, it just contains the lateral/control data.\n",
    "data.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Short URL Destination Short URL Origin FirstName LastName   ID  Lateral  \\\n",
       "0              duke.edu          gwu.edu        H.   Powell  216        1   \n",
       "\n",
       "   LateralYear                            Origin School  \\\n",
       "0         2012  George Washington University Law School   \n",
       "\n",
       "              Destination School  multi_obs  \n",
       "0  Duke University School of Law      False  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Short URL Destination</th>\n      <th>Short URL Origin</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>ID</th>\n      <th>Lateral</th>\n      <th>LateralYear</th>\n      <th>Origin School</th>\n      <th>Destination School</th>\n      <th>multi_obs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>duke.edu</td>\n      <td>gwu.edu</td>\n      <td>H.</td>\n      <td>Powell</td>\n      <td>216</td>\n      <td>1</td>\n      <td>2012</td>\n      <td>George Washington University Law School</td>\n      <td>Duke University School of Law</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original name: H. Powell\n",
      "['gwu.edu', 'duke.edu']\n",
      "H. Powell\n",
      "URL: gwu.edu\n",
      "gwu.edu in: https://extremism.gwu.edu/sites/g/files/zaxdzs2191/f/EncryptedExtremism.pdf\n",
      "H. Robert Powell\n",
      "URL: gwu.edu\n",
      "URL: duke.edu\n",
      "H. Jefferson Powell\n",
      "URL: gwu.edu\n",
      "URL: duke.edu\n",
      "duke.edu in: https://law.duke.edu/fac/powell/\n"
     ]
    }
   ],
   "source": [
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    #This line gets the school URLs from the dataframe\n",
    "    if data_type == \"lateral\":\n",
    "        school_url = [data['Short URL Origin'][i], data['Short URL Destination'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "        new_school = data['Destination School'][i]\n",
    "    elif data_type == \"control\":\n",
    "        school_url = [data['Short URL Origin'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "    multi_obs = data[\"multi_obs\"][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(\"Original name: {}\".format(full_name))\n",
    "    \n",
    "    # Search by author to find potential alternative first and middle names:\n",
    "    print(school_url)\n",
    "    fm_names, err_fm_names = search_names(mid_first_name, last_name, school_url, driver, g_driver, s_driver)\n",
    "\n",
    "\n",
    "    # Create a list of values to append to the dataframe\n",
    "    # We convert fm_names and err_fm_names to lists of strings during this step\n",
    "    values_alt_names = [prof_id, mid_first_name, last_name, fm_names, err_fm_names, multi_obs]\n",
    "    dict_values_alt_names = dict(zip(alt_names_columns, values_alt_names))\n",
    "    alt_names_data.append(dict_values_alt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the names back into the dataframe of alternate names\n",
    "df_alt_names = pd.DataFrame(columns = alt_names_columns)\n",
    "\n",
    "# Only complete these steps if we have data to add\n",
    "if alt_names_data:\n",
    "    # Append all of the data to the ouput dataset and output to Excel\n",
    "    df_alt_names = df_alt_names.append(alt_names_data)\n",
    "\n",
    "    # MERGE: Merge on the other variables from the lateral data\n",
    "    alt_name_full = pd.merge(df_alt_names, data.drop([\"FirstName\", \"LastName\", \"multi_obs\"], axis = 1), how = \"left\", left_on = \"ID\", right_on = \"ID\")\n",
    "\n",
    "else:\n",
    "    alt_name_full = df_alt_names_final\n",
    "\n",
    "# # If there was already output data, we append the data to the existing data\n",
    "if append == 1:\n",
    "    alt_names = pd.concat([df_existing_alt_names, alt_name_full], ignore_index = True)\n",
    "else:\n",
    "    alt_names = alt_name_full\n",
    "\n",
    "# DROP: Drop extra columns\n",
    "# alt_names.drop([\"diff_last_name\", \"mid_first_name\", \"last_name\"], axis = 1, inplace = True)\n",
    "\n",
    "# Export to Excel\n",
    "alt_names.to_excel(intr_path / \"alt_names.xlsx\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Matthew, Matthew D.'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()\n",
    "# hein_scraping_intput_data = pd.read_excel(intr_path / \"hein_scraping_input_data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1d54edab4942>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhein_scraping_intput_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"unusual_name_flag\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhein_scraping_intput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mflag_unusual_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fm_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FirstName\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7546\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7547\u001b[0m         )\n\u001b[1;32m-> 7548\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\selenium_env\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                     \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                         \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-1d54edab4942>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhein_scraping_intput_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"unusual_name_flag\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhein_scraping_intput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mflag_unusual_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fm_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FirstName\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-81ebbaa27c34>\u001b[0m in \u001b[0;36mflag_unusual_names\u001b[1;34m(alt_names, first_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mflag_unusual_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malt_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Create a list of the alt names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0malt_name_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0malt_name_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "hein_scraping_intput_data[\"unusual_name_flag\"] = hein_scraping_intput_data.apply(lambda x: flag_unusual_names(x[\"fm_names\"], x[\"FirstName\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_unusual_names(alt_names, first_name):\n",
    "    # Create a list of the alt names\n",
    "    alt_name_list = alt_names.split(\",\")\n",
    "    alt_name_list = [x.strip() for x in test_list]\n",
    "\n",
    "    flag = 0\n",
    "    # Perform the data checks to see if we want to flag this name\n",
    "    if len(alt_name_list) > 2:\n",
    "        flag = 1\n",
    "    for alt_name in alt_name_list:\n",
    "        # Name starts with an initial\n",
    "        if re.search(r\"^\\w\\.\\s\", alt_name):\n",
    "            flag = 2\n",
    "        # The alt_name first name does not match the first name from the original data\n",
    "        if alt_name.split(\" \")[0] != first_name:\n",
    "            flag = 3\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "web_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}