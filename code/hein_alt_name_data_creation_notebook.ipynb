{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import nltk\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import pathlib \n",
    "\n",
    "from modules.create_path import create_path\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_google, similar_names, search_names\n",
    "from modules.data_manipulation_functions import remove_commas, check_files, concat_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paths for the data directories\n",
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()\n",
    "\n",
    "# Create the paths for the Chrome binary and selenium driver\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\n",
    "\n",
    "# Initalize the browsers that we are going to use\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "g_driver = create_browser(chrome_binary_path, selenium_driver_full_path)\n",
    "\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from the working directory\n",
    "# The datasets in the working directory have already \n",
    "# been cleaned.\n",
    "control = pd.read_excel(work_path / \"control.xlsx\")\n",
    "lateral = pd.read_excel(work_path / \"lateral.xlsx\")\n",
    "\n",
    "# Load the name modification dataset\n",
    "# This is a dataframe of names that we want to manually change \n",
    "# This can be used if we found errors in the data (for example, names were not scraped)\n",
    "# This dataset is edited by hand, so it is stored in the input directory\n",
    "name_mod = pd.read_excel(input_path / 'name_mod.xlsx')\n",
    "\n",
    "# Data type: This is the type of data that we are using.\n",
    "# The control group members only have one university url because they\n",
    "# did not move. The lateral group members have two urls.\n",
    "data_type = \"lateral\"\n",
    "\n",
    "#This step sets the data \n",
    "data = lateral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already exists. Names that have already been scraped will be skipped\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store the alternate name data\n",
    "alt_names_data = []\n",
    "# Create the alt-names dataset\n",
    "alt_names_columns = [\"ID\", \"mid_first_name\", \"last_name\", \"fm_names\", \"err_fm_names\", \"diff_last_name\"]\n",
    "# Add the names back into the dataframe of alternate names\n",
    "df_alt_names = pd.DataFrame(columns = alt_names_columns)\n",
    "\n",
    "# Check to see if the file for the alternate names data already exists\n",
    "alt_names_file = intr_path / \"alt_names.xlsx\"\n",
    "if alt_names_file.exists():\n",
    "    print(\"Data already exists. Names that have already been scraped will be skipped\")\n",
    "    append = 1\n",
    "    df_alt_names_final = pd.read_excel(alt_names_file)\n",
    "    # alt_name_full = pd.merge(df_alt_names, lateral, how = \"right\", left_on = \"ID\", right_on = \"ID\")\n",
    "    data = pd.merge(data, df_alt_names_final[\"ID\"], how = \"outer\", left_on = \"ID\", right_on = \"ID\", indicator=True)\n",
    "    data = data[data['_merge'] == 'left_only']\n",
    "    data = data.drop([\"_merge\"], axis = 1)\n",
    "else:\n",
    "    append = 0\n",
    "data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This loop goes through each name\n",
    "for i in range(len(data)):\n",
    "    #This section gets the professor's information from the dataframe \n",
    "    # Get variable values from the dataframe\n",
    "    prof_id = data['ID'][i]\n",
    "    mid_first_name = data['FirstName'][i]\n",
    "    last_name = data['LastName'][i]\n",
    "    full_name = mid_first_name + ' ' +  last_name\n",
    "    #This line gets the school URLs from the dataframe\n",
    "    if data_type == \"lateral\":\n",
    "        school_url = [data['Short URL Origin'][i], data['Short URL Destination'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "        new_school = data['Destination School'][i]\n",
    "    elif data_type == \"control\":\n",
    "        school_url = [data['Short URL Origin'][i]]\n",
    "        school = data['Origin School'][i]\n",
    "\n",
    "    # Print the name that we are considering\n",
    "    print(full_name)\n",
    "    \n",
    "    # Search by author to find potential alternative first and middle names:\n",
    "    print(school_url)\n",
    "    fm_names, err_fm_names = search_names(mid_first_name, last_name, school_url, driver, g_driver)\n",
    "\n",
    "\n",
    "    # Create a list of values to append to the dataframe\n",
    "    # We convert fm_names and err_fm_names to lists of strings during this step\n",
    "    values_alt_names = [prof_id, mid_first_name, last_name, fm_names, err_fm_names]\n",
    "    dict_values_alt_names = dict(zip(alt_names_columns, values_alt_names))\n",
    "    alt_names_data.append(dict_values_alt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only complete these steps if we have data to add\n",
    "if alt_names_data:\n",
    "    # Append all of the data to the ouput dataset and output to Excel\n",
    "    df_alt_names = df_alt_names.append(alt_names_data)\n",
    "\n",
    "    # MERGE: Merge on the other variables from the lateral data\n",
    "    alt_name_full = pd.merge(df_alt_names, lateral, how = \"left\", left_on = \"ID\", right_on = \"ID\")\n",
    "\n",
    "else:\n",
    "    alt_name_full = df_alt_names_final\n",
    "\n",
    "# Export to Excel\n",
    "alt_name_full.to_excel(intr_path / \"alt_names_full.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "web_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}