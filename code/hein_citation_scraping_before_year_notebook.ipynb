{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('selenium_env': conda)"
  },
  "interpreter": {
   "hash": "a6b47012f14d21a47a116f37ec44e00a94b21aded5509aefe8b70d4e685c6dab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import time\r\n",
    "import random\r\n",
    "import math\r\n",
    "import pathlib \r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.chrome.options import Options\r\n",
    "from selenium.webdriver.common.keys import Keys\r\n",
    "from selenium.common.exceptions import NoSuchElementException\r\n",
    "\r\n",
    "import modules.hein_scraping_functions\r\n",
    "from modules.create_path import create_path\r\n",
    "from modules.hein_scraping_functions import create_browser, webpage_wait, get_paper_data, mod_names, check_bing, search_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "input_path, work_path, intr_path, out_path, selenium_driver_path = create_path()\r\n",
    "# Create the paths for the Chrome binary and selenium driver\r\n",
    "chrome_binary_path = pathlib.Path(\"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\")\r\n",
    "selenium_driver_full_path = selenium_driver_path / \"chromedriver.exe\"\r\n",
    "\r\n",
    "# Initalize the browsers that we are going to use\r\n",
    "driver = create_browser(chrome_binary_path, selenium_driver_full_path)\r\n",
    "\r\n",
    "driver.get(\"http://proxy.its.virginia.edu/login?url=http://heinonline.org/HOL/Welcome\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Load the datasets from the working directory\r\n",
    "# The datasets in the working directory have already \r\n",
    "# been cleaned.\r\n",
    "input_data = pd.read_excel(input_path / \"search strings for control synth move date.xlsx\")\r\n",
    "\r\n",
    "data = input_data\r\n",
    "\r\n",
    "# Load the scraped pages df\r\n",
    "df_scraped_pages = pd.read_excel(out_path / \"_scraped_pages.xlsx\")\r\n",
    "\r\n",
    "# Check to see if the file for the alternate names data already exists.\r\n",
    "# If it does, we only want to look for the missing observations\r\n",
    "df_cur = intr_path / \"_cites_before_year_control.xlsx\"\r\n",
    "if df_cur.exists():\r\n",
    "    print(\"Data already exists. Papers that have already been scraped will be skipped\")\r\n",
    "    # Set the append flag to 1\r\n",
    "    append = 1\r\n",
    "    # Create the dataset of existing alt names.\r\n",
    "    df_existing_data = pd.read_excel(df_cur)\r\n",
    "    # df_existing_data['ID'] = df_existing_data['ID'].apply(lambda x: '{0:0>4}'.format(x))\r\n",
    "    # Complete a left outer join of the existing alt names and the lateral/control data to get \r\n",
    "    # a list of the names that we still need to scrape alt names for.\r\n",
    "    data = pd.merge(input_data, df_existing_data[[\"ID\", \"Title\", \"BBCite\"]], how = \"outer\", left_on = [\"ID\", \"Title\", \"BBCite\"], right_on = [\"ID\", \"Title\", \"BBCite\"], indicator=True)\r\n",
    "    data = data[data['_merge'] == 'left_only']\r\n",
    "    data = data.drop([\"_merge\"], axis = 1)\r\n",
    "    append_df = df_existing_data\r\n",
    "else:\r\n",
    "    # Set the append flag to zero because we won't have any data to append\r\n",
    "    append = 0\r\n",
    "    data = input_data\r\n",
    "    # append_df = pd.DataFrame().reindex(columns=list(data.columns) + [\"google scholar cite count\"])\r\n",
    "cite_data = data\r\n",
    "cite_data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already exists. Papers that have already been scraped will be skipped\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        ID                                              Title PaperType  \\\n",
       "778   1036  DNA Rules: Legal and Conceptual Implications o...   article   \n",
       "1380  1064                                           Foreword  comments   \n",
       "5483  1236  Selling Mayberry: Communities and Individuals ...   article   \n",
       "\n",
       "                                                Authors  SynthLatYr  \\\n",
       "778                     Burk, Dan L. (Cited 3325 times)        2018   \n",
       "1380                   Cox, James D. (Cited 1902 times)        2015   \n",
       "5483  Parchomovsky, Gideon (Cited 2334 times); Siege...        2014   \n",
       "\n",
       "      NumCoauthors                               BBCite  OrigArtCites  \\\n",
       "778              1        92 Calif. L. Rev. 1553 (2004)            24   \n",
       "1380             1  60 Law and Contemp. Probs. 1 (1997)             1   \n",
       "5483             2          92 Calif. L. Rev. 75 (2004)            77   \n",
       "\n",
       "                            Journal  Year  Lateral  Year<=LatYear  \\\n",
       "778           California Law Review  2004        0              1   \n",
       "1380  Law and Contemporary Problems  1997        0              1   \n",
       "5483          California Law Review  2004        0              1   \n",
       "\n",
       "                    BBCite w/o year  BeginYear  EndYear  \n",
       "778          92 Calif. L. Rev. 1553       2002     2018  \n",
       "1380  60 Law and Contemp. Probs. 1b       1995     2015  \n",
       "5483           92 Calif. L. Rev. 75       2002     2014  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>PaperType</th>\n",
       "      <th>Authors</th>\n",
       "      <th>SynthLatYr</th>\n",
       "      <th>NumCoauthors</th>\n",
       "      <th>BBCite</th>\n",
       "      <th>OrigArtCites</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lateral</th>\n",
       "      <th>Year&lt;=LatYear</th>\n",
       "      <th>BBCite w/o year</th>\n",
       "      <th>BeginYear</th>\n",
       "      <th>EndYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>1036</td>\n",
       "      <td>DNA Rules: Legal and Conceptual Implications o...</td>\n",
       "      <td>article</td>\n",
       "      <td>Burk, Dan L. (Cited 3325 times)</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>92 Calif. L. Rev. 1553 (2004)</td>\n",
       "      <td>24</td>\n",
       "      <td>California Law Review</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>92 Calif. L. Rev. 1553</td>\n",
       "      <td>2002</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>1064</td>\n",
       "      <td>Foreword</td>\n",
       "      <td>comments</td>\n",
       "      <td>Cox, James D. (Cited 1902 times)</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>60 Law and Contemp. Probs. 1 (1997)</td>\n",
       "      <td>1</td>\n",
       "      <td>Law and Contemporary Problems</td>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60 Law and Contemp. Probs. 1b</td>\n",
       "      <td>1995</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>1236</td>\n",
       "      <td>Selling Mayberry: Communities and Individuals ...</td>\n",
       "      <td>article</td>\n",
       "      <td>Parchomovsky, Gideon (Cited 2334 times); Siege...</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>92 Calif. L. Rev. 75 (2004)</td>\n",
       "      <td>77</td>\n",
       "      <td>California Law Review</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>92 Calif. L. Rev. 75</td>\n",
       "      <td>2002</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Loop over the citation data\r\n",
    "for i in range(len(cite_data)):\r\n",
    "    found_paper = 0\r\n",
    "    dict = cite_data.to_dict('records')[i]\r\n",
    "    # Get the ID and BBcite\r\n",
    "    id = dict[\"ID\"]\r\n",
    "    bbcite = dict[\"BBCite\"]\r\n",
    "    year = dict[\"SynthLatYr\"]\r\n",
    "    # print(\"ID {}\".format(id))\r\n",
    "    # print(\"BBCite {}\".format(bbcite))\r\n",
    "    # print(\"Switch year {}\".format(year))\r\n",
    "    # Search for the Hein pages for this ID in the scraped pages data\r\n",
    "    for link in df_scraped_pages.query('@id == id')[\"links\"]:\r\n",
    "        if found_paper == 1:\r\n",
    "            break\r\n",
    "        # print(link)\r\n",
    "        driver.get(link)\r\n",
    "        #This section scrapes the paper data. The index values are based on the way the xpaths are incremented\r\n",
    "        #The scroll number tracks the number of times the page has scrolled. This is for pages with a large number of \r\n",
    "        #papers. The xpaths change when the page scrolls.\r\n",
    "        title_index = 3\r\n",
    "        stats_index = 4\r\n",
    "        topic_index = 0\r\n",
    "        scroll_num = 0\r\n",
    "        element = \"init\"\r\n",
    "\r\n",
    "        while element:\r\n",
    "            # Check the papers until we find the correct Bbcite\r\n",
    "            if scroll_num == 0:\r\n",
    "                element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[1]/div/div/div[' + str(title_index) + ']/div[2]')      \r\n",
    "            elif scroll_num > 0:\r\n",
    "                element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]')\r\n",
    "            \r\n",
    "            if type(element) == list:\r\n",
    "                title = element[0]\r\n",
    "            else:\r\n",
    "                title = element\r\n",
    "\r\n",
    "            # If we find the bbcite, check for the citiations link\r\n",
    "            if bbcite in title.text:\r\n",
    "                # print(\"found bbcite\")\r\n",
    "                if scroll_num == 0:\r\n",
    "                    # print(\"stats index {}\".format(stats_index))\r\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[2]/div/a')\r\n",
    "                    if not element:\r\n",
    "                        element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div/div/div/div[' + str(stats_index) + ']/div[3]/div/a')\r\n",
    "                elif scroll_num > 0:\r\n",
    "                    element = driver.find_elements_by_xpath('//*[@id=\"save_results\"]/div[' + str(stats_index) + ']/div[2]/div/a')\r\n",
    "                # If there are citaitions, check them, otherwise, continue\r\n",
    "                if element:\r\n",
    "                    citation = element[0]\r\n",
    "                else:\r\n",
    "                    # print(\"Note: There were no citations for {}\".format(bbcite))\r\n",
    "                    # If the citaion field was not found, make the citation count zero\r\n",
    "                    dict[\"Cites Before Lateral Year\"] = 0\r\n",
    "                    append_df = append_df.append(dict, ignore_index = True)\r\n",
    "                    append_df.to_excel(intr_path / \"_cites_before_year_control.xlsx\", index = False)\r\n",
    "                    found_paper = 1\r\n",
    "                    break\r\n",
    "                \r\n",
    "                # If there were citations, calculate the number that occurred before the given year\r\n",
    "                if 'Cited by' in citation.text and 'Case' not in citation.text:\r\n",
    "                    cited_link = citation.get_attribute('href')\r\n",
    "                    driver.get(cited_link)\r\n",
    "                    # Click the year field\r\n",
    "                    webpage_wait('//*[@id=\"face_show_in\"]/aside/a/h3', driver)\r\n",
    "                    year_box = driver.find_element_by_xpath('//*[@id=\"face_show_in\"]/aside/a/h3').click()\r\n",
    "                    # Enter the ending year\r\n",
    "                    year_high = driver.find_element_by_xpath('//*[@id=\"yearhi\"]')\r\n",
    "                    year_high.send_keys(str(year))\r\n",
    "\r\n",
    "                    year_go = driver.find_element_by_xpath('//*[@id=\"dateadd\"]/input[12]').click()\r\n",
    "\r\n",
    "                    # Return the results\r\n",
    "                    try:\r\n",
    "                        result_element = driver.find_element_by_xpath('//*[@id=\"results_total\"]')\r\n",
    "                        count_match = re.search(r\"^(0|[1-9]\\d{0,2},?\\d*) results\", result_element.text)\r\n",
    "                        citation_count = count_match.group(1)\r\n",
    "                    except NoSuchElementException:\r\n",
    "                        print(\"Note: No citations were found after the lateral move for {}\".format(bbcite))\r\n",
    "                        citation_count = 0\r\n",
    "                    dict[\"Cites Before Lateral Year\"] = citation_count\r\n",
    "                    append_df = append_df.append(dict, ignore_index = True)\r\n",
    "                    append_df.to_excel(intr_path / \"_cites_before_year_control.xlsx\", index = False)\r\n",
    "                    found_paper = 1\r\n",
    "                    break\r\n",
    "                else:\r\n",
    "                    # print(\"Note: There were no citations for {}\".format(bbcite))\r\n",
    "                    # If the citaion field was not found, make the citation count zero\r\n",
    "                    dict[\"Cites Before Lateral Year\"] = 0\r\n",
    "                    append_df = append_df.append(dict, ignore_index = True)\r\n",
    "                    append_df.to_excel(intr_path / \"_cites_before_year_control.xlsx\", index = False)\r\n",
    "                    found_paper = 1\r\n",
    "                    break\r\n",
    "            else:\r\n",
    "                #The indices are augmented to get the next paper\r\n",
    "                stats_index += 4\r\n",
    "                title_index += 4\r\n",
    "                #Check that next paper exists:\r\n",
    "                if scroll_num == 0:\r\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div/div/div/div[' + str(title_index) + ']/div[2]/dt[1]/div'\r\n",
    "                #If the page has scrolled, the xpath we need to check has changed\r\n",
    "                if scroll_num > 0:\r\n",
    "                    x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\r\n",
    "                element = driver.find_elements_by_xpath(x_path_title)\r\n",
    "                #If we can't find a next paper, it could be because we need to scroll again\r\n",
    "                #This section attempts to scroll the page. \r\n",
    "                if not element:\r\n",
    "                    scroll_num +=1\r\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\r\n",
    "                    box_element = driver.find_elements_by_xpath('//*[@id=\"results_total\"]')\r\n",
    "                    num_papers = int(box_element[0].text.split(' ')[0])\r\n",
    "                    #If there are more than 100 papers, we know there are still paper left to scrape\r\n",
    "                    if num_papers > 100*scroll_num:\r\n",
    "                        time.sleep(15)\r\n",
    "                        title_index = 3\r\n",
    "                        stats_index = 4\r\n",
    "                        topic_index = 0\r\n",
    "                        x_path_title = '//*[@id=\"save_results\"]/div[' + str(title_index) + ']/div[2]/dt[1]/div'\r\n",
    "                        element = driver.find_elements_by_xpath(x_path_title)   \r\n",
    "\r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}