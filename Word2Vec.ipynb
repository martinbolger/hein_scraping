{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Source: http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "#https://github.com/adventuresinML/adventures-in-ml-code/blob/master/keras_word2vec.py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, merge, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "filename = maybe_download('text8.zip', url, 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = read_data(filename)\n",
    "print(vocabulary[:7])\n",
    "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab = []\n",
    "def remove_stop_words(vocab):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for word in vocab:\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "                filtered_vocab.append(word)\n",
    "    return filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "#     filtered_vocab = remove_stop_words(vocabulary)\n",
    "#     print(filtered_vocab[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    #del filtered_vocab, vocabulary  # Hint to reduce memory.\n",
    "    del vocabulary\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "[5234, 3081, 12, 6, 195, 2, 3134]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[292, 668], [5786, 5498], [7821, 2497], [3215, 7], [7490, 5635], [8, 3652], [3075, 2260], [14, 38], [1116, 24], [6309, 8348]] [1, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " 'the': 1,\n",
       " 'of': 2,\n",
       " 'and': 3,\n",
       " 'one': 4,\n",
       " 'in': 5,\n",
       " 'a': 6,\n",
       " 'to': 7,\n",
       " 'zero': 8,\n",
       " 'nine': 9,\n",
       " 'two': 10,\n",
       " 'is': 11,\n",
       " 'as': 12,\n",
       " 'eight': 13,\n",
       " 'for': 14,\n",
       " 's': 15,\n",
       " 'five': 16,\n",
       " 'three': 17,\n",
       " 'was': 18,\n",
       " 'by': 19,\n",
       " 'that': 20,\n",
       " 'four': 21,\n",
       " 'six': 22,\n",
       " 'seven': 23,\n",
       " 'with': 24,\n",
       " 'on': 25,\n",
       " 'are': 26,\n",
       " 'it': 27,\n",
       " 'from': 28,\n",
       " 'or': 29,\n",
       " 'his': 30,\n",
       " 'an': 31,\n",
       " 'be': 32,\n",
       " 'this': 33,\n",
       " 'which': 34,\n",
       " 'at': 35,\n",
       " 'he': 36,\n",
       " 'also': 37,\n",
       " 'not': 38,\n",
       " 'have': 39,\n",
       " 'were': 40,\n",
       " 'has': 41,\n",
       " 'but': 42,\n",
       " 'other': 43,\n",
       " 'their': 44,\n",
       " 'its': 45,\n",
       " 'first': 46,\n",
       " 'they': 47,\n",
       " 'some': 48,\n",
       " 'had': 49,\n",
       " 'all': 50,\n",
       " 'more': 51,\n",
       " 'most': 52,\n",
       " 'can': 53,\n",
       " 'been': 54,\n",
       " 'such': 55,\n",
       " 'many': 56,\n",
       " 'who': 57,\n",
       " 'new': 58,\n",
       " 'used': 59,\n",
       " 'there': 60,\n",
       " 'after': 61,\n",
       " 'when': 62,\n",
       " 'into': 63,\n",
       " 'american': 64,\n",
       " 'time': 65,\n",
       " 'these': 66,\n",
       " 'only': 67,\n",
       " 'see': 68,\n",
       " 'may': 69,\n",
       " 'than': 70,\n",
       " 'world': 71,\n",
       " 'i': 72,\n",
       " 'b': 73,\n",
       " 'would': 74,\n",
       " 'd': 75,\n",
       " 'no': 76,\n",
       " 'however': 77,\n",
       " 'between': 78,\n",
       " 'about': 79,\n",
       " 'over': 80,\n",
       " 'years': 81,\n",
       " 'states': 82,\n",
       " 'people': 83,\n",
       " 'war': 84,\n",
       " 'during': 85,\n",
       " 'united': 86,\n",
       " 'known': 87,\n",
       " 'if': 88,\n",
       " 'called': 89,\n",
       " 'use': 90,\n",
       " 'th': 91,\n",
       " 'system': 92,\n",
       " 'often': 93,\n",
       " 'state': 94,\n",
       " 'so': 95,\n",
       " 'history': 96,\n",
       " 'will': 97,\n",
       " 'up': 98,\n",
       " 'while': 99,\n",
       " 'where': 100,\n",
       " 'city': 101,\n",
       " 'being': 102,\n",
       " 'english': 103,\n",
       " 'then': 104,\n",
       " 'any': 105,\n",
       " 'both': 106,\n",
       " 'under': 107,\n",
       " 'out': 108,\n",
       " 'made': 109,\n",
       " 'well': 110,\n",
       " 'her': 111,\n",
       " 'e': 112,\n",
       " 'number': 113,\n",
       " 'government': 114,\n",
       " 'them': 115,\n",
       " 'm': 116,\n",
       " 'later': 117,\n",
       " 'since': 118,\n",
       " 'him': 119,\n",
       " 'part': 120,\n",
       " 'name': 121,\n",
       " 'c': 122,\n",
       " 'century': 123,\n",
       " 'through': 124,\n",
       " 'because': 125,\n",
       " 'x': 126,\n",
       " 'university': 127,\n",
       " 'early': 128,\n",
       " 'life': 129,\n",
       " 'british': 130,\n",
       " 'year': 131,\n",
       " 'like': 132,\n",
       " 'same': 133,\n",
       " 'including': 134,\n",
       " 'became': 135,\n",
       " 'example': 136,\n",
       " 'day': 137,\n",
       " 'each': 138,\n",
       " 'even': 139,\n",
       " 'work': 140,\n",
       " 'language': 141,\n",
       " 'although': 142,\n",
       " 'several': 143,\n",
       " 'form': 144,\n",
       " 'john': 145,\n",
       " 'u': 146,\n",
       " 'national': 147,\n",
       " 'very': 148,\n",
       " 'much': 149,\n",
       " 'g': 150,\n",
       " 'french': 151,\n",
       " 'before': 152,\n",
       " 'general': 153,\n",
       " 'what': 154,\n",
       " 't': 155,\n",
       " 'against': 156,\n",
       " 'n': 157,\n",
       " 'high': 158,\n",
       " 'links': 159,\n",
       " 'could': 160,\n",
       " 'based': 161,\n",
       " 'those': 162,\n",
       " 'now': 163,\n",
       " 'second': 164,\n",
       " 'de': 165,\n",
       " 'music': 166,\n",
       " 'another': 167,\n",
       " 'large': 168,\n",
       " 'she': 169,\n",
       " 'f': 170,\n",
       " 'external': 171,\n",
       " 'german': 172,\n",
       " 'different': 173,\n",
       " 'modern': 174,\n",
       " 'great': 175,\n",
       " 'do': 176,\n",
       " 'common': 177,\n",
       " 'set': 178,\n",
       " 'list': 179,\n",
       " 'south': 180,\n",
       " 'series': 181,\n",
       " 'major': 182,\n",
       " 'game': 183,\n",
       " 'power': 184,\n",
       " 'long': 185,\n",
       " 'country': 186,\n",
       " 'king': 187,\n",
       " 'law': 188,\n",
       " 'group': 189,\n",
       " 'film': 190,\n",
       " 'still': 191,\n",
       " 'until': 192,\n",
       " 'north': 193,\n",
       " 'international': 194,\n",
       " 'term': 195,\n",
       " 'we': 196,\n",
       " 'end': 197,\n",
       " 'book': 198,\n",
       " 'found': 199,\n",
       " 'own': 200,\n",
       " 'political': 201,\n",
       " 'party': 202,\n",
       " 'order': 203,\n",
       " 'usually': 204,\n",
       " 'president': 205,\n",
       " 'church': 206,\n",
       " 'you': 207,\n",
       " 'death': 208,\n",
       " 'theory': 209,\n",
       " 'area': 210,\n",
       " 'around': 211,\n",
       " 'include': 212,\n",
       " 'god': 213,\n",
       " 'ii': 214,\n",
       " 'way': 215,\n",
       " 'did': 216,\n",
       " 'military': 217,\n",
       " 'population': 218,\n",
       " 'using': 219,\n",
       " 'though': 220,\n",
       " 'small': 221,\n",
       " 'following': 222,\n",
       " 'within': 223,\n",
       " 'non': 224,\n",
       " 'human': 225,\n",
       " 'left': 226,\n",
       " 'main': 227,\n",
       " 'among': 228,\n",
       " 'point': 229,\n",
       " 'r': 230,\n",
       " 'due': 231,\n",
       " 'p': 232,\n",
       " 'considered': 233,\n",
       " 'public': 234,\n",
       " 'popular': 235,\n",
       " 'computer': 236,\n",
       " 'west': 237,\n",
       " 'family': 238,\n",
       " 'east': 239,\n",
       " 'information': 240,\n",
       " 'important': 241,\n",
       " 'european': 242,\n",
       " 'man': 243,\n",
       " 'sometimes': 244,\n",
       " 'right': 245,\n",
       " 'old': 246,\n",
       " 'free': 247,\n",
       " 'word': 248,\n",
       " 'without': 249,\n",
       " 'last': 250,\n",
       " 'us': 251,\n",
       " 'members': 252,\n",
       " 'given': 253,\n",
       " 'times': 254,\n",
       " 'roman': 255,\n",
       " 'make': 256,\n",
       " 'h': 257,\n",
       " 'age': 258,\n",
       " 'place': 259,\n",
       " 'l': 260,\n",
       " 'thus': 261,\n",
       " 'science': 262,\n",
       " 'case': 263,\n",
       " 'become': 264,\n",
       " 'systems': 265,\n",
       " 'union': 266,\n",
       " 'born': 267,\n",
       " 'york': 268,\n",
       " 'line': 269,\n",
       " 'countries': 270,\n",
       " 'does': 271,\n",
       " 'isbn': 272,\n",
       " 'st': 273,\n",
       " 'control': 274,\n",
       " 'various': 275,\n",
       " 'others': 276,\n",
       " 'house': 277,\n",
       " 'article': 278,\n",
       " 'island': 279,\n",
       " 'should': 280,\n",
       " 'led': 281,\n",
       " 'back': 282,\n",
       " 'period': 283,\n",
       " 'player': 284,\n",
       " 'europe': 285,\n",
       " 'languages': 286,\n",
       " 'central': 287,\n",
       " 'water': 288,\n",
       " 'few': 289,\n",
       " 'western': 290,\n",
       " 'home': 291,\n",
       " 'began': 292,\n",
       " 'generally': 293,\n",
       " 'less': 294,\n",
       " 'k': 295,\n",
       " 'similar': 296,\n",
       " 'written': 297,\n",
       " 'original': 298,\n",
       " 'best': 299,\n",
       " 'must': 300,\n",
       " 'according': 301,\n",
       " 'school': 302,\n",
       " 'france': 303,\n",
       " 'air': 304,\n",
       " 'single': 305,\n",
       " 'force': 306,\n",
       " 'v': 307,\n",
       " 'land': 308,\n",
       " 'groups': 309,\n",
       " 'down': 310,\n",
       " 'how': 311,\n",
       " 'works': 312,\n",
       " 'development': 313,\n",
       " 'official': 314,\n",
       " 'support': 315,\n",
       " 'england': 316,\n",
       " 'j': 317,\n",
       " 'rather': 318,\n",
       " 'data': 319,\n",
       " 'space': 320,\n",
       " 'greek': 321,\n",
       " 'km': 322,\n",
       " 'named': 323,\n",
       " 'germany': 324,\n",
       " 'just': 325,\n",
       " 'games': 326,\n",
       " 'said': 327,\n",
       " 'version': 328,\n",
       " 'late': 329,\n",
       " 'earth': 330,\n",
       " 'company': 331,\n",
       " 'every': 332,\n",
       " 'economic': 333,\n",
       " 'short': 334,\n",
       " 'published': 335,\n",
       " 'black': 336,\n",
       " 'army': 337,\n",
       " 'off': 338,\n",
       " 'london': 339,\n",
       " 'million': 340,\n",
       " 'body': 341,\n",
       " 'field': 342,\n",
       " 'christian': 343,\n",
       " 'either': 344,\n",
       " 'social': 345,\n",
       " 'empire': 346,\n",
       " 'o': 347,\n",
       " 'developed': 348,\n",
       " 'standard': 349,\n",
       " 'court': 350,\n",
       " 'service': 351,\n",
       " 'kingdom': 352,\n",
       " 'along': 353,\n",
       " 'college': 354,\n",
       " 'republic': 355,\n",
       " 'sea': 356,\n",
       " 'america': 357,\n",
       " 'today': 358,\n",
       " 'result': 359,\n",
       " 'held': 360,\n",
       " 'team': 361,\n",
       " 'light': 362,\n",
       " 'means': 363,\n",
       " 'never': 364,\n",
       " 'especially': 365,\n",
       " 'third': 366,\n",
       " 'further': 367,\n",
       " 'character': 368,\n",
       " 'forces': 369,\n",
       " 'take': 370,\n",
       " 'men': 371,\n",
       " 'society': 372,\n",
       " 'show': 373,\n",
       " 'open': 374,\n",
       " 'possible': 375,\n",
       " 'fact': 376,\n",
       " 'battle': 377,\n",
       " 'took': 378,\n",
       " 'former': 379,\n",
       " 'books': 380,\n",
       " 'soviet': 381,\n",
       " 'river': 382,\n",
       " 'children': 383,\n",
       " 'having': 384,\n",
       " 'good': 385,\n",
       " 'local': 386,\n",
       " 'current': 387,\n",
       " 'son': 388,\n",
       " 'process': 389,\n",
       " 'natural': 390,\n",
       " 'present': 391,\n",
       " 'himself': 392,\n",
       " 'islands': 393,\n",
       " 'total': 394,\n",
       " 'near': 395,\n",
       " 'white': 396,\n",
       " 'days': 397,\n",
       " 'person': 398,\n",
       " 'itself': 399,\n",
       " 'seen': 400,\n",
       " 'culture': 401,\n",
       " 'little': 402,\n",
       " 'above': 403,\n",
       " 'software': 404,\n",
       " 'largest': 405,\n",
       " 'words': 406,\n",
       " 'upon': 407,\n",
       " 'level': 408,\n",
       " 'father': 409,\n",
       " 'created': 410,\n",
       " 'side': 411,\n",
       " 'red': 412,\n",
       " 'references': 413,\n",
       " 'press': 414,\n",
       " 'full': 415,\n",
       " 'region': 416,\n",
       " 'almost': 417,\n",
       " 'image': 418,\n",
       " 'al': 419,\n",
       " 'famous': 420,\n",
       " 'play': 421,\n",
       " 'came': 422,\n",
       " 'role': 423,\n",
       " 'once': 424,\n",
       " 'certain': 425,\n",
       " 'league': 426,\n",
       " 'jewish': 427,\n",
       " 'james': 428,\n",
       " 'january': 429,\n",
       " 'site': 430,\n",
       " 'again': 431,\n",
       " 'numbers': 432,\n",
       " 'art': 433,\n",
       " 'member': 434,\n",
       " 'areas': 435,\n",
       " 'movement': 436,\n",
       " 'religious': 437,\n",
       " 'type': 438,\n",
       " 'march': 439,\n",
       " 'community': 440,\n",
       " 'story': 441,\n",
       " 'played': 442,\n",
       " 'production': 443,\n",
       " 'released': 444,\n",
       " 'center': 445,\n",
       " 'rights': 446,\n",
       " 'real': 447,\n",
       " 'related': 448,\n",
       " 'foreign': 449,\n",
       " 'low': 450,\n",
       " 'ancient': 451,\n",
       " 'terms': 452,\n",
       " 'view': 453,\n",
       " 'source': 454,\n",
       " 'act': 455,\n",
       " 'minister': 456,\n",
       " 'change': 457,\n",
       " 'energy': 458,\n",
       " 'produced': 459,\n",
       " 'research': 460,\n",
       " 'actor': 461,\n",
       " 'making': 462,\n",
       " 'civil': 463,\n",
       " 'december': 464,\n",
       " 'women': 465,\n",
       " 'special': 466,\n",
       " 'style': 467,\n",
       " 'william': 468,\n",
       " 'design': 469,\n",
       " 'japanese': 470,\n",
       " 'available': 471,\n",
       " 'chinese': 472,\n",
       " 'forms': 473,\n",
       " 'canada': 474,\n",
       " 'northern': 475,\n",
       " 'died': 476,\n",
       " 'class': 477,\n",
       " 'living': 478,\n",
       " 'next': 479,\n",
       " 'particular': 480,\n",
       " 'program': 481,\n",
       " 'council': 482,\n",
       " 'television': 483,\n",
       " 'head': 484,\n",
       " 'david': 485,\n",
       " 'china': 486,\n",
       " 'middle': 487,\n",
       " 'established': 488,\n",
       " 'hand': 489,\n",
       " 'bc': 490,\n",
       " 'far': 491,\n",
       " 'july': 492,\n",
       " 'function': 493,\n",
       " 'position': 494,\n",
       " 'y': 495,\n",
       " 'built': 496,\n",
       " 'george': 497,\n",
       " 'band': 498,\n",
       " 'together': 499,\n",
       " 'w': 500,\n",
       " 'latin': 501,\n",
       " 'thought': 502,\n",
       " 'eastern': 503,\n",
       " 'charles': 504,\n",
       " 'parts': 505,\n",
       " 'instead': 506,\n",
       " 'study': 507,\n",
       " 'might': 508,\n",
       " 'india': 509,\n",
       " 'code': 510,\n",
       " 'included': 511,\n",
       " 'meaning': 512,\n",
       " 'trade': 513,\n",
       " 'per': 514,\n",
       " 'june': 515,\n",
       " 'least': 516,\n",
       " 'half': 517,\n",
       " 'model': 518,\n",
       " 'economy': 519,\n",
       " 'prime': 520,\n",
       " 'traditional': 521,\n",
       " 'always': 522,\n",
       " 'capital': 523,\n",
       " 'range': 524,\n",
       " 'november': 525,\n",
       " 'emperor': 526,\n",
       " 'young': 527,\n",
       " 'anti': 528,\n",
       " 'final': 529,\n",
       " 'text': 530,\n",
       " 'players': 531,\n",
       " 'uk': 532,\n",
       " 'april': 533,\n",
       " 'run': 534,\n",
       " 'september': 535,\n",
       " 'addition': 536,\n",
       " 'radio': 537,\n",
       " 'live': 538,\n",
       " 'august': 539,\n",
       " 'taken': 540,\n",
       " 'note': 541,\n",
       " 'italian': 542,\n",
       " 'lost': 543,\n",
       " 'nature': 544,\n",
       " 'project': 545,\n",
       " 'technology': 546,\n",
       " 'spanish': 547,\n",
       " 'october': 548,\n",
       " 'recent': 549,\n",
       " 'rate': 550,\n",
       " 'won': 551,\n",
       " 'true': 552,\n",
       " 'value': 553,\n",
       " 'uses': 554,\n",
       " 'russian': 555,\n",
       " 'est': 556,\n",
       " 'wrote': 557,\n",
       " 'effect': 558,\n",
       " 'album': 559,\n",
       " 'southern': 560,\n",
       " 'africa': 561,\n",
       " 'whose': 562,\n",
       " 'top': 563,\n",
       " 'historical': 564,\n",
       " 'australia': 565,\n",
       " 'catholic': 566,\n",
       " 'particularly': 567,\n",
       " 'self': 568,\n",
       " 'structure': 569,\n",
       " 'record': 570,\n",
       " 'evidence': 571,\n",
       " 'rule': 572,\n",
       " 'themselves': 573,\n",
       " 'influence': 574,\n",
       " 'cases': 575,\n",
       " 'subject': 576,\n",
       " 'referred': 577,\n",
       " 'continued': 578,\n",
       " 'nations': 579,\n",
       " 'below': 580,\n",
       " 'rock': 581,\n",
       " 'japan': 582,\n",
       " 'com': 583,\n",
       " 'song': 584,\n",
       " 'throughout': 585,\n",
       " 'names': 586,\n",
       " 'female': 587,\n",
       " 'title': 588,\n",
       " 'therefore': 589,\n",
       " 'our': 590,\n",
       " 'office': 591,\n",
       " 'star': 592,\n",
       " 'paul': 593,\n",
       " 'too': 594,\n",
       " 'cities': 595,\n",
       " 'february': 596,\n",
       " 'independent': 597,\n",
       " 'author': 598,\n",
       " 'problem': 599,\n",
       " 'species': 600,\n",
       " 'education': 601,\n",
       " 'done': 602,\n",
       " 'philosophy': 603,\n",
       " 'come': 604,\n",
       " 'higher': 605,\n",
       " 'originally': 606,\n",
       " 'market': 607,\n",
       " 'town': 608,\n",
       " 'my': 609,\n",
       " 'season': 610,\n",
       " 'love': 611,\n",
       " 'strong': 612,\n",
       " 'israel': 613,\n",
       " 'irish': 614,\n",
       " 'writer': 615,\n",
       " 'films': 616,\n",
       " 'elements': 617,\n",
       " 'robert': 618,\n",
       " 'whether': 619,\n",
       " 'despite': 620,\n",
       " 'eventually': 621,\n",
       " 'here': 622,\n",
       " 'football': 623,\n",
       " 'action': 624,\n",
       " 'internet': 625,\n",
       " 'individual': 626,\n",
       " 'sound': 627,\n",
       " 'network': 628,\n",
       " 'described': 629,\n",
       " 'practice': 630,\n",
       " 'characters': 631,\n",
       " 're': 632,\n",
       " 'royal': 633,\n",
       " 'la': 634,\n",
       " 'events': 635,\n",
       " 'formed': 636,\n",
       " 'commonly': 637,\n",
       " 'base': 638,\n",
       " 'received': 639,\n",
       " 'african': 640,\n",
       " 'problems': 641,\n",
       " 'food': 642,\n",
       " 'jews': 643,\n",
       " 'able': 644,\n",
       " 'male': 645,\n",
       " 'typically': 646,\n",
       " 'mass': 647,\n",
       " 'complex': 648,\n",
       " 'lower': 649,\n",
       " 'includes': 650,\n",
       " 'outside': 651,\n",
       " 'legal': 652,\n",
       " 'complete': 653,\n",
       " 'significant': 654,\n",
       " 'parliament': 655,\n",
       " 'actually': 656,\n",
       " 'business': 657,\n",
       " 'fiction': 658,\n",
       " 'physical': 659,\n",
       " 'followed': 660,\n",
       " 'deaths': 661,\n",
       " 'key': 662,\n",
       " 'leader': 663,\n",
       " 'widely': 664,\n",
       " 'page': 665,\n",
       " 'basic': 666,\n",
       " 'types': 667,\n",
       " 'henry': 668,\n",
       " 'elected': 669,\n",
       " 'beginning': 670,\n",
       " 'fire': 671,\n",
       " 'building': 672,\n",
       " 'independence': 673,\n",
       " 'went': 674,\n",
       " 'movie': 675,\n",
       " 'aircraft': 676,\n",
       " 'ever': 677,\n",
       " 'canadian': 678,\n",
       " 'material': 679,\n",
       " 'births': 680,\n",
       " 'video': 681,\n",
       " 'news': 682,\n",
       " 'future': 683,\n",
       " 'scientific': 684,\n",
       " 'simply': 685,\n",
       " 'go': 686,\n",
       " 'defined': 687,\n",
       " 'laws': 688,\n",
       " 'get': 689,\n",
       " 'close': 690,\n",
       " 'industry': 691,\n",
       " 'specific': 692,\n",
       " 'examples': 693,\n",
       " 'believe': 694,\n",
       " 'services': 695,\n",
       " 'idea': 696,\n",
       " 'method': 697,\n",
       " 'introduced': 698,\n",
       " 'points': 699,\n",
       " 'return': 700,\n",
       " 'cause': 701,\n",
       " 'indian': 702,\n",
       " 'britain': 703,\n",
       " 'features': 704,\n",
       " 'majority': 705,\n",
       " 'size': 706,\n",
       " 'post': 707,\n",
       " 'lead': 708,\n",
       " 'organization': 709,\n",
       " 'cannot': 710,\n",
       " 'designed': 711,\n",
       " 'ireland': 712,\n",
       " 'cross': 713,\n",
       " 'classical': 714,\n",
       " 'personal': 715,\n",
       " 'writing': 716,\n",
       " 'concept': 717,\n",
       " 'associated': 718,\n",
       " 'required': 719,\n",
       " 'soon': 720,\n",
       " 'changes': 721,\n",
       " 'california': 722,\n",
       " 'located': 723,\n",
       " 'sense': 724,\n",
       " 'believed': 725,\n",
       " 'away': 726,\n",
       " 'started': 727,\n",
       " 'co': 728,\n",
       " 'religion': 729,\n",
       " 'mother': 730,\n",
       " 'county': 731,\n",
       " 'rules': 732,\n",
       " 'studies': 733,\n",
       " 'yet': 734,\n",
       " 'find': 735,\n",
       " 'knowledge': 736,\n",
       " 'put': 737,\n",
       " 'founded': 738,\n",
       " 'policy': 739,\n",
       " 'currently': 740,\n",
       " 'provide': 741,\n",
       " 'working': 742,\n",
       " 'media': 743,\n",
       " 'election': 744,\n",
       " 'australian': 745,\n",
       " 'me': 746,\n",
       " 'thomas': 747,\n",
       " 'allowed': 748,\n",
       " 'russia': 749,\n",
       " 'earlier': 750,\n",
       " 'greater': 751,\n",
       " 'limited': 752,\n",
       " 'object': 753,\n",
       " 'brought': 754,\n",
       " 'online': 755,\n",
       " 'association': 756,\n",
       " 'lord': 757,\n",
       " 'mostly': 758,\n",
       " 'blue': 759,\n",
       " 'constitution': 760,\n",
       " 'across': 761,\n",
       " 'added': 762,\n",
       " 'interest': 763,\n",
       " 'things': 764,\n",
       " 'relations': 765,\n",
       " 'speed': 766,\n",
       " 'federal': 767,\n",
       " 'singer': 768,\n",
       " 'effects': 769,\n",
       " 'growth': 770,\n",
       " 'sources': 771,\n",
       " 'your': 772,\n",
       " 'remains': 773,\n",
       " 'z': 774,\n",
       " 'probably': 775,\n",
       " 'gave': 776,\n",
       " 'simple': 777,\n",
       " 'attack': 778,\n",
       " 'longer': 779,\n",
       " 'reference': 780,\n",
       " 'saint': 781,\n",
       " 'success': 782,\n",
       " 'killed': 783,\n",
       " 'past': 784,\n",
       " 'career': 785,\n",
       " 'need': 786,\n",
       " 'park': 787,\n",
       " 'definition': 788,\n",
       " 'say': 789,\n",
       " 'etc': 790,\n",
       " 'give': 791,\n",
       " 'peace': 792,\n",
       " 'chief': 793,\n",
       " 'stories': 794,\n",
       " 'security': 795,\n",
       " 'wide': 796,\n",
       " 'ball': 797,\n",
       " 'saw': 798,\n",
       " 'machine': 799,\n",
       " 'better': 800,\n",
       " 'cell': 801,\n",
       " 'leading': 802,\n",
       " 'becomes': 803,\n",
       " 'spain': 804,\n",
       " 'larger': 805,\n",
       " 'products': 806,\n",
       " 'parties': 807,\n",
       " 'night': 808,\n",
       " 'remained': 809,\n",
       " 'prize': 810,\n",
       " 'months': 811,\n",
       " 'website': 812,\n",
       " 'big': 813,\n",
       " 'cultural': 814,\n",
       " 'money': 815,\n",
       " 'help': 816,\n",
       " 'territory': 817,\n",
       " 'private': 818,\n",
       " 'moved': 819,\n",
       " 'letter': 820,\n",
       " 'wife': 821,\n",
       " 'politics': 822,\n",
       " 'lines': 823,\n",
       " 'largely': 824,\n",
       " 'contains': 825,\n",
       " 'companies': 826,\n",
       " 'lake': 827,\n",
       " 'perhaps': 828,\n",
       " 'green': 829,\n",
       " 'already': 830,\n",
       " 'dead': 831,\n",
       " 'iii': 832,\n",
       " 'library': 833,\n",
       " 'separate': 834,\n",
       " 'refer': 835,\n",
       " 'makes': 836,\n",
       " 'appeared': 837,\n",
       " 'dutch': 838,\n",
       " 'holy': 839,\n",
       " 'era': 840,\n",
       " 'novel': 841,\n",
       " 'successful': 842,\n",
       " 'italy': 843,\n",
       " 'letters': 844,\n",
       " 'results': 845,\n",
       " 'matter': 846,\n",
       " 'produce': 847,\n",
       " 'origin': 848,\n",
       " 'claim': 849,\n",
       " 'whole': 850,\n",
       " 'directly': 851,\n",
       " 'attempt': 852,\n",
       " 'actress': 853,\n",
       " 'surface': 854,\n",
       " 'revolution': 855,\n",
       " 'highly': 856,\n",
       " 'caused': 857,\n",
       " 'status': 858,\n",
       " 'musical': 859,\n",
       " 'richard': 860,\n",
       " 'commercial': 861,\n",
       " 'division': 862,\n",
       " 'color': 863,\n",
       " 'health': 864,\n",
       " 'coast': 865,\n",
       " 'release': 866,\n",
       " 'latter': 867,\n",
       " 'authority': 868,\n",
       " 'treaty': 869,\n",
       " 'turn': 870,\n",
       " 'michael': 871,\n",
       " 'nation': 872,\n",
       " 'direct': 873,\n",
       " 'asia': 874,\n",
       " 'edition': 875,\n",
       " 'programming': 876,\n",
       " 'playing': 877,\n",
       " 'date': 878,\n",
       " 'mary': 879,\n",
       " 'native': 880,\n",
       " 'whom': 881,\n",
       " 'married': 882,\n",
       " 'towards': 883,\n",
       " 'issues': 884,\n",
       " 'double': 885,\n",
       " 'primary': 886,\n",
       " 'basis': 887,\n",
       " 'allow': 888,\n",
       " 'enough': 889,\n",
       " 'memory': 890,\n",
       " 'reason': 891,\n",
       " 'web': 892,\n",
       " 'exist': 893,\n",
       " 'provided': 894,\n",
       " 'oil': 895,\n",
       " 'course': 896,\n",
       " 'functions': 897,\n",
       " 'alexander': 898,\n",
       " 'analysis': 899,\n",
       " 'chemical': 900,\n",
       " 'mid': 901,\n",
       " 'replaced': 902,\n",
       " 'queen': 903,\n",
       " 'claims': 904,\n",
       " 'tv': 905,\n",
       " 'sun': 906,\n",
       " 'literature': 907,\n",
       " 'metal': 908,\n",
       " 'amount': 909,\n",
       " 'divided': 910,\n",
       " 'blood': 911,\n",
       " 'likely': 912,\n",
       " 'access': 913,\n",
       " 'average': 914,\n",
       " 'length': 915,\n",
       " 'smaller': 916,\n",
       " 'medical': 917,\n",
       " 'property': 918,\n",
       " 'students': 919,\n",
       " 'degree': 920,\n",
       " 'elections': 921,\n",
       " 'club': 922,\n",
       " 'claimed': 923,\n",
       " 'performance': 924,\n",
       " 'director': 925,\n",
       " 'digital': 926,\n",
       " 'front': 927,\n",
       " 'museum': 928,\n",
       " 'difficult': 929,\n",
       " 'tradition': 930,\n",
       " 'nearly': 931,\n",
       " 'schools': 932,\n",
       " 'washington': 933,\n",
       " 'gas': 934,\n",
       " 'jesus': 935,\n",
       " 'map': 936,\n",
       " 'louis': 937,\n",
       " 'rome': 938,\n",
       " 'unit': 939,\n",
       " 'baseball': 940,\n",
       " 'mind': 941,\n",
       " 'peter': 942,\n",
       " 'mark': 943,\n",
       " 'collection': 944,\n",
       " 'product': 945,\n",
       " 'congress': 946,\n",
       " 'programs': 947,\n",
       " 'changed': 948,\n",
       " 'ideas': 949,\n",
       " 'moon': 950,\n",
       " 'entire': 951,\n",
       " 'user': 952,\n",
       " 'ground': 953,\n",
       " 'records': 954,\n",
       " 'frequently': 955,\n",
       " 'increase': 956,\n",
       " 'highest': 957,\n",
       " 'sent': 958,\n",
       " 'finally': 959,\n",
       " 'board': 960,\n",
       " 'don': 961,\n",
       " 'notable': 962,\n",
       " 'read': 963,\n",
       " 'methods': 964,\n",
       " 'recently': 965,\n",
       " 'bit': 966,\n",
       " 'involved': 967,\n",
       " 'variety': 968,\n",
       " 'call': 969,\n",
       " 'democratic': 970,\n",
       " 'ten': 971,\n",
       " 'served': 972,\n",
       " 'minor': 973,\n",
       " 'hard': 974,\n",
       " 'birth': 975,\n",
       " 'objects': 976,\n",
       " 'nuclear': 977,\n",
       " 'increased': 978,\n",
       " 'section': 979,\n",
       " 'street': 980,\n",
       " 'windows': 981,\n",
       " 'relatively': 982,\n",
       " 'car': 983,\n",
       " 'move': 984,\n",
       " 'create': 985,\n",
       " 'returned': 986,\n",
       " 'bank': 987,\n",
       " 'conditions': 988,\n",
       " 'operation': 989,\n",
       " 'adopted': 990,\n",
       " 'relationship': 991,\n",
       " 'christ': 992,\n",
       " 'hall': 993,\n",
       " 'appear': 994,\n",
       " 'rest': 995,\n",
       " 'child': 996,\n",
       " 'element': 997,\n",
       " 'appears': 998,\n",
       " 'takes': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = dot([target, context], axes = 0, normalize = True)\n",
    "#similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = dot([target, context], axes=1, normalize = False)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(inputs=[input_target, input_context], outputs=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6924301385879517\n",
      "Iteration 1000, loss=0.7097952961921692\n",
      "Iteration 2000, loss=0.7116062045097351\n",
      "Iteration 3000, loss=0.7068607807159424\n",
      "Iteration 4000, loss=0.6946460008621216\n",
      "Iteration 5000, loss=0.6970285177230835\n",
      "Iteration 6000, loss=0.6675528287887573\n",
      "Iteration 7000, loss=0.6621053218841553\n",
      "Iteration 8000, loss=0.6677365303039551\n",
      "Iteration 9000, loss=0.7078180313110352\n",
      "Iteration 10000, loss=0.7082197666168213\n",
      "Iteration 11000, loss=0.6940311789512634\n",
      "Iteration 12000, loss=0.6624252200126648\n",
      "Iteration 13000, loss=0.6789559125900269\n",
      "Iteration 14000, loss=0.683051347732544\n",
      "Iteration 15000, loss=0.7093185186386108\n",
      "Iteration 16000, loss=0.6986109018325806\n",
      "Iteration 17000, loss=0.6468752026557922\n",
      "Iteration 18000, loss=0.6591153144836426\n",
      "Iteration 19000, loss=0.7077077031135559\n",
      "Iteration 20000, loss=0.7078066468238831\n",
      "Iteration 21000, loss=0.6901941299438477\n",
      "Iteration 22000, loss=0.7145197987556458\n",
      "Iteration 23000, loss=0.6526499390602112\n",
      "Iteration 24000, loss=0.7029169201850891\n",
      "Iteration 25000, loss=0.6635842323303223\n",
      "Iteration 26000, loss=0.4651157259941101\n",
      "Iteration 27000, loss=0.6448202729225159\n",
      "Iteration 28000, loss=0.7126420140266418\n",
      "Iteration 29000, loss=0.791311502456665\n",
      "Iteration 30000, loss=0.6916847229003906\n",
      "Iteration 31000, loss=0.690248966217041\n",
      "Iteration 32000, loss=0.681338906288147\n",
      "Iteration 33000, loss=0.7379325032234192\n",
      "Iteration 34000, loss=0.7415109276771545\n",
      "Iteration 35000, loss=0.7091537117958069\n",
      "Iteration 36000, loss=0.6636674404144287\n",
      "Iteration 37000, loss=0.6310761570930481\n",
      "Iteration 38000, loss=0.7251506447792053\n",
      "Iteration 39000, loss=0.7121499180793762\n",
      "Iteration 40000, loss=0.7454696893692017\n",
      "Iteration 41000, loss=0.6498095393180847\n",
      "Iteration 42000, loss=0.6392667293548584\n",
      "Iteration 43000, loss=0.6213343143463135\n",
      "Iteration 44000, loss=0.631300687789917\n",
      "Iteration 45000, loss=0.7916322946548462\n",
      "Iteration 46000, loss=0.400653600692749\n",
      "Iteration 47000, loss=0.725066065788269\n",
      "Iteration 48000, loss=0.6811763048171997\n",
      "Iteration 49000, loss=0.4745895564556122\n",
      "Iteration 50000, loss=0.6108486652374268\n",
      "Iteration 51000, loss=0.6159943342208862\n",
      "Iteration 52000, loss=0.8368535041809082\n",
      "Iteration 53000, loss=0.8126026391983032\n",
      "Iteration 54000, loss=0.6748584508895874\n",
      "Iteration 55000, loss=0.7186274528503418\n",
      "Iteration 56000, loss=0.7220332622528076\n",
      "Iteration 57000, loss=0.6361075639724731\n",
      "Iteration 58000, loss=0.7349066138267517\n",
      "Iteration 59000, loss=0.1571112871170044\n",
      "Iteration 60000, loss=0.6251724362373352\n",
      "Iteration 61000, loss=0.9128717184066772\n",
      "Iteration 62000, loss=0.9648028016090393\n",
      "Iteration 63000, loss=0.6930667161941528\n",
      "Iteration 64000, loss=0.632921040058136\n",
      "Iteration 65000, loss=0.6132548451423645\n",
      "Iteration 66000, loss=0.691658616065979\n",
      "Iteration 67000, loss=0.7704923152923584\n",
      "Iteration 68000, loss=0.5099669694900513\n",
      "Iteration 69000, loss=0.561502993106842\n",
      "Iteration 70000, loss=0.6288299560546875\n",
      "Iteration 71000, loss=0.8048506379127502\n",
      "Iteration 72000, loss=0.5387498140335083\n",
      "Iteration 73000, loss=0.5190922021865845\n",
      "Iteration 74000, loss=0.1633332073688507\n",
      "Iteration 75000, loss=0.36953043937683105\n",
      "Iteration 76000, loss=0.704123854637146\n",
      "Iteration 77000, loss=0.6634919047355652\n",
      "Iteration 78000, loss=1.0950323343276978\n",
      "Iteration 79000, loss=0.6466196179389954\n",
      "Iteration 80000, loss=0.00012398537364788353\n",
      "Iteration 81000, loss=1.192093321833454e-07\n",
      "Iteration 82000, loss=0.9494158029556274\n",
      "Iteration 83000, loss=0.7470259070396423\n",
      "Iteration 84000, loss=0.8811811208724976\n",
      "Iteration 85000, loss=0.6380850076675415\n",
      "Iteration 86000, loss=0.8864940404891968\n",
      "Iteration 87000, loss=0.5045290589332581\n",
      "Iteration 88000, loss=1.121724247932434\n",
      "Iteration 89000, loss=0.5660993456840515\n",
      "Iteration 90000, loss=0.8468120098114014\n",
      "Iteration 91000, loss=0.5012353658676147\n",
      "Iteration 92000, loss=0.5685180425643921\n",
      "Iteration 93000, loss=0.9531753659248352\n",
      "Iteration 94000, loss=0.9044541120529175\n",
      "Iteration 95000, loss=0.4361426830291748\n",
      "Iteration 96000, loss=0.3286886215209961\n",
      "Iteration 97000, loss=0.9990411400794983\n",
      "Iteration 98000, loss=0.5958908796310425\n",
      "Iteration 99000, loss=0.5303495526313782\n",
      "Iteration 100000, loss=0.5229634046554565\n",
      "Iteration 101000, loss=0.5224841833114624\n",
      "Iteration 102000, loss=0.6921191811561584\n",
      "Iteration 103000, loss=0.7081038355827332\n",
      "Iteration 104000, loss=0.6426813006401062\n",
      "Iteration 105000, loss=0.5393646359443665\n",
      "Iteration 106000, loss=0.5814304947853088\n",
      "Iteration 107000, loss=0.6490896344184875\n",
      "Iteration 108000, loss=0.561303973197937\n",
      "Iteration 109000, loss=0.002928248606622219\n",
      "Iteration 110000, loss=0.47759997844696045\n",
      "Iteration 111000, loss=0.5162805318832397\n",
      "Iteration 112000, loss=0.8097270131111145\n",
      "Iteration 113000, loss=0.19089053571224213\n",
      "Iteration 114000, loss=0.6747979521751404\n",
      "Iteration 115000, loss=1.1010940074920654\n",
      "Iteration 116000, loss=0.5710257291793823\n",
      "Iteration 117000, loss=0.5511837601661682\n"
     ]
    }
   ],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()\n",
    "\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "#     if cnt % 10000 == 0:\n",
    "#         sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to term: become, began, life, boeing, long, usage, australia, humans,\n",
      "Nearest to external: links, six, nine, five, two, zero, one, four,\n",
      "Nearest to nine: seven, one, zero, eight, six, two, three, five,\n",
      "Nearest to four: six, five, one, seven, zero, eight, two, nine,\n",
      "Nearest to two: one, zero, three, six, eight, seven, nine, five,\n",
      "Nearest to still: seven, two, one, first, eight, five, six, zero,\n",
      "Nearest to king: two, one, eight, louis, zero, three, six, new,\n",
      "Nearest to university: cambridge, press, college, born, nine, eight, one, first,\n",
      "Nearest to five: one, zero, four, six, eight, seven, two, nine,\n",
      "Nearest to british: two, nine, seven, eight, zero, four, five, one,\n",
      "Nearest to war: zero, nine, four, two, five, one, eight, also,\n",
      "Nearest to german: seven, five, four, two, world, nine, three, six,\n",
      "Nearest to international: zero, two, may, square, also, eight, nine, five,\n",
      "Nearest to french: one, four, six, original, seven, also, language, spanish,\n",
      "Nearest to government: one, seven, three, zero, two, five, six, eight,\n",
      "Nearest to years: zero, three, one, two, nine, eight, also, four,\n"
     ]
    }
   ],
   "source": [
    "sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
